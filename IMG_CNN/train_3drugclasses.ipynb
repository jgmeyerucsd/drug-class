{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs breeds\n",
    "\n",
    "https://youtu.be/JNxcznsrRb8?t=1h31m8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/drugs/pics/\"\n",
    "sz = 224\n",
    "arch = resnext101_64\n",
    "bs = 128\n",
    "\n",
    "label_csv = f'{PATH}3labels.csv'\n",
    "n = len(list(open(label_csv))) - 1 # header is not counted (-1)\n",
    "val_idxs = get_cv_idxs(n,seed=random.sample(range(1000), 1)) # random 20% data for validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz, bs, val_idxs): # sz: image size, bs: batch size\n",
    "    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'train', f'{PATH}3labels.csv',\n",
    "                                       val_idxs=val_idxs, suffix='.png', tfms=tfms, bs=bs)\n",
    "    return data if sz > 300 else data.resize(340, 'tmp') # Reading the jpgs and resizing is slow for big images, so resizing them all to 340 first saves time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_loop1(k, epochs, bs):\n",
    "    validation_accuracy = []\n",
    "    for reps in range(k):\n",
    "        val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1)) # random 20% data for validation set\n",
    "        data = get_data(sz, bs, val_idxs)\n",
    "        learn = ConvLearner.pretrained(arch, data, precompute=True)\n",
    "        val_loss, val_acc = learn.fit(1e-2, epochs)\n",
    "        validation_accuracy.append(val_acc)\n",
    "        learn.save(str(reps)+'_3cls_10fold.model')\n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9023829e40ad444ab5053c0c5fcc8e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d72b1359ff4c578bde79caf2023575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.082428   0.893146   0.595349  \n",
      "    1      0.937164   0.847831   0.630698                  \n",
      "    2      0.864589   0.820164   0.64186                   \n",
      "    3      0.803079   0.798268   0.649302                  \n",
      "    4      0.755609   0.798082   0.666047                  \n",
      "    5      0.720466   0.782393   0.664186                  \n",
      "    6      0.693629   0.75844    0.686512                  \n",
      "    7      0.672639   0.752539   0.67907                   \n",
      "    8      0.649167   0.752267   0.691163                  \n",
      "    9      0.625356   0.765875   0.672558                  \n",
      "    10     0.610798   0.740797   0.666047                  \n",
      "    11     0.600115   0.725843   0.691163                  \n",
      "    12     0.582194   0.736498   0.688372                  \n",
      "    13     0.575747   0.750157   0.67907                   \n",
      "    14     0.556026   0.72897    0.697674                  \n",
      "    15     0.534883   0.745649   0.690233                  \n",
      "    16     0.51772    0.745133   0.692093                  \n",
      "    17     0.514092   0.722899   0.702326                  \n",
      "    18     0.501982   0.716302   0.716279                  \n",
      "    19     0.491177   0.747591   0.701395                  \n",
      "    20     0.477814   0.734928   0.708837                  \n",
      "    21     0.467      0.723357   0.705116                  \n",
      "    22     0.455017   0.73847    0.709767                  \n",
      "    23     0.446588   0.739929   0.710698                  \n",
      "    24     0.438749   0.738774   0.713488                  \n",
      "    25     0.432707   0.732465   0.708837                  \n",
      "    26     0.439174   0.744667   0.717209                  \n",
      "    27     0.427768   0.72588    0.72093                   \n",
      "    28     0.418807   0.733683   0.713488                  \n",
      "    29     0.411428   0.741384   0.709767                  \n",
      "    30     0.410418   0.734451   0.715349                  \n",
      "    31     0.396833   0.760466   0.714419                  \n",
      "    32     0.387902   0.74689    0.72093                   \n",
      "    33     0.385187   0.750808   0.724651                  \n",
      "    34     0.382644   0.750042   0.702326                  \n",
      "    35     0.378386   0.750175   0.712558                  \n",
      "    36     0.371842   0.762684   0.711628                  \n",
      "    37     0.366278   0.76247    0.711628                  \n",
      "    38     0.355048   0.767227   0.722791                  \n",
      "    39     0.355024   0.773505   0.71907                   \n",
      "    40     0.355336   0.783864   0.709767                  \n",
      "    41     0.349931   0.746659   0.71907                   \n",
      "    42     0.341332   0.770796   0.712558                  \n",
      "    43     0.334341   0.789885   0.706977                  \n",
      "    44     0.334046   0.790611   0.72093                   \n",
      "    45     0.328769   0.787499   0.708837                  \n",
      "    46     0.322072   0.792247   0.71907                   \n",
      "    47     0.326598   0.808817   0.723721                  \n",
      "    48     0.318975   0.780404   0.71814                   \n",
      "    49     0.317042   0.819659   0.709767                 \n",
      "    50     0.310226   0.802646   0.707907                  \n",
      "    51     0.309385   0.786273   0.72                      \n",
      "    52     0.303469   0.788578   0.726512                  \n",
      "    53     0.305648   0.784812   0.728372                  \n",
      "    54     0.305494   0.817137   0.733023                  \n",
      "    55     0.287587   0.805993   0.722791                  \n",
      "    56     0.282785   0.821228   0.724651                  \n",
      "    57     0.287321   0.800902   0.729302                  \n",
      "    58     0.284156   0.806263   0.725581                  \n",
      "    59     0.281451   0.844264   0.72093                   \n",
      "    60     0.28178    0.825866   0.715349                  \n",
      "    61     0.276237   0.833986   0.727442                  \n",
      "    62     0.276315   0.817831   0.730233                  \n",
      "    63     0.268368   0.82215    0.729302                  \n",
      "    64     0.274537   0.820619   0.725581                  \n",
      "    65     0.269741   0.822604   0.731163                  \n",
      "    66     0.264839   0.82726    0.732093                  \n",
      "    67     0.262731   0.840762   0.71814                   \n",
      "    68     0.265112   0.84406    0.725581                  \n",
      "    69     0.25496    0.84734    0.733023                  \n",
      "    70     0.257632   0.832296   0.728372                  \n",
      "    71     0.25343    0.847818   0.726512                  \n",
      "    72     0.257598   0.825568   0.72186                   \n",
      "    73     0.251769   0.853277   0.730233                  \n",
      "    74     0.249903   0.836      0.728372                  \n",
      "    75     0.244733   0.861099   0.724651                  \n",
      "    76     0.242802   0.880238   0.726512                  \n",
      "    77     0.244951   0.851336   0.728372                  \n",
      "    78     0.233782   0.832276   0.726512                  \n",
      "    79     0.245334   0.866384   0.728372                  \n",
      "    80     0.243367   0.864423   0.727442                  \n",
      "    81     0.237175   0.85946    0.730233                  \n",
      "    82     0.231163   0.849871   0.731163                  \n",
      "    83     0.223741   0.856616   0.736744                  \n",
      "    84     0.221283   0.846924   0.731163                  \n",
      "    85     0.222071   0.886131   0.733023                 \n",
      "    86     0.223736   0.871076   0.733023                  \n",
      "    87     0.217568   0.886699   0.735814                  \n",
      "    88     0.225225   0.881303   0.731163                  \n",
      "    89     0.220859   0.916927   0.724651                  \n",
      "    90     0.220168   0.870067   0.730233                  \n",
      "    91     0.215848   0.878176   0.735814                  \n",
      "    92     0.221085   0.877156   0.727442                  \n",
      "    93     0.220699   0.865341   0.725581                  \n",
      "    94     0.214739   0.883037   0.733023                  \n",
      "    95     0.2168     0.882614   0.728372                  \n",
      "    96     0.213973   0.883014   0.728372                  \n",
      "    97     0.207292   0.866215   0.727442                  \n",
      "    98     0.21856    0.897446   0.733953                  \n",
      "    99     0.209848   0.897186   0.728372                  \n",
      "   100     0.211182   0.891148   0.728372                  \n",
      "   101     0.206608   0.873229   0.733953                  \n",
      "   102     0.206757   0.87124    0.735814                  \n",
      "   103     0.205831   0.903499   0.736744                  \n",
      "   104     0.198574   0.889252   0.730233                  \n",
      "   105     0.193791   0.896233   0.734884                  \n",
      "   106     0.188032   0.902684   0.725581                  \n",
      "   107     0.189106   0.914756   0.733953                  \n",
      "   108     0.192782   0.905697   0.732093                  \n",
      "   109     0.197941   0.917567   0.729302                  \n",
      "   110     0.20036    0.905028   0.731163                  \n",
      "   111     0.193135   0.908218   0.729302                  \n",
      "   112     0.194945   0.91534    0.733953                  \n",
      "   113     0.196728   0.913765   0.729302                  \n",
      "   114     0.196199   0.895002   0.735814                  \n",
      "   115     0.189378   0.901954   0.725581                  \n",
      "   116     0.185985   0.895161   0.731163                  \n",
      "   117     0.185202   0.9118     0.730233                  \n",
      "   118     0.185115   0.923524   0.725581                  \n",
      "   119     0.183714   0.905574   0.735814                  \n",
      "   120     0.18662    0.898519   0.733023                  \n",
      "   121     0.186938   0.928522   0.738605                  \n",
      "   122     0.189141   0.933083   0.727442                  \n",
      "   123     0.183489   0.924042   0.742326                  \n",
      "   124     0.186368   0.91183    0.741395                  \n",
      "   125     0.183038   0.937802   0.732093                  \n",
      "   126     0.187794   0.898482   0.726512                  \n",
      "   127     0.183819   0.91473    0.735814                  \n",
      "   128     0.184815   0.924603   0.734884                  \n",
      "   129     0.18318    0.914365   0.730233                  \n",
      "   130     0.175781   0.915055   0.735814                  \n",
      "   131     0.178674   0.920693   0.734884                  \n",
      "   132     0.172258   0.952203   0.727442                  \n",
      "   133     0.174222   0.935124   0.733953                  \n",
      "   134     0.173015   0.948126   0.729302                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.174764   0.926356   0.745116                  \n",
      "   136     0.180361   0.955911   0.739535                  \n",
      "   137     0.179099   0.93259    0.739535                  \n",
      "   138     0.172065   0.921189   0.739535                  \n",
      "   139     0.174497   0.926973   0.741395                  \n",
      "   140     0.174721   0.942037   0.738605                  \n",
      "   141     0.174387   0.939967   0.740465                  \n",
      "   142     0.169221   0.940541   0.728372                  \n",
      "   143     0.163354   0.948158   0.734884                  \n",
      "   144     0.164596   0.958318   0.729302                  \n",
      "   145     0.173311   0.953618   0.729302                  \n",
      "   146     0.16589    0.969783   0.734884                  \n",
      "   147     0.168151   0.957577   0.733023                  \n",
      "   148     0.166201   0.956449   0.733953                  \n",
      "   149     0.160794   0.954914   0.735814                  \n",
      "   150     0.162215   0.983884   0.733953                  \n",
      "   151     0.165545   0.97544    0.728372                  \n",
      "   152     0.159121   0.951011   0.733953                  \n",
      "   153     0.155947   0.958737   0.735814                  \n",
      "   154     0.163387   0.955205   0.736744                  \n",
      "   155     0.156175   0.955037   0.732093                  \n",
      "   156     0.159866   0.951564   0.732093                  \n",
      "   157     0.161743   0.964216   0.734884                 \n",
      "   158     0.161618   0.965687   0.731163                  \n",
      "   159     0.159008   0.954321   0.731163                  \n",
      "   160     0.156514   0.945828   0.735814                  \n",
      "   161     0.160372   0.954801   0.732093                  \n",
      "   162     0.160369   0.957256   0.733023                  \n",
      "   163     0.158575   0.96335    0.731163                  \n",
      "   164     0.165863   0.971034   0.733953                  \n",
      "   165     0.153892   0.954498   0.739535                  \n",
      "   166     0.154569   0.978394   0.737674                  \n",
      "   167     0.154182   0.972837   0.732093                  \n",
      "   168     0.15099    0.958398   0.743256                  \n",
      "   169     0.154911   0.973855   0.729302                  \n",
      "   170     0.152258   0.968763   0.738605                  \n",
      "   171     0.148509   0.983897   0.743256                  \n",
      "   172     0.158791   0.957812   0.740465                  \n",
      "   173     0.150031   0.955588   0.733023                  \n",
      "   174     0.151124   0.955754   0.735814                  \n",
      "   175     0.147953   0.983981   0.735814                  \n",
      "   176     0.144955   0.987745   0.733953                  \n",
      "   177     0.144085   0.967089   0.742326                  \n",
      "   178     0.145093   0.973808   0.735814                  \n",
      "   179     0.145915   0.981675   0.742326                  \n",
      "   180     0.149878   0.995928   0.739535                  \n",
      "   181     0.157591   0.963738   0.737674                  \n",
      "   182     0.153188   0.958783   0.745116                  \n",
      "   183     0.153243   0.978111   0.740465                  \n",
      "   184     0.151631   0.952796   0.741395                  \n",
      "   185     0.145001   0.972493   0.738605                  \n",
      "   186     0.144822   0.980418   0.743256                  \n",
      "   187     0.151268   0.982398   0.741395                  \n",
      "   188     0.147671   0.984586   0.731163                  \n",
      "   189     0.141003   1.00895    0.733023                  \n",
      "   190     0.141816   0.974762   0.742326                  \n",
      "   191     0.144111   0.96937    0.736744                  \n",
      "   192     0.143035   0.987113   0.740465                  \n",
      "   193     0.143246   0.975      0.744186                  \n",
      "   194     0.140755   0.974141   0.742326                  \n",
      "   195     0.139976   1.009855   0.732093                  \n",
      "   196     0.139509   0.986641   0.745116                  \n",
      "   197     0.143438   0.992505   0.740465                  \n",
      "   198     0.141892   0.98967    0.740465                  \n",
      "   199     0.14464    1.01053    0.736744                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dfcffb1052489d81f91dd58b13d010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5d1477c589497d91095b67d4a88f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.072064   0.856998   0.615814  \n",
      "    1      0.91575    0.809448   0.646512                  \n",
      "    2      0.83529    0.785833   0.653953                  \n",
      "    3      0.786819   0.766103   0.657674                  \n",
      "    4      0.752897   0.768413   0.667907                  \n",
      "    5      0.718568   0.767966   0.666047                  \n",
      "    6      0.688864   0.747331   0.667907                  \n",
      "    7      0.658208   0.72378    0.685581                  \n",
      "    8      0.637758   0.697137   0.703256                  \n",
      "    9      0.618858   0.710435   0.688372                  \n",
      "    10     0.593951   0.689882   0.706977                  \n",
      "    11     0.574097   0.692829   0.710698                  \n",
      "    12     0.560845   0.68936    0.727442                  \n",
      "    13     0.545475   0.668319   0.710698                  \n",
      "    14     0.534441   0.675486   0.712558                  \n",
      "    15     0.512338   0.674481   0.722791                  \n",
      "    16     0.503479   0.676737   0.728372                  \n",
      "    17     0.498241   0.67738    0.72                      \n",
      "    18     0.48398    0.667199   0.72                      \n",
      "    19     0.468511   0.665515   0.729302                  \n",
      "    20     0.465234   0.664869   0.72186                   \n",
      "    21     0.458176   0.667366   0.72                      \n",
      "    22     0.446626   0.677657   0.729302                  \n",
      "    23     0.447161   0.681288   0.723721                  \n",
      "    24     0.436319   0.673167   0.72                      \n",
      "    25     0.427844   0.677681   0.732093                  \n",
      "    26     0.421021   0.67162    0.733023                  \n",
      "    27     0.404899   0.673935   0.724651                  \n",
      "    28     0.405516   0.675067   0.739535                  \n",
      "    29     0.395978   0.707654   0.71907                   \n",
      "    30     0.394808   0.687769   0.733953                  \n",
      "    31     0.391114   0.687837   0.726512                  \n",
      "    32     0.389943   0.673983   0.740465                  \n",
      "    33     0.37869    0.691878   0.733953                  \n",
      "    34     0.361466   0.676951   0.729302                  \n",
      "    35     0.365546   0.704897   0.732093                  \n",
      "    36     0.361801   0.652716   0.741395                  \n",
      "    37     0.353213   0.670944   0.746047                  \n",
      "    38     0.345776   0.684426   0.739535                  \n",
      "    39     0.337078   0.691229   0.739535                  \n",
      "    40     0.331713   0.671159   0.736744                  \n",
      "    41     0.32891    0.69213    0.733953                  \n",
      "    42     0.32728    0.674582   0.733023                  \n",
      "    43     0.322558   0.676458   0.734884                  \n",
      "    44     0.318813   0.686307   0.740465                  \n",
      "    45     0.3118     0.6801     0.748837                 \n",
      "    46     0.305204   0.68664    0.741395                  \n",
      "    47     0.298017   0.695238   0.742326                  \n",
      "    48     0.300466   0.705192   0.730233                  \n",
      "    49     0.30751    0.722559   0.740465                  \n",
      "    50     0.294462   0.714557   0.739535                  \n",
      "    51     0.297064   0.703693   0.735814                  \n",
      "    52     0.302272   0.731538   0.738605                  \n",
      "    53     0.298305   0.736061   0.736744                  \n",
      "    54     0.292356   0.710686   0.740465                  \n",
      "    55     0.286603   0.698762   0.731163                  \n",
      "    56     0.271356   0.695005   0.735814                  \n",
      "    57     0.263989   0.719964   0.738605                  \n",
      "    58     0.256902   0.714048   0.733953                  \n",
      "    59     0.252681   0.722259   0.740465                  \n",
      "    60     0.258042   0.726319   0.735814                  \n",
      "    61     0.254551   0.736644   0.740465                  \n",
      "    62     0.262652   0.720072   0.746977                  \n",
      "    63     0.260352   0.726767   0.744186                  \n",
      "    64     0.259138   0.744146   0.738605                  \n",
      "    65     0.259388   0.723704   0.747907                  \n",
      "    66     0.254709   0.764834   0.736744                  \n",
      "    67     0.25434    0.761998   0.730233                  \n",
      "    68     0.249437   0.726809   0.747907                  \n",
      "    69     0.252511   0.731554   0.748837                  \n",
      "    70     0.246785   0.733268   0.748837                  \n",
      "    71     0.248161   0.765608   0.746047                  \n",
      "    72     0.242928   0.742392   0.748837                  \n",
      "    73     0.244696   0.752926   0.744186                  \n",
      "    74     0.250638   0.743345   0.741395                  \n",
      "    75     0.244667   0.73072    0.741395                  \n",
      "    76     0.238799   0.755865   0.746977                  \n",
      "    77     0.234136   0.759758   0.737674                  \n",
      "    78     0.226522   0.744755   0.732093                  \n",
      "    79     0.217957   0.73549    0.740465                  \n",
      "    80     0.222077   0.767501   0.735814                  \n",
      "    81     0.22475    0.757606   0.750698                  \n",
      "    82     0.217995   0.756069   0.745116                  \n",
      "    83     0.225629   0.747599   0.742326                  \n",
      "    84     0.223798   0.75887    0.746047                  \n",
      "    85     0.220821   0.766184   0.741395                  \n",
      "    86     0.212575   0.756516   0.735814                  \n",
      "    87     0.208725   0.787511   0.745116                  \n",
      "    88     0.210802   0.77099    0.737674                  \n",
      "    89     0.212114   0.766805   0.741395                  \n",
      "    90     0.214565   0.791984   0.732093                  \n",
      "    91     0.212273   0.803318   0.734884                  \n",
      "    92     0.209182   0.780602   0.746977                  \n",
      "    93     0.206307   0.773068   0.737674                  \n",
      "    94     0.198739   0.78023    0.730233                  \n",
      "    95     0.195129   0.792064   0.738605                  \n",
      "    96     0.200004   0.771011   0.742326                  \n",
      "    97     0.200058   0.78027    0.748837                  \n",
      "    98     0.208491   0.812015   0.745116                  \n",
      "    99     0.203167   0.796543   0.751628                  \n",
      "   100     0.19675    0.774494   0.752558                  \n",
      "   101     0.202792   0.762311   0.745116                  \n",
      "   102     0.200526   0.779198   0.741395                  \n",
      "   103     0.193213   0.78694    0.743256                  \n",
      "   104     0.191693   0.782418   0.747907                  \n",
      "   105     0.202325   0.79116    0.733023                  \n",
      "   106     0.198643   0.787918   0.742326                  \n",
      "   107     0.189192   0.789281   0.748837                  \n",
      "   108     0.201553   0.805474   0.740465                  \n",
      "   109     0.193967   0.788163   0.746047                  \n",
      "   110     0.198379   0.788702   0.746047                  \n",
      "   111     0.190462   0.778185   0.749767                  \n",
      "   112     0.187701   0.776504   0.751628                  \n",
      "   113     0.189646   0.797353   0.742326                  \n",
      "   114     0.186011   0.808994   0.736744                  \n",
      "   115     0.1817     0.775463   0.749767                  \n",
      "   116     0.176869   0.792426   0.749767                  \n",
      "   117     0.181259   0.81084    0.745116                  \n",
      "   118     0.179781   0.808626   0.749767                  \n",
      "   119     0.178385   0.791273   0.750698                  \n",
      "   120     0.176951   0.785183   0.748837                  \n",
      "   121     0.171348   0.797803   0.749767                  \n",
      "   122     0.177078   0.815375   0.746047                  \n",
      "   123     0.180196   0.811489   0.743256                  \n",
      "   124     0.178659   0.81547    0.751628                  \n",
      "   125     0.173962   0.803557   0.750698                  \n",
      "   126     0.169512   0.824719   0.742326                  \n",
      "   127     0.176278   0.809532   0.748837                  \n",
      "   128     0.177373   0.818611   0.746977                  \n",
      "   129     0.176648   0.800418   0.747907                  \n",
      "   130     0.171386   0.817427   0.735814                  \n",
      "   131     0.16214    0.819584   0.743256                  \n",
      "   132     0.165861   0.817092   0.744186                  \n",
      "   133     0.161948   0.818573   0.741395                  \n",
      "   134     0.164485   0.838667   0.743256                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.167294   0.822084   0.750698                  \n",
      "   136     0.168727   0.840172   0.737674                  \n",
      "   137     0.167354   0.811289   0.745116                  \n",
      "   138     0.169443   0.833756   0.749767                  \n",
      "   139     0.164545   0.811686   0.743256                  \n",
      "   140     0.163646   0.802147   0.746977                  \n",
      "   141     0.154362   0.849802   0.733023                  \n",
      "   142     0.158037   0.830189   0.749767                  \n",
      "   143     0.162726   0.805921   0.746977                  \n",
      "   144     0.162662   0.806092   0.746977                  \n",
      "   145     0.159683   0.810871   0.745116                  \n",
      "   146     0.154797   0.813735   0.749767                  \n",
      "   147     0.157851   0.824045   0.740465                  \n",
      "   148     0.162475   0.810373   0.747907                  \n",
      "   149     0.158612   0.808852   0.753488                  \n",
      "   150     0.160465   0.83609    0.752558                  \n",
      "   151     0.159621   0.816194   0.748837                  \n",
      "   152     0.162022   0.827858   0.746047                  \n",
      "   153     0.153867   0.833178   0.746977                  \n",
      "   154     0.153223   0.836922   0.749767                  \n",
      "   155     0.154099   0.833869   0.748837                  \n",
      "   156     0.154523   0.842784   0.740465                  \n",
      "   157     0.151222   0.843234   0.747907                  \n",
      "   158     0.148546   0.8655     0.743256                  \n",
      "   159     0.152569   0.83772    0.748837                  \n",
      "   160     0.149978   0.818952   0.752558                  \n",
      "   161     0.154482   0.819      0.743256                  \n",
      "   162     0.145479   0.832481   0.745116                 \n",
      "   163     0.144869   0.853965   0.747907                  \n",
      "   164     0.147741   0.839255   0.749767                  \n",
      "   165     0.149284   0.82705    0.749767                  \n",
      "   166     0.148329   0.841882   0.747907                  \n",
      "   167     0.145585   0.861664   0.746977                  \n",
      "   168     0.14357    0.848332   0.751628                  \n",
      "   169     0.150226   0.864507   0.746977                  \n",
      "   170     0.149364   0.843967   0.755349                  \n",
      "   171     0.147779   0.874383   0.745116                  \n",
      "   172     0.145661   0.848338   0.747907                  \n",
      "   173     0.142595   0.842318   0.746047                  \n",
      "   174     0.149299   0.842504   0.754419                  \n",
      "   175     0.146707   0.839778   0.749767                  \n",
      "   176     0.147193   0.851613   0.752558                  \n",
      "   177     0.149234   0.84229    0.757209                  \n",
      "   178     0.147994   0.831533   0.753488                  \n",
      "   179     0.147624   0.856497   0.757209                  \n",
      "   180     0.144385   0.869975   0.751628                  \n",
      "   181     0.143134   0.866456   0.745116                  \n",
      "   182     0.142982   0.851001   0.746047                  \n",
      "   183     0.1394     0.841291   0.754419                  \n",
      "   184     0.138608   0.83882    0.75814                   \n",
      "   185     0.139507   0.852315   0.76                      \n",
      "   186     0.141901   0.836922   0.75907                   \n",
      "   187     0.142429   0.843058   0.76093                   \n",
      "   188     0.14417    0.858189   0.746977                  \n",
      "   189     0.138887   0.841259   0.752558                  \n",
      "   190     0.136603   0.870082   0.752558                  \n",
      "   191     0.133691   0.846587   0.749767                  \n",
      "   192     0.13544    0.838447   0.76                      \n",
      "   193     0.13548    0.851138   0.756279                  \n",
      "   194     0.132229   0.853618   0.749767                  \n",
      "   195     0.136652   0.845391   0.755349                  \n",
      "   196     0.131758   0.852451   0.753488                  \n",
      "   197     0.131341   0.848553   0.757209                  \n",
      "   198     0.136416   0.847074   0.751628                  \n",
      "   199     0.132107   0.856088   0.751628                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1238c38b434c37aba5afccd4ca6db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbbc98381d741d89ad7660580008aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.04527    0.886909   0.611163  \n",
      "    1      0.932057   0.848691   0.622326                  \n",
      "    2      0.856383   0.837951   0.619535                  \n",
      "    3      0.801584   0.812044   0.643721                  \n",
      "    4      0.756045   0.801358   0.650233                  \n",
      "    5      0.716925   0.781133   0.662326                  \n",
      "    6      0.701786   0.773765   0.668837                  \n",
      "    7      0.670261   0.764345   0.671628                  \n",
      "    8      0.648019   0.7447     0.689302                  \n",
      "    9      0.633842   0.742504   0.68186                   \n",
      "    10     0.615597   0.749803   0.688372                  \n",
      "    11     0.594399   0.740093   0.692093                  \n",
      "    12     0.57292    0.721759   0.702326                  \n",
      "    13     0.561556   0.749712   0.696744                  \n",
      "    14     0.548305   0.729001   0.697674                  \n",
      "    15     0.530837   0.717324   0.699535                  \n",
      "    16     0.528871   0.738881   0.687442                  \n",
      "    17     0.516482   0.707189   0.703256                  \n",
      "    18     0.499591   0.734762   0.698605                  \n",
      "    19     0.492228   0.734168   0.706977                  \n",
      "    20     0.478329   0.710653   0.711628                  \n",
      "    21     0.464269   0.733659   0.699535                  \n",
      "    22     0.454885   0.712392   0.711628                  \n",
      "    23     0.460947   0.73334    0.709767                  \n",
      "    24     0.450444   0.726644   0.715349                  \n",
      "    25     0.436255   0.730264   0.716279                  \n",
      "    26     0.433494   0.720856   0.722791                  \n",
      "    27     0.422989   0.72954    0.713488                  \n",
      "    28     0.415278   0.714568   0.72                      \n",
      "    29     0.40887    0.708724   0.72                      \n",
      "    30     0.405977   0.707368   0.72186                   \n",
      "    31     0.394958   0.729436   0.713488                  \n",
      "    32     0.385048   0.722378   0.724651                  \n",
      "    33     0.382678   0.718646   0.72                      \n",
      "    34     0.376585   0.727344   0.726512                  \n",
      "    35     0.368299   0.746527   0.72                      \n",
      "    36     0.353263   0.734312   0.72186                   \n",
      "    37     0.359455   0.72012    0.72093                   \n",
      "    38     0.356565   0.716652   0.726512                  \n",
      "    39     0.350166   0.719015   0.72186                   \n",
      "    40     0.34849    0.730962   0.72                      \n",
      "    41     0.345692   0.739579   0.72                      \n",
      "    42     0.33207    0.742007   0.726512                  \n",
      "    43     0.326684   0.791432   0.712558                  \n",
      "    44     0.33277    0.741104   0.729302                  \n",
      "    45     0.321613   0.736369   0.715349                  \n",
      "    46     0.325908   0.765688   0.728372                  \n",
      "    47     0.323408   0.76492    0.723721                  \n",
      "    48     0.309212   0.787555   0.72093                   \n",
      "    49     0.303863   0.77049    0.722791                  \n",
      "    50     0.310859   0.768362   0.72                      \n",
      "    51     0.305416   0.779139   0.72186                   \n",
      "    52     0.298408   0.761487   0.724651                  \n",
      "    53     0.294799   0.750031   0.729302                  \n",
      "    54     0.296503   0.802147   0.724651                  \n",
      "    55     0.283562   0.766853   0.715349                  \n",
      "    56     0.278941   0.757121   0.722791                  \n",
      "    57     0.272558   0.781566   0.72186                   \n",
      "    58     0.27437    0.768269   0.727442                  \n",
      "    59     0.268906   0.796356   0.728372                  \n",
      "    60     0.27103    0.782654   0.726512                  \n",
      "    61     0.267855   0.787268   0.728372                  \n",
      "    62     0.270721   0.796898   0.722791                  \n",
      "    63     0.263731   0.809655   0.725581                  \n",
      "    64     0.26582    0.820469   0.724651                  \n",
      "    65     0.263264   0.814171   0.717209                  \n",
      "    66     0.258816   0.812271   0.725581                  \n",
      "    67     0.255516   0.811824   0.72                      \n",
      "    68     0.247325   0.815855   0.729302                  \n",
      "    69     0.252468   0.834463   0.724651                  \n",
      "    70     0.249745   0.816535   0.72                      \n",
      "    71     0.247779   0.804207   0.722791                  \n",
      "    72     0.23958    0.818587   0.72093                   \n",
      "    73     0.234969   0.81356    0.733023                  \n",
      "    74     0.232453   0.819877   0.727442                  \n",
      "    75     0.231495   0.830485   0.72093                   \n",
      "    76     0.235783   0.812974   0.723721                  \n",
      "    77     0.237041   0.818809   0.731163                  \n",
      "    78     0.236023   0.804959   0.724651                  \n",
      "    79     0.233136   0.822032   0.725581                  \n",
      "    80     0.227598   0.835512   0.727442                  \n",
      "    81     0.227996   0.83389    0.725581                  \n",
      "    82     0.237585   0.847449   0.725581                  \n",
      "    83     0.230158   0.846139   0.723721                  \n",
      "    84     0.220117   0.814104   0.729302                  \n",
      "    85     0.219887   0.810983   0.732093                  \n",
      "    86     0.216114   0.834214   0.731163                  \n",
      "    87     0.210065   0.821124   0.727442                  \n",
      "    88     0.212008   0.835448   0.735814                  \n",
      "    89     0.218735   0.84154    0.727442                  \n",
      "    90     0.218144   0.841475   0.734884                  \n",
      "    91     0.214916   0.827577   0.727442                  \n",
      "    92     0.209345   0.852523   0.716279                  \n",
      "    93     0.209437   0.839435   0.715349                  \n",
      "    94     0.203965   0.853816   0.724651                  \n",
      "    95     0.201418   0.847079   0.727442                  \n",
      "    96     0.200685   0.867712   0.72                      \n",
      "    97     0.200471   0.852151   0.729302                  \n",
      "    98     0.207723   0.844799   0.728372                  \n",
      "    99     0.205647   0.836209   0.730233                  \n",
      "   100     0.197505   0.840666   0.724651                  \n",
      "   101     0.198493   0.835115   0.735814                  \n",
      "   102     0.199133   0.889492   0.726512                  \n",
      "   103     0.195418   0.866431   0.729302                  \n",
      "   104     0.191792   0.869005   0.727442                  \n",
      "   105     0.183072   0.855403   0.732093                  \n",
      "   106     0.184771   0.882707   0.728372                  \n",
      "   107     0.184213   0.91946    0.730233                  \n",
      "   108     0.184616   0.875668   0.733023                  \n",
      "   109     0.186324   0.865901   0.733023                  \n",
      "   110     0.193375   0.879268   0.737674                  \n",
      "   111     0.195507   0.873874   0.733023                  \n",
      "   112     0.200402   0.869089   0.732093                  \n",
      "   113     0.2004     0.858153   0.736744                  \n",
      "   114     0.191692   0.86739    0.727442                  \n",
      "   115     0.182864   0.892176   0.727442                  \n",
      "   116     0.186645   0.899498   0.728372                  \n",
      "   117     0.188974   0.856693   0.727442                  \n",
      "   118     0.183144   0.879477   0.733953                  \n",
      "   119     0.185549   0.859868   0.735814                  \n",
      "   120     0.179458   0.850479   0.731163                  \n",
      "   121     0.179233   0.868657   0.733953                  \n",
      "   122     0.180946   0.863034   0.734884                  \n",
      "   123     0.18376    0.887486   0.730233                  \n",
      "   124     0.179634   0.873788   0.736744                  \n",
      "   125     0.176759   0.879333   0.727442                  \n",
      "   126     0.167526   0.870473   0.730233                  \n",
      "   127     0.170271   0.891705   0.737674                 \n",
      "   128     0.171158   0.868884   0.744186                  \n",
      "   129     0.170026   0.867299   0.742326                  \n",
      "   130     0.173421   0.86514    0.736744                  \n",
      "   131     0.17645    0.884176   0.739535                  \n",
      "   132     0.173322   0.881912   0.735814                  \n",
      "   133     0.172424   0.871185   0.738605                  \n",
      "   134     0.170493   0.875079   0.746047                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.17158    0.882326   0.742326                  \n",
      "   136     0.171627   0.869959   0.733023                  \n",
      "   137     0.166839   0.89224    0.739535                  \n",
      "   138     0.16489    0.899922   0.745116                  \n",
      "   139     0.169525   0.891828   0.742326                  \n",
      "   140     0.165123   0.894556   0.739535                  \n",
      "   141     0.161843   0.90081    0.733953                  \n",
      "   142     0.166316   0.902678   0.733953                  \n",
      "   143     0.170188   0.89974    0.733953                  \n",
      "   144     0.17102    0.904341   0.729302                  \n",
      "   145     0.15773    0.900008   0.738605                  \n",
      "   146     0.155093   0.881814   0.735814                  \n",
      "   147     0.155997   0.91596    0.733953                  \n",
      "   148     0.155962   0.906349   0.732093                  \n",
      "   149     0.160678   0.904986   0.733023                  \n",
      "   150     0.158147   0.905211   0.732093                  \n",
      "   151     0.155111   0.912443   0.734884                  \n",
      "   152     0.157888   0.900824   0.738605                  \n",
      "   153     0.15782    0.893194   0.737674                  \n",
      "   154     0.159034   0.897927   0.737674                  \n",
      "   155     0.158795   0.921614   0.741395                  \n",
      "   156     0.159214   0.940913   0.733023                  \n",
      "   157     0.160941   0.91397    0.733023                  \n",
      "   158     0.158964   0.916429   0.736744                  \n",
      "   159     0.154639   0.923682   0.740465                  \n",
      "   160     0.152823   0.925209   0.733023                  \n",
      "   161     0.150527   0.909558   0.736744                  \n",
      "   162     0.156732   0.912065   0.737674                  \n",
      "   163     0.153999   0.904857   0.737674                  \n",
      "   164     0.152621   0.89762    0.741395                  \n",
      "   165     0.154008   0.930537   0.733953                  \n",
      "   166     0.151464   0.917077   0.732093                  \n",
      "   167     0.151234   0.920051   0.737674                  \n",
      "   168     0.148631   0.9301     0.745116                  \n",
      "   169     0.153694   0.937701   0.738605                  \n",
      "   170     0.15187    0.925657   0.738605                  \n",
      "   171     0.149362   0.936261   0.738605                  \n",
      "   172     0.147945   0.94406    0.738605                  \n",
      "   173     0.149579   0.923683   0.735814                  \n",
      "   174     0.149165   0.928434   0.738605                  \n",
      "   175     0.149945   0.950456   0.735814                  \n",
      "   176     0.151184   0.920277   0.738605                  \n",
      "   177     0.147364   0.943324   0.736744                  \n",
      "   178     0.149252   0.936265   0.741395                  \n",
      "   179     0.148141   0.909893   0.741395                  \n",
      "   180     0.154055   0.921767   0.740465                  \n",
      "   181     0.146527   0.935023   0.737674                  \n",
      "   182     0.146999   0.926406   0.740465                  \n",
      "   183     0.140775   0.93249    0.741395                  \n",
      "   184     0.139764   0.940461   0.737674                  \n",
      "   185     0.133589   0.94155    0.737674                  \n",
      "   186     0.137301   0.928168   0.732093                  \n",
      "   187     0.141594   0.943246   0.739535                  \n",
      "   188     0.142807   0.941467   0.733953                  \n",
      "   189     0.137398   0.941494   0.739535                  \n",
      "   190     0.139931   0.943326   0.737674                  \n",
      "   191     0.139541   0.940556   0.738605                  \n",
      "   192     0.140221   0.954019   0.736744                  \n",
      "   193     0.135923   0.945183   0.736744                  \n",
      "   194     0.134857   0.936579   0.741395                  \n",
      "   195     0.13881    0.938798   0.741395                  \n",
      "   196     0.138164   0.949433   0.736744                  \n",
      "   197     0.14048    0.961728   0.733023                  \n",
      "   198     0.137481   0.941441   0.731163                  \n",
      "   199     0.133283   0.955878   0.733953                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b9c74bd4844678b642d4324fb5784f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579afa6015504250a690c8088b911aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.057624   0.85219    0.626047  \n",
      "    1      0.919705   0.811833   0.646512                  \n",
      "    2      0.845149   0.767229   0.659535                  \n",
      "    3      0.794241   0.798174   0.655814                  \n",
      "    4      0.75568    0.757002   0.674419                  \n",
      "    5      0.715189   0.753033   0.671628                  \n",
      "    6      0.688509   0.779001   0.656744                  \n",
      "    7      0.66129    0.735571   0.68                      \n",
      "    8      0.640523   0.718196   0.696744                  \n",
      "    9      0.614924   0.718168   0.684651                  \n",
      "    10     0.601256   0.694418   0.711628                  \n",
      "    11     0.582119   0.705438   0.708837                  \n",
      "    12     0.560599   0.701572   0.704186                  \n",
      "    13     0.548001   0.7125     0.699535                  \n",
      "    14     0.535185   0.694013   0.711628                  \n",
      "    15     0.532902   0.689848   0.72093                   \n",
      "    16     0.513688   0.705279   0.706977                  \n",
      "    17     0.506302   0.677167   0.731163                  \n",
      "    18     0.492214   0.721934   0.707907                  \n",
      "    19     0.477275   0.685149   0.715349                  \n",
      "    20     0.476628   0.719935   0.713488                  \n",
      "    21     0.458458   0.682428   0.717209                  \n",
      "    22     0.464508   0.686227   0.723721                  \n",
      "    23     0.448894   0.696904   0.735814                  \n",
      "    24     0.439673   0.688896   0.724651                  \n",
      "    25     0.425238   0.68378    0.733023                  \n",
      "    26     0.423581   0.682094   0.730233                  \n",
      "    27     0.411948   0.718338   0.72093                   \n",
      "    28     0.403781   0.714072   0.725581                  \n",
      "    29     0.390373   0.701603   0.731163                  \n",
      "    30     0.384774   0.700539   0.738605                  \n",
      "    31     0.391006   0.71186    0.732093                  \n",
      "    32     0.386345   0.679364   0.741395                  \n",
      "    33     0.374931   0.711774   0.729302                  \n",
      "    34     0.365534   0.710477   0.737674                  \n",
      "    35     0.355098   0.736531   0.734884                  \n",
      "    36     0.346974   0.715568   0.729302                  \n",
      "    37     0.353505   0.70593    0.740465                  \n",
      "    38     0.338689   0.72455    0.735814                  \n",
      "    39     0.340822   0.724072   0.728372                  \n",
      "    40     0.336187   0.723819   0.736744                  \n",
      "    41     0.340222   0.72088    0.738605                  \n",
      "    42     0.337196   0.740751   0.726512                  \n",
      "    43     0.33153    0.729059   0.736744                  \n",
      "    44     0.320061   0.753276   0.71814                   \n",
      "    45     0.315301   0.72497    0.743256                  \n",
      "    46     0.307764   0.740338   0.736744                  \n",
      "    47     0.309631   0.734446   0.727442                  \n",
      "    48     0.314358   0.73238    0.743256                 \n",
      "    49     0.313744   0.713936   0.743256                  \n",
      "    50     0.30391    0.746384   0.734884                  \n",
      "    51     0.301003   0.731029   0.732093                  \n",
      "    52     0.299512   0.738747   0.738605                  \n",
      "    53     0.292724   0.741204   0.733023                  \n",
      "    54     0.289409   0.752062   0.731163                  \n",
      "    55     0.288124   0.731784   0.733023                  \n",
      "    56     0.286686   0.757154   0.742326                  \n",
      "    57     0.283108   0.772912   0.727442                  \n",
      "    58     0.280204   0.758529   0.736744                  \n",
      "    59     0.280378   0.757303   0.736744                  \n",
      "    60     0.274425   0.775944   0.725581                  \n",
      "    61     0.263768   0.749538   0.753488                  \n",
      "    62     0.261894   0.766506   0.742326                  \n",
      "    63     0.264693   0.749812   0.755349                  \n",
      "    64     0.265871   0.756666   0.747907                  \n",
      "    65     0.26005    0.789082   0.735814                  \n",
      "    66     0.251711   0.775441   0.730233                  \n",
      "    67     0.251486   0.775971   0.738605                  \n",
      "    68     0.245671   0.754012   0.739535                  \n",
      "    69     0.2423     0.776652   0.741395                  \n",
      "    70     0.24097    0.77115    0.746047                  \n",
      "    71     0.24067    0.750783   0.740465                  \n",
      "    72     0.239263   0.782994   0.740465                  \n",
      "    73     0.230648   0.786329   0.748837                  \n",
      "    74     0.229422   0.778569   0.740465                  \n",
      "    75     0.235095   0.776188   0.736744                  \n",
      "    76     0.233802   0.780446   0.734884                  \n",
      "    77     0.237098   0.773969   0.738605                  \n",
      "    78     0.237757   0.796069   0.733953                  \n",
      "    79     0.236731   0.779146   0.742326                  \n",
      "    80     0.227235   0.784804   0.742326                  \n",
      "    81     0.225036   0.796987   0.737674                  \n",
      "    82     0.229831   0.801264   0.736744                  \n",
      "    83     0.230075   0.772854   0.734884                  \n",
      "    84     0.21952    0.798118   0.745116                  \n",
      "    85     0.214671   0.795878   0.744186                  \n",
      "    86     0.22264    0.80624    0.744186                  \n",
      "    87     0.217692   0.785893   0.750698                  \n",
      "    88     0.215443   0.797181   0.733023                  \n",
      "    89     0.216549   0.788168   0.742326                  \n",
      "    90     0.215829   0.791527   0.743256                  \n",
      "    91     0.212761   0.784377   0.746977                  \n",
      "    92     0.21705    0.793439   0.746977                  \n",
      "    93     0.210955   0.796202   0.748837                  \n",
      "    94     0.208436   0.802461   0.746977                  \n",
      "    95     0.207222   0.799728   0.744186                  \n",
      "    96     0.203755   0.795328   0.745116                  \n",
      "    97     0.202191   0.803683   0.746047                  \n",
      "    98     0.207575   0.797927   0.740465                  \n",
      "    99     0.206808   0.812571   0.739535                  \n",
      "   100     0.208107   0.780646   0.738605                  \n",
      "   101     0.204      0.788634   0.742326                  \n",
      "   102     0.202235   0.801749   0.749767                  \n",
      "   103     0.198065   0.816326   0.752558                  \n",
      "   104     0.192757   0.795983   0.752558                  \n",
      "   105     0.192439   0.81072    0.738605                  \n",
      "   106     0.19461    0.826704   0.746047                  \n",
      "   107     0.196185   0.81918    0.746977                  \n",
      "   108     0.192708   0.827112   0.735814                  \n",
      "   109     0.187768   0.823708   0.745116                  \n",
      "   110     0.187394   0.810794   0.75814                   \n",
      "   111     0.188105   0.836987   0.749767                  \n",
      "   112     0.18512    0.837655   0.735814                  \n",
      "   113     0.188216   0.816529   0.748837                  \n",
      "   114     0.182431   0.822756   0.741395                  \n",
      "   115     0.180154   0.820063   0.742326                  \n",
      "   116     0.185845   0.82848    0.749767                  \n",
      "   117     0.1964     0.812495   0.750698                  \n",
      "   118     0.184264   0.83418    0.748837                  \n",
      "   119     0.186145   0.831568   0.746977                 \n",
      "   120     0.187007   0.82685    0.746977                  \n",
      "   121     0.179475   0.830563   0.743256                  \n",
      "   122     0.177833   0.830503   0.746977                  \n",
      "   123     0.174862   0.827194   0.748837                  \n",
      "   124     0.175271   0.820627   0.746977                  \n",
      "   125     0.179207   0.826233   0.737674                  \n",
      "   126     0.174622   0.827292   0.744186                  \n",
      "   127     0.177398   0.856573   0.736744                  \n",
      "   128     0.172228   0.845822   0.740465                  \n",
      "   129     0.174171   0.833423   0.748837                  \n",
      "   130     0.172554   0.837404   0.746047                  \n",
      "   131     0.173679   0.834028   0.737674                  \n",
      "   132     0.170294   0.841855   0.738605                  \n",
      "   133     0.164294   0.836059   0.742326                  \n",
      "   134     0.165858   0.85194    0.740465                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.164314   0.847538   0.733023                  \n",
      "   136     0.167551   0.879371   0.739535                  \n",
      "   137     0.16818    0.842694   0.743256                  \n",
      "   138     0.169999   0.848346   0.742326                  \n",
      "   139     0.167625   0.83384    0.749767                  \n",
      "   140     0.164506   0.856635   0.745116                  \n",
      "   141     0.164328   0.840494   0.752558                  \n",
      "   142     0.162268   0.838614   0.746047                  \n",
      "   143     0.159062   0.842908   0.741395                  \n",
      "   144     0.162111   0.862533   0.735814                  \n",
      "   145     0.157283   0.863925   0.736744                  \n",
      "   146     0.160054   0.854942   0.738605                  \n",
      "   147     0.159418   0.865534   0.736744                  \n",
      "   148     0.157691   0.859946   0.737674                  \n",
      "   149     0.159968   0.87384    0.746047                  \n",
      "   150     0.165573   0.880446   0.734884                 \n",
      "   151     0.165558   0.85391    0.751628                  \n",
      "   152     0.161522   0.851373   0.737674                  \n",
      "   153     0.157217   0.871418   0.741395                  \n",
      "   154     0.159619   0.864305   0.743256                  \n",
      "   155     0.163964   0.846312   0.752558                  \n",
      "   156     0.159682   0.870565   0.735814                  \n",
      "   157     0.164048   0.894722   0.733953                  \n",
      "   158     0.154647   0.870652   0.737674                  \n",
      "   159     0.156737   0.874415   0.736744                  \n",
      "   160     0.157375   0.861634   0.744186                  \n",
      "   161     0.150203   0.857529   0.739535                  \n",
      "   162     0.149035   0.864512   0.746977                  \n",
      "   163     0.154038   0.866517   0.743256                  \n",
      "   164     0.150684   0.875727   0.751628                  \n",
      "   165     0.152673   0.861466   0.744186                  \n",
      "   166     0.15332    0.85252    0.747907                  \n",
      "   167     0.153703   0.865549   0.746977                  \n",
      "   168     0.156013   0.87053    0.746047                  \n",
      "   169     0.150009   0.859755   0.745116                  \n",
      "   170     0.14682    0.886439   0.741395                  \n",
      "   171     0.145525   0.874152   0.746047                  \n",
      "   172     0.148421   0.874572   0.743256                  \n",
      "   173     0.149571   0.861471   0.740465                  \n",
      "   174     0.1456     0.863083   0.742326                  \n",
      "   175     0.145881   0.868204   0.744186                  \n",
      "   176     0.145499   0.872197   0.746047                  \n",
      "   177     0.145639   0.858094   0.743256                  \n",
      "   178     0.146505   0.859032   0.738605                  \n",
      "   179     0.148046   0.859594   0.744186                  \n",
      "   180     0.147189   0.854201   0.748837                  \n",
      "   181     0.146737   0.853285   0.752558                  \n",
      "   182     0.149181   0.853277   0.750698                  \n",
      "   183     0.149722   0.839591   0.746977                  \n",
      "   184     0.147914   0.880956   0.744186                  \n",
      "   185     0.142062   0.862295   0.744186                  \n",
      "   186     0.140395   0.870692   0.744186                  \n",
      "   187     0.143645   0.862022   0.746977                  \n",
      "   188     0.146453   0.897861   0.734884                  \n",
      "   189     0.147809   0.86473    0.749767                  \n",
      "   190     0.148383   0.865996   0.747907                  \n",
      "   191     0.14051    0.873011   0.746047                  \n",
      "   192     0.143891   0.901636   0.737674                  \n",
      "   193     0.13962    0.881565   0.745116                  \n",
      "   194     0.136248   0.890946   0.742326                  \n",
      "   195     0.140641   0.8778     0.737674                  \n",
      "   196     0.14449    0.886557   0.746977                  \n",
      "   197     0.143179   0.879965   0.742326                  \n",
      "   198     0.144031   0.881205   0.732093                  \n",
      "   199     0.140322   0.897343   0.737674                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd478ce293c49a798317f933d13fb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37905d20da9d4b20910b54c01245f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.066126   0.925577   0.591628  \n",
      "    1      0.923518   0.847359   0.614884                  \n",
      "    2      0.849016   0.830009   0.625116                  \n",
      "    3      0.799584   0.777279   0.661395                  \n",
      "    4      0.762304   0.784875   0.658605                  \n",
      "    5      0.732171   0.767574   0.666047                  \n",
      "    6      0.702414   0.758057   0.667907                  \n",
      "    7      0.672446   0.748148   0.687442                  \n",
      "    8      0.652604   0.731264   0.699535                  \n",
      "    9      0.628207   0.722137   0.696744                  \n",
      "    10     0.607928   0.729005   0.693023                  \n",
      "    11     0.587025   0.712258   0.697674                  \n",
      "    12     0.571302   0.732256   0.706047                  \n",
      "    13     0.556666   0.717184   0.699535                  \n",
      "    14     0.551646   0.711297   0.707907                  \n",
      "    15     0.535314   0.70639    0.704186                  \n",
      "    16     0.522842   0.695897   0.707907                  \n",
      "    17     0.511606   0.703701   0.710698                  \n",
      "    18     0.495627   0.690733   0.72                      \n",
      "    19     0.48051    0.713169   0.72093                   \n",
      "    20     0.476408   0.732539   0.709767                  \n",
      "    21     0.461237   0.717084   0.708837                  \n",
      "    22     0.458315   0.708038   0.708837                  \n",
      "    23     0.447996   0.718863   0.699535                  \n",
      "    24     0.448994   0.733825   0.701395                  \n",
      "    25     0.443438   0.749042   0.701395                  \n",
      "    26     0.44157    0.702468   0.717209                  \n",
      "    27     0.434471   0.70944    0.71907                   \n",
      "    28     0.421828   0.715727   0.711628                  \n",
      "    29     0.418233   0.701053   0.722791                  \n",
      "    30     0.404778   0.720757   0.706977                  \n",
      "    31     0.407891   0.717165   0.715349                  \n",
      "    32     0.402243   0.737855   0.712558                  \n",
      "    33     0.385869   0.739753   0.71814                   \n",
      "    34     0.376226   0.72204    0.716279                  \n",
      "    35     0.374679   0.731978   0.716279                  \n",
      "    36     0.36781    0.716561   0.733023                  \n",
      "    37     0.37154    0.715595   0.738605                  \n",
      "    38     0.361343   0.760512   0.72093                   \n",
      "    39     0.342498   0.773742   0.716279                  \n",
      "    40     0.346341   0.743728   0.724651                  \n",
      "    41     0.345313   0.745786   0.716279                  \n",
      "    42     0.330663   0.757012   0.72186                   \n",
      " 41%|      | 14/34 [00:00<00:00, 36.83it/s, loss=0.329]"
     ]
    }
   ],
   "source": [
    "v_a = k_fold_cross_loop1(10,200,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7358139539319415, 0.7367441863791887, 0.7423255817834721, 0.7460465120160302, 0.7386046516063601, 0.733953488649324, 0.7423255817834721, 0.7432558143416116, 0.7367441864900811, 0.7479069770768632]\n"
     ]
    }
   ],
   "source": [
    "print(v_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=0.7403720934058345\n",
      "stdev.=0.004431071761999424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('mean='+str(np.mean(v_a)))\n",
    "print('stdev.='+str(np.std(v_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69 0.12 0.2 ]                            \n",
      " [0.13 0.51 0.35]\n",
      " [0.11 0.13 0.76]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldx_loop2(k, epochs, name, precomp, bs):\n",
    "    validation_accuracy = []\n",
    "    for reps in range(k):\n",
    "        val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1)) # random 20% data for validation set\n",
    "        data = get_data(sz, bs)\n",
    "        learn = ConvLearner.pretrained(arch, data, precompute=precomp, ps =0.5)\n",
    "        val_loss, val_acc = learn.fit(1e-2, epochs)\n",
    "        validation_accuracy.append(val_acc)\n",
    "        learn.save(str(reps)+name)\n",
    "    return validation_accuracy, learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd19e2642364a52aa0a696f86018694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0575559c97f045eaaf0bcf903a715e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.175313   0.91654    0.579535  \n",
      "    1      1.062641   0.880123   0.60186                  \n",
      "    2      1.002226   0.872993   0.605581                 \n",
      "    3      0.960023   0.857914   0.620465                  \n",
      "    4      0.9271     0.841847   0.627907                  \n",
      "    5      0.909901   0.826804   0.627907                  \n",
      "    6      0.891133   0.819678   0.636279                  \n",
      "    7      0.874831   0.818093   0.628837                  \n",
      "    8      0.859965   0.804972   0.64                      \n",
      "    9      0.853661   0.798769   0.633488                  \n",
      "    10     0.834958   0.797326   0.642791                  \n",
      "    11     0.83243    0.785742   0.652093                  \n",
      "    12     0.822138   0.788079   0.650233                  \n",
      "    13     0.818411   0.78963    0.649302                  \n",
      "    14     0.813005   0.775093   0.646512                  \n",
      "    15     0.810052   0.773537   0.642791                  \n",
      "    16     0.805838   0.783428   0.64186                   \n",
      "    17     0.802598   0.775331   0.644651                  \n",
      "    18     0.790968   0.76554    0.644651                  \n",
      "    19     0.78475    0.765814   0.665116                  \n",
      "    20     0.781932   0.756165   0.661395                  \n",
      "    21     0.771885   0.756996   0.659535                  \n",
      "    22     0.767589   0.745298   0.656744                  \n",
      "    23     0.760327   0.753445   0.661395                  \n",
      "    24     0.754322   0.744832   0.661395                  \n",
      "    25     0.75917    0.751285   0.658605                  \n",
      "    26     0.75787    0.752621   0.659535                  \n",
      "    27     0.748627   0.748776   0.654884                  \n",
      "    28     0.742105   0.737697   0.664186                  \n",
      "    29     0.741687   0.730426   0.670698                  \n",
      "    30     0.73637    0.733086   0.668837                  \n",
      "    31     0.731021   0.743819   0.667907                  \n",
      "    32     0.726736   0.737932   0.670698                  \n",
      "    33     0.72195    0.733814   0.68093                   \n",
      "    34     0.725921   0.732404   0.68                      \n",
      "    35     0.72339    0.723701   0.675349                  \n",
      "    36     0.723344   0.726265   0.673488                  \n",
      "    37     0.720332   0.723322   0.674419                  \n",
      "    38     0.71395    0.741649   0.668837                  \n",
      "    39     0.708111   0.734165   0.668837                  \n",
      "    40     0.704514   0.738038   0.673488                  \n",
      "    41     0.702684   0.742637   0.675349                  \n",
      "    42     0.703087   0.726331   0.677209                  \n",
      "    43     0.701765   0.732407   0.676279                  \n",
      "    44     0.699359   0.721711   0.676279                  \n",
      "    45     0.695957   0.720748   0.676279                  \n",
      "    46     0.697429   0.71578    0.674419                  \n",
      "    47     0.691948   0.713599   0.682791                  \n",
      "    48     0.692092   0.7223     0.685581                  \n",
      "    49     0.69018    0.713707   0.682791                  \n",
      "    50     0.690555   0.724119   0.684651                  \n",
      "    51     0.683436   0.716305   0.696744                  \n",
      "    52     0.681565   0.712534   0.687442                  \n",
      "    53     0.676562   0.716809   0.686512                  \n",
      "    54     0.679705   0.706935   0.682791                  \n",
      "    55     0.677609   0.706748   0.691163                  \n",
      "    56     0.671669   0.718367   0.68                      \n",
      "    57     0.675343   0.704979   0.687442                  \n",
      "    58     0.674849   0.707149   0.693953                  \n",
      "    59     0.669791   0.711169   0.686512                  \n",
      "    60     0.660245   0.712064   0.68093                   \n",
      "    61     0.667953   0.708894   0.677209                  \n",
      "    62     0.657495   0.713039   0.693023                  \n",
      "    63     0.660743   0.713447   0.67907                   \n",
      "    64     0.658058   0.709563   0.687442                  \n",
      "    65     0.658685   0.723694   0.692093                  \n",
      "    66     0.65901    0.702538   0.691163                  \n",
      "    67     0.656419   0.712334   0.696744                  \n",
      "    68     0.654353   0.708974   0.684651                  \n",
      "    69     0.652114   0.703018   0.687442                  \n",
      "    70     0.649186   0.703905   0.692093                  \n",
      "    71     0.645563   0.717831   0.677209                  \n",
      "    72     0.651157   0.706613   0.692093                  \n",
      "    73     0.654893   0.705729   0.693953                  \n",
      "    74     0.654819   0.705758   0.693953                  \n",
      "    75     0.647792   0.703402   0.682791                  \n",
      "    76     0.639365   0.703243   0.690233                  \n",
      "    77     0.631387   0.702788   0.695814                  \n",
      "    78     0.634629   0.704285   0.693953                  \n",
      "    79     0.638163   0.709014   0.688372                  \n",
      "    80     0.627115   0.70957    0.693953                  \n",
      "    81     0.626677   0.706604   0.696744                  \n",
      "    82     0.626876   0.699123   0.698605                  \n",
      "    83     0.632245   0.692268   0.706047                  \n",
      "    84     0.628717   0.701241   0.693953                  \n",
      "    85     0.627459   0.689831   0.699535                  \n",
      "    86     0.617795   0.698688   0.696744                  \n",
      "    87     0.621253   0.691544   0.693953                  \n",
      "    88     0.621694   0.691881   0.693953                  \n",
      "    89     0.619448   0.692689   0.698605                  \n",
      "    90     0.619067   0.697285   0.696744                  \n",
      "    91     0.624634   0.69945    0.699535                  \n",
      "    92     0.620587   0.697244   0.690233                  \n",
      "    93     0.623593   0.696979   0.696744                  \n",
      "    94     0.617712   0.691374   0.705116                  \n",
      "    95     0.61227    0.695917   0.705116                  \n",
      "    96     0.609172   0.686745   0.706047                  \n",
      "    97     0.60583    0.691165   0.695814                  \n",
      "    98     0.607973   0.692695   0.697674                  \n",
      "    99     0.603116   0.691508   0.703256                  \n",
      "   100     0.602975   0.693637   0.698605                  \n",
      "   101     0.600826   0.691455   0.689302                  \n",
      "   102     0.604383   0.681575   0.696744                  \n",
      "   103     0.605468   0.685654   0.698605                  \n",
      "   104     0.60509    0.689262   0.703256                  \n",
      "   105     0.605278   0.680051   0.704186                  \n",
      "   106     0.600949   0.682636   0.701395                  \n",
      "   107     0.596047   0.684229   0.701395                  \n",
      "   108     0.597872   0.693244   0.703256                  \n",
      "   109     0.59911    0.693823   0.706977                  \n",
      "   110     0.596537   0.696096   0.712558                  \n",
      "   111     0.593478   0.685161   0.702326                  \n",
      "   112     0.592176   0.679494   0.705116                  \n",
      "   113     0.59173    0.683346   0.705116                  \n",
      "   114     0.587515   0.69028    0.699535                  \n",
      "   115     0.586522   0.681925   0.708837                  \n",
      "   116     0.588966   0.694901   0.700465                  \n",
      "   117     0.589523   0.692326   0.703256                  \n",
      "   118     0.585977   0.688826   0.694884                  \n",
      "   119     0.582136   0.684808   0.705116                  \n",
      "   120     0.578039   0.684029   0.707907                  \n",
      "   121     0.58036    0.686714   0.704186                  \n",
      "   122     0.573528   0.678183   0.706047                  \n",
      "   123     0.579508   0.689715   0.706977                  \n",
      "   124     0.577273   0.682606   0.710698                  \n",
      "   125     0.573915   0.686763   0.701395                  \n",
      "   126     0.574568   0.684627   0.706047                  \n",
      "   127     0.571869   0.679299   0.702326                  \n",
      "   128     0.569742   0.679321   0.709767                  \n",
      "   129     0.566877   0.675229   0.709767                  \n",
      "   130     0.573396   0.675708   0.710698                  \n",
      "   131     0.566099   0.671584   0.706047                  \n",
      "   132     0.567984   0.674109   0.715349                  \n",
      "   133     0.566669   0.677654   0.708837                  \n",
      "   134     0.566299   0.687078   0.700465                  \n",
      "   135     0.562357   0.679976   0.706977                  \n",
      "   136     0.560327   0.68202    0.705116                  \n",
      "   137     0.559915   0.677082   0.704186                  \n",
      "   138     0.562681   0.678606   0.709767                  \n",
      "   139     0.560646   0.683382   0.704186                  \n",
      "   140     0.55744    0.681088   0.708837                  \n",
      "   141     0.55498    0.679571   0.708837                  \n",
      "   142     0.551316   0.672007   0.703256                  \n",
      "   143     0.550682   0.673472   0.712558                  \n",
      "   144     0.550303   0.68234    0.708837                  \n",
      "   145     0.547461   0.679238   0.712558                  \n",
      "   146     0.549129   0.682509   0.71814                   \n",
      "   147     0.547377   0.684908   0.704186                  \n",
      "   148     0.542372   0.67491    0.708837                  \n",
      "   149     0.543133   0.679793   0.710698                  \n",
      "   150     0.541117   0.674089   0.717209                  \n",
      "   151     0.546028   0.673598   0.717209                  \n",
      "   152     0.546935   0.674066   0.713488                  \n",
      "   153     0.538203   0.678494   0.707907                  \n",
      "   154     0.541834   0.6775     0.717209                  \n",
      "   155     0.543081   0.668293   0.71814                   \n",
      "   156     0.549359   0.670036   0.700465                  \n",
      "   157     0.549458   0.677394   0.706047                  \n",
      "   158     0.545823   0.676823   0.716279                  \n",
      "   159     0.538534   0.680476   0.712558                  \n",
      "   160     0.537756   0.681686   0.709767                  \n",
      "   161     0.539163   0.672274   0.704186                  \n",
      "   162     0.538623   0.678691   0.705116                  \n",
      "   163     0.532821   0.680182   0.716279                  \n",
      "   164     0.533541   0.677561   0.71907                   \n",
      "   165     0.531113   0.67171    0.715349                  \n",
      "   166     0.531158   0.673885   0.707907                  \n",
      "   167     0.529153   0.679351   0.711628                  \n",
      "   168     0.523546   0.669888   0.713488                  \n",
      "   169     0.52853    0.673932   0.712558                  \n",
      "   170     0.53019    0.676269   0.72                      \n",
      "   171     0.528831   0.676231   0.711628                  \n",
      "   172     0.529012   0.682392   0.717209                  \n",
      "   173     0.527426   0.680315   0.709767                  \n",
      "   174     0.526351   0.670392   0.712558                  \n",
      "   175     0.522369   0.66585    0.71907                   \n",
      "   176     0.519418   0.667266   0.71907                   \n",
      "   177     0.520681   0.663864   0.72093                   \n",
      "   178     0.529565   0.671713   0.717209                  \n",
      "   179     0.5253     0.674389   0.712558                  \n",
      "   180     0.524771   0.672818   0.72186                   \n",
      "   181     0.520802   0.674664   0.714419                  \n",
      "   182     0.515061   0.668213   0.717209                  \n",
      "   183     0.515401   0.670545   0.71814                   \n",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "v_a, learn = kfoldx_loop2(10,200,'_aug_3cls_10fldx.model', False, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79 0.09 0.12]                            \n",
      " [0.09 0.66 0.25]\n",
      " [0.05 0.17 0.77]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.load('1_aug_3cls_10fldx.model')\n",
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78 0.09 0.13]                            \n",
      " [0.1  0.68 0.22]\n",
      " [0.05 0.17 0.78]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.load('9_3cls_10fold.model')\n",
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### compare dropout values - 0, 0.2, 0.4, 0.6, 0.8, train twice each\n",
    "\n",
    "\n",
    "def kfld_loop3(k, epochs, name, precomp, bs, dropouts):\n",
    "    #dropoutlist = []\n",
    "    validation_accuracy = []\n",
    "    for do in dropouts:\n",
    "        for reps in range(k):\n",
    "            val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1)) # random 20% data for validation set\n",
    "            data = get_data(sz, bs)\n",
    "            print('dropout='+str(do))\n",
    "            learn = ConvLearner.pretrained(arch, data, precompute=precomp, ps = do)\n",
    "            val_loss, val_acc = learn.fit(1e-2, epochs)\n",
    "            validation_accuracy.append(val_acc)\n",
    "            learn.save(str(reps)+name)\n",
    "    return validation_accuracy, learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5491fad571fe47d7b183347dc96a5cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b033d5bb65048698997b979f74458db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.976561   0.89415    0.593488  \n",
      "    1      0.808688   0.791851   0.663256                 \n",
      "    2      0.70985    0.773325   0.670698                 \n",
      "    3      0.625495   0.739861   0.684651                 \n",
      "    4      0.556548   0.725481   0.696744                 \n",
      "    5      0.491422   0.722146   0.706047                 \n",
      "    6      0.435993   0.699665   0.697674                  \n",
      "    7      0.383896   0.710502   0.704186                  \n",
      "    8      0.34035    0.720761   0.72093                  \n",
      "    9      0.304428   0.714908   0.72186                  \n",
      "    10     0.277996   0.702172   0.717209                 \n",
      "    11     0.250573   0.754226   0.712558                  \n",
      "    12     0.229333   0.763109   0.72186                  \n",
      "    13     0.21233    0.782533   0.733953                 \n",
      "    14     0.200056   0.798922   0.712558                  \n",
      "    15     0.186432   0.843511   0.712558                  \n",
      "    16     0.177469   0.821396   0.723721                  \n",
      "    17     0.173282   0.828139   0.72186                   \n",
      "    18     0.170485   0.827716   0.701395                  \n",
      "    19     0.163535   0.837528   0.722791                  \n",
      "    20     0.162144   0.826945   0.716279                  \n",
      "    21     0.159513   0.881691   0.706977                  \n",
      "    22     0.155302   0.877262   0.71907                  \n",
      "    23     0.151232   0.84815    0.724651                 \n",
      "    24     0.145293   0.841897   0.731163                 \n",
      "    25     0.14206    0.872708   0.717209                 \n",
      "    26     0.141728   0.893696   0.726512                 \n",
      "    27     0.144104   0.860675   0.727442                 \n",
      "    28     0.142795   0.858538   0.731163                  \n",
      "    29     0.139764   0.880413   0.726512                 \n",
      "    30     0.134229   0.885547   0.723721                  \n",
      "    31     0.131681   0.874158   0.733023                 \n",
      "    32     0.130514   0.917688   0.72093                  \n",
      "    33     0.131849   0.862832   0.724651                 \n",
      "    34     0.133218   0.898269   0.715349                 \n",
      "    35     0.129368   0.873616   0.731163                 \n",
      "    36     0.1267     0.890121   0.727442                 \n",
      "    37     0.126711   0.890377   0.737674                 \n",
      "    38     0.129815   0.917133   0.72093                  \n",
      "    39     0.124315   0.878495   0.736744                 \n",
      "    40     0.1211     0.897614   0.737674                 \n",
      "    41     0.118776   0.871322   0.727442                 \n",
      "    42     0.120377   0.854543   0.730233                 \n",
      "    43     0.119753   0.916456   0.72186                  \n",
      "    44     0.117834   0.851369   0.731163                 \n",
      "    45     0.116888   0.902327   0.731163                 \n",
      "    46     0.115625   0.948659   0.722791                 \n",
      "    47     0.113762   0.895372   0.723721                 \n",
      "    48     0.114129   0.904219   0.736744                 \n",
      "    49     0.114298   0.909703   0.732093                 \n",
      "    50     0.114789   0.9014     0.733023                 \n",
      "    51     0.113882   0.890411   0.733023                 \n",
      "    52     0.113219   0.895668   0.723721                 \n",
      "    53     0.110454   0.960603   0.72                     \n",
      "    54     0.110988   0.898247   0.738605                 \n",
      "    55     0.109935   0.874036   0.735814                 \n",
      "    56     0.109092   0.928768   0.733023                 \n",
      "    57     0.108528   0.890983   0.742326                 \n",
      "    58     0.109422   0.901966   0.735814                 \n",
      "    59     0.107239   0.934726   0.726512                 \n",
      "    60     0.108834   0.920997   0.732093                 \n",
      "    61     0.107695   0.934164   0.729302                  \n",
      "    62     0.107502   0.910715   0.732093                 \n",
      "    63     0.106277   0.920097   0.726512                 \n",
      "    64     0.10555    0.95089    0.715349                 \n",
      "    65     0.10457    0.900131   0.733023                 \n",
      "    66     0.106331   0.993377   0.715349                  \n",
      "    67     0.106207   0.92233    0.728372                 \n",
      "    68     0.107317   0.920375   0.731163                  \n",
      "    69     0.107572   0.911822   0.736744                 \n",
      "    70     0.110383   0.899919   0.724651                 \n",
      "    71     0.10695    0.905328   0.727442                 \n",
      "    72     0.107158   0.944467   0.725581                 \n",
      "    73     0.106812   0.93339    0.746977                 \n",
      "    74     0.105029   0.922839   0.734884                 \n",
      "    75     0.104829   0.925383   0.735814                 \n",
      "    76     0.108586   0.925215   0.733023                 \n",
      "    77     0.106421   0.917692   0.733023                 \n",
      "    78     0.1053     0.9018     0.733023                 \n",
      "    79     0.106618   0.932359   0.727442                 \n",
      "    80     0.105434   0.940012   0.735814                 \n",
      "    81     0.104903   0.929584   0.745116                 \n",
      "    82     0.104772   0.92856    0.728372                 \n",
      "    83     0.102523   0.91533    0.731163                  \n",
      "    84     0.104925   0.929917   0.736744                  \n",
      "    85     0.105262   0.918368   0.726512                  \n",
      "    86     0.102251   0.915501   0.738605                  \n",
      "    87     0.099062   0.926932   0.734884                   \n",
      "    88     0.099006   0.935773   0.734884                  \n",
      "    89     0.098609   0.96347    0.729302                  \n",
      "    90     0.098764   0.918005   0.732093                  \n",
      "    91     0.100286   0.923242   0.726512                  \n",
      "    92     0.099689   0.90839    0.740465                  \n",
      "    93     0.099249   0.906965   0.727442                  \n",
      "    94     0.098046   0.912615   0.738605                  \n",
      "    95     0.097581   0.92274    0.736744                  \n",
      "    96     0.096714   0.927474   0.733953                  \n",
      "    97     0.099546   0.941742   0.733953                  \n",
      "    98     0.097637   0.901843   0.728372                  \n",
      "    99     0.09523    0.930281   0.740465                  \n",
      "   100     0.097638   0.923937   0.738605                  \n",
      "   101     0.098381   0.958522   0.728372                  \n",
      "   102     0.097766   0.929837   0.726512                  \n",
      "   103     0.097722   0.91342    0.733953                  \n",
      "   104     0.097803   0.960098   0.72186                   \n",
      "   105     0.099385   0.988975   0.722791                  \n",
      "   106     0.100755   0.944841   0.730233                  \n",
      "   107     0.098243   0.94336    0.730233                  \n",
      "   108     0.099141   0.909024   0.741395                  \n",
      "   109     0.099763   0.928948   0.734884                  \n",
      "   110     0.098705   0.97853    0.725581                  \n",
      "   111     0.096413   0.939197   0.729302                  \n",
      "   112     0.09354    0.931738   0.729302                 \n",
      "   113     0.091964   0.947726   0.735814                  \n",
      "   114     0.091708   0.937379   0.736744                  \n",
      "   115     0.092013   0.941547   0.739535                  \n",
      "   116     0.093146   0.947454   0.729302                  \n",
      "   117     0.092056   0.936928   0.736744                  \n",
      "   118     0.091894   0.930146   0.742326                  \n",
      "   119     0.093506   0.948045   0.732093                  \n",
      "   120     0.094251   0.940719   0.733023                  \n",
      "   121     0.092422   0.938551   0.738605                  \n",
      "   122     0.091776   0.934002   0.733953                  \n",
      "   123     0.09078    0.942608   0.739535                  \n",
      "   124     0.092676   0.958257   0.740465                  \n",
      "   125     0.092392   0.934147   0.736744                  \n",
      "   126     0.090836   0.939458   0.735814                  \n",
      "   127     0.090235   0.941577   0.733023                  \n",
      "   128     0.089423   0.945705   0.730233                  \n",
      "   129     0.090314   0.951151   0.737674                  \n",
      "   130     0.090918   0.948401   0.731163                  \n",
      "   131     0.091271   0.946326   0.736744                  \n",
      "   132     0.091946   0.983628   0.723721                  \n",
      "   133     0.091288   0.981212   0.730233                  \n",
      "   134     0.093375   0.988493   0.724651                  \n",
      "   135     0.093607   0.96541    0.734884                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   136     0.095117   0.954761   0.734884                  \n",
      "   137     0.093233   0.941975   0.732093                  \n",
      "   138     0.093606   0.989867   0.730233                  \n",
      "   139     0.092938   0.955021   0.734884                  \n",
      "   140     0.091096   0.943311   0.733953                  \n",
      "   141     0.090876   0.950743   0.735814                  \n",
      "   142     0.090045   0.932722   0.739535                  \n",
      "   143     0.090288   0.955786   0.737674                  \n",
      "   144     0.089594   0.956105   0.731163                  \n",
      "   145     0.091193   0.942268   0.738605                  \n",
      "   146     0.090954   0.971752   0.731163                  \n",
      "   147     0.091107   0.945832   0.733954                  \n",
      "   148     0.091138   0.967122   0.735814                  \n",
      "   149     0.091536   0.993608   0.731163                  \n",
      "   150     0.09058    0.961263   0.730233                  \n",
      "   151     0.092835   0.971904   0.732093                  \n",
      "   152     0.091382   0.945454   0.735814                  \n",
      "   153     0.091956   0.972316   0.737674                  \n",
      "   154     0.090607   0.954284   0.733023                  \n",
      "   155     0.090328   0.9411     0.738605                  \n",
      "   156     0.088322   0.956412   0.739535                  \n",
      "   157     0.089415   0.953399   0.735814                  \n",
      "   158     0.089481   0.962223   0.737674                   \n",
      "   159     0.089536   0.969189   0.735814                  \n",
      "   160     0.089082   0.968183   0.733953                  \n",
      "   161     0.090563   0.989598   0.736744                 \n",
      "   162     0.09057    0.975288   0.736744                  \n",
      "   163     0.088325   0.967363   0.727442                  \n",
      "   164     0.089443   0.966158   0.732093                  \n",
      "   165     0.08965    1.010412   0.727442                  \n",
      "   166     0.089083   0.973247   0.740465                  \n",
      "   167     0.088607   0.939925   0.738605                  \n",
      "   168     0.091186   0.961991   0.739535                  \n",
      "   169     0.089955   0.951037   0.734884                  \n",
      "   170     0.089969   0.943723   0.744186                  \n",
      "   171     0.090839   0.956961   0.735814                  \n",
      "   172     0.088882   1.001442   0.726512                  \n",
      "   173     0.088242   0.952397   0.739535                  \n",
      "   174     0.088317   0.967981   0.740465                  \n",
      "   175     0.087607   0.955257   0.740465                  \n",
      "   176     0.085725   0.995496   0.735814                  \n",
      "   177     0.085092   0.966941   0.737674                  \n",
      "   178     0.084937   0.971354   0.734884                  \n",
      "   179     0.08417    0.979274   0.740465                  \n",
      "   180     0.084448   0.978214   0.743256                  \n",
      "   181     0.083978   0.944063   0.742326                  \n",
      "   182     0.084827   0.970499   0.733953                  \n",
      "   183     0.086725   0.966668   0.740465                  \n",
      "   184     0.085845   1.008388   0.733953                  \n",
      "   185     0.085548   0.979823   0.738605                  \n",
      "   186     0.087613   0.983978   0.737674                  \n",
      "   187     0.086954   0.979452   0.737674                  \n",
      "   188     0.086966   0.972975   0.741395                  \n",
      "   189     0.087571   0.966702   0.740465                  \n",
      "   190     0.086551   0.963476   0.738605                  \n",
      "   191     0.085327   0.970729   0.735814                   \n",
      "   192     0.084052   0.99314    0.742326                  \n",
      "   193     0.084043   0.969138   0.735814                  \n",
      "   194     0.084831   0.958072   0.735814                  \n",
      "   195     0.085949   1.010529   0.729302                  \n",
      "   196     0.088056   0.969787   0.733023                  \n",
      "   197     0.086521   0.957967   0.742326                  \n",
      "   198     0.087464   0.964392   0.735814                  \n",
      "   199     0.087532   0.961651   0.741395                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9271d3261f6143338a4f6545e874a89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4986851c3007473b9d6cb928e01e0655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.967924   0.886246   0.587907  \n",
      "    1      0.805155   0.814728   0.63814                  \n",
      "    2      0.701167   0.751001   0.666047                 \n",
      "    3      0.6177     0.731335   0.684651                 \n",
      "    4      0.548655   0.750697   0.702326                 \n",
      "    5      0.491624   0.782631   0.68                     \n",
      "    6      0.439223   0.710653   0.713488                 \n",
      "    7      0.388512   0.722166   0.707907                 \n",
      "    8      0.34377    0.752126   0.701395                 \n",
      "    9      0.307103   0.771196   0.708837                  \n",
      "    10     0.274356   0.790897   0.71814                  \n",
      "    11     0.247958   0.798353   0.705116                 \n",
      "    12     0.23029    0.827776   0.693023                 \n",
      "    13     0.210442   0.816744   0.706977                  \n",
      "    14     0.196574   0.804717   0.707907                 \n",
      "    15     0.18765    0.831259   0.709767                 \n",
      "    16     0.179434   0.847032   0.71907                  \n",
      "    17     0.175473   0.875302   0.703256                 \n",
      "    18     0.167796   0.86699    0.713488                 \n",
      "    19     0.163485   0.878929   0.706977                 \n",
      "    20     0.158607   0.929926   0.705116                 \n",
      "    21     0.157393   0.882183   0.722791                 \n",
      "    22     0.152196   0.886904   0.733023                 \n",
      "    23     0.146941   0.943048   0.708837                  \n",
      "    24     0.143299   0.919485   0.714419                 \n",
      "    25     0.14371    0.91156    0.716279                 \n",
      "    26     0.1398     0.944875   0.703256                 \n",
      "    27     0.137939   0.892253   0.728372                 \n",
      "    28     0.137356   0.884835   0.728372                 \n",
      "    29     0.133674   0.914903   0.72186                  \n",
      "    30     0.131616   0.878147   0.72093                  \n",
      "    31     0.131947   0.968186   0.686512                 \n",
      "    32     0.130811   0.963507   0.702326                 \n",
      "    33     0.131296   0.932412   0.711628                 \n",
      "    34     0.127526   0.99686    0.701395                 \n",
      "    35     0.127131   0.992367   0.715349                 \n",
      "    36     0.125238   0.959595   0.72186                  \n",
      "    37     0.125286   0.976073   0.701395                  \n",
      "    38     0.125051   0.951922   0.705116                 \n",
      "    39     0.126398   0.924723   0.734884                  \n",
      "    40     0.129384   0.966116   0.72186                   \n",
      "    41     0.126163   0.931533   0.726512                  \n",
      "    42     0.123586   0.939856   0.717209                  \n",
      "    43     0.124332   0.956543   0.715349                  \n",
      "    44     0.121791   0.919866   0.717209                  \n",
      "    45     0.121115   0.952422   0.710698                  \n",
      "    46     0.118789   0.930007   0.722791                  \n",
      "    47     0.116298   0.957857   0.712558                  \n",
      "    48     0.116708   0.9191     0.716279                  \n",
      "    49     0.116422   0.953084   0.733953                  \n",
      "    50     0.115866   0.93865    0.725581                 \n",
      "    51     0.113274   0.96143    0.72                     \n",
      "    52     0.111967   0.9812     0.717209                 \n",
      "    53     0.110867   0.949686   0.722791                 \n",
      "    54     0.109401   0.961268   0.726512                  \n",
      "    55     0.106911   0.972498   0.72093                   \n",
      "    56     0.106983   0.948919   0.71907                   \n",
      "    57     0.105933   0.966304   0.726512                  \n",
      "    58     0.106033   0.963326   0.72                      \n",
      "    59     0.10736    0.97538    0.725581                  \n",
      "    60     0.107979   0.971499   0.723721                 \n",
      "    61     0.107795   0.977618   0.72186                  \n",
      "    62     0.109148   0.958978   0.72093                  \n",
      "    63     0.106441   0.957259   0.716279                 \n",
      "    64     0.103768   0.982384   0.731163                  \n",
      "    65     0.10375    0.942241   0.712558                  \n",
      "    66     0.10428    0.958232   0.717209                  \n",
      "    67     0.102836   0.976103   0.725581                 \n",
      "    68     0.101726   0.978215   0.725581                 \n",
      "    69     0.104132   0.996578   0.713488                  \n",
      "    70     0.103507   0.985957   0.715349                  \n",
      "    71     0.100771   0.97712    0.714419                  \n",
      "    72     0.102779   0.970834   0.72093                   \n",
      "    73     0.101084   0.961502   0.72093                  \n",
      "    74     0.100735   0.972239   0.722791                   \n",
      "    75     0.101556   0.994555   0.727442                  \n",
      "    76     0.102759   0.962676   0.71814                   \n",
      "    77     0.10288    0.983073   0.708837                  \n",
      "    78     0.10068    0.987832   0.729302                 \n",
      "    79     0.10037    0.98671    0.723721                  \n",
      "    80     0.099702   1.000734   0.728372                  \n",
      "    81     0.100348   0.976105   0.716279                 \n",
      "    82     0.101065   0.979999   0.722791                  \n",
      "    83     0.100122   0.978556   0.716279                  \n",
      "    84     0.098348   0.969556   0.72186                   \n",
      "    85     0.099137   0.976199   0.711628                  \n",
      "    86     0.097471   0.994719   0.715349                   \n",
      "    87     0.097052   1.009281   0.72186                    \n",
      "    88     0.097694   0.972089   0.723721                   \n",
      "    89     0.097083   0.982925   0.72                       \n",
      "    90     0.097643   0.960053   0.71814                    \n",
      "    91     0.098523   0.990805   0.722791                  \n",
      "    92     0.098242   1.005054   0.71907                   \n",
      "    93     0.097393   1.004476   0.713488                  \n",
      "    94     0.09777    0.964243   0.722791                  \n",
      "    95     0.099676   1.022392   0.72                      \n",
      "    96     0.098457   0.979306   0.713488                  \n",
      "    97     0.099155   0.967854   0.72093                   \n",
      "    98     0.09894    1.000399   0.71907                   \n",
      "    99     0.098611   1.017851   0.713488                  \n",
      "   100     0.097627   0.984807   0.71814                   \n",
      "   101     0.097893   0.984238   0.722791                  \n",
      "   102     0.095313   0.991486   0.717209                  \n",
      "   103     0.0943     1.010976   0.727442                  \n",
      "   104     0.095133   0.981303   0.713488                  \n",
      "   105     0.095096   1.035339   0.733953                  \n",
      "   106     0.093841   0.988817   0.707907                  \n",
      "   107     0.094481   0.97897    0.723721                  \n",
      "   108     0.095102   1.001944   0.72093                   \n",
      "   109     0.094756   1.022727   0.723721                  \n",
      "   110     0.097174   1.017462   0.724651                  \n",
      "   111     0.09789    1.050843   0.728372                  \n",
      "   112     0.098667   1.013532   0.71907                   \n",
      "   113     0.100223   0.995572   0.713488                 \n",
      "   114     0.097598   0.97851    0.731163                  \n",
      "   115     0.095656   1.018714   0.726512                  \n",
      "   116     0.095329   1.01744    0.717209                  \n",
      "   117     0.093353   0.990107   0.723721                  \n",
      "   118     0.093163   0.994578   0.72093                   \n",
      "   119     0.092158   0.977619   0.732093                  \n",
      "   120     0.091752   1.011287   0.72186                   \n",
      "   121     0.090949   1.028222   0.72                      \n",
      "   122     0.089957   0.995605   0.729302                  \n",
      "   123     0.09012    0.99533    0.71907                   \n",
      "   124     0.09012    1.003009   0.727442                  \n",
      "   125     0.090511   0.996303   0.716279                  \n",
      "   126     0.090483   1.018621   0.722791                  \n",
      "   127     0.091676   1.037052   0.716279                  \n",
      "   128     0.092934   1.036464   0.724651                  \n",
      "   129     0.093194   1.023125   0.726512                  \n",
      "   130     0.091026   0.992132   0.72093                   \n",
      "   131     0.088778   0.992691   0.729302                  \n",
      "   132     0.089477   1.01574    0.734884                  \n",
      "   133     0.088291   1.023106   0.72186                   \n",
      "   134     0.089694   1.014569   0.733953                  \n",
      "   135     0.089503   1.031637   0.72186                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   136     0.089979   1.038088   0.726512                  \n",
      "   137     0.08956    1.020883   0.727442                  \n",
      "   138     0.088206   0.991515   0.71907                   \n",
      "   139     0.09006    1.007406   0.722791                  \n",
      "   140     0.089794   1.015875   0.72093                   \n",
      "   141     0.088377   1.023298   0.722791                  \n",
      "   142     0.089454   0.995448   0.71907                    \n",
      "   143     0.089587   1.021442   0.71907                    \n",
      "   144     0.089415   1.008898   0.728372                  \n",
      "   145     0.092241   1.006803   0.731163                   \n",
      "   146     0.093292   1.027898   0.726512                  \n",
      "   147     0.092399   0.994664   0.723721                  \n",
      "   148     0.090966   1.029633   0.705116                  \n",
      "   149     0.089337   1.029756   0.725581                  \n",
      "   150     0.090238   1.016605   0.732093                  \n",
      "   151     0.091534   0.999423   0.728372                  \n",
      "   152     0.08906    1.04401    0.715349                  \n",
      "   153     0.089508   1.040701   0.728372                  \n",
      "   154     0.091655   0.998391   0.725581                  \n",
      "   155     0.088793   1.026261   0.726512                  \n",
      "   156     0.088449   1.000175   0.72                      \n",
      "   157     0.088156   1.040517   0.717209                  \n",
      "   158     0.088417   1.028114   0.715349                  \n",
      "   159     0.088941   1.017664   0.727442                  \n",
      "   160     0.088147   1.026499   0.72                       \n",
      "   161     0.088219   1.021004   0.72093                    \n",
      "   162     0.08646    1.038087   0.731163                   \n",
      "   163     0.088091   1.037598   0.711628                  \n",
      "   164     0.087442   1.034222   0.72                       \n",
      "   165     0.086615   1.031274   0.72093                   \n",
      "   166     0.084407   1.075663   0.71814                    \n",
      "   167     0.08595    1.035981   0.728372                   \n",
      "   168     0.08732    1.029832   0.726512                   \n",
      "   169     0.087748   1.043991   0.723721                  \n",
      "   170     0.087157   1.038649   0.725581                  \n",
      "   171     0.086232   1.094952   0.709767                  \n",
      "   172     0.087574   1.042052   0.732093                  \n",
      "   173     0.086731   1.036994   0.729302                   \n",
      "   174     0.087407   1.024592   0.71907                   \n",
      "   175     0.087109   1.017144   0.71814                    \n",
      "   176     0.086507   1.018613   0.732093                  \n",
      "   177     0.087246   1.040208   0.71814                    \n",
      "   178     0.08759    1.026161   0.727442                  \n",
      "   179     0.088847   1.053272   0.71814                    \n",
      "   180     0.087426   1.018089   0.726512                   \n",
      "   181     0.087454   1.032007   0.71907                    \n",
      "   182     0.087811   1.033215   0.728372                  \n",
      "   183     0.087372   1.048581   0.72093                   \n",
      "   184     0.087949   1.055715   0.722791                   \n",
      "   185     0.08801    1.032389   0.722791                  \n",
      "   186     0.086432   1.036982   0.727442                  \n",
      "   187     0.086114   1.036754   0.726512                  \n",
      "   188     0.08491    1.025095   0.726512                   \n",
      "   189     0.084875   1.059363   0.731163                   \n",
      "   190     0.085448   1.054262   0.723721                  \n",
      "   191     0.085087   1.035584   0.717209                   \n",
      "   192     0.088383   1.042248   0.733953                  \n",
      "   193     0.08769    1.025351   0.72                       \n",
      "   194     0.085369   1.019997   0.723721                   \n",
      "   195     0.084314   1.043341   0.723721                  \n",
      "   196     0.083193   1.01755    0.723721                  \n",
      "   197     0.085051   1.071276   0.725581                  \n",
      "   198     0.085938   1.032384   0.71907                   \n",
      "   199     0.086028   1.031454   0.71814                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54ec849f16245b69a268a73cf5edf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb0b9170cae4b0e8c898c5b6cef9d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      0.958261   0.884483   0.592558  \n",
      "    1      0.805154   0.789172   0.658605                 \n",
      "    2      0.700371   0.754372   0.686512                 \n",
      "    3      0.619522   0.782857   0.672558                 \n",
      "    4      0.543764   0.713626   0.702326                  \n",
      "    5      0.480376   0.745043   0.686512                 \n",
      "    6      0.42608    0.738548   0.709767                  \n",
      "    7      0.376295   0.772442   0.697674                  \n",
      "    8      0.336189   0.759992   0.716279                 \n",
      "    9      0.298836   0.756349   0.717209                  \n",
      "    10     0.268499   0.785536   0.725581                 \n",
      "    11     0.243611   0.787404   0.710698                 \n",
      "    12     0.224611   0.811039   0.72                     \n",
      "    13     0.20897    0.813631   0.709767                 \n",
      "    14     0.200493   0.815753   0.711628                 \n",
      "    15     0.190438   0.785035   0.731163                  \n",
      "    16     0.176742   0.810001   0.726512                 \n",
      "    17     0.169129   0.851054   0.724651                 \n",
      "    18     0.164868   0.873448   0.707907                 \n",
      "    19     0.162843   0.857205   0.726512                 \n",
      "    20     0.160235   0.904387   0.733953                  \n",
      "    21     0.15449    0.892059   0.724651                 \n",
      "    22     0.155519   0.876405   0.730233                 \n",
      "    23     0.152112   0.909863   0.734884                 \n",
      "    24     0.152457   0.934323   0.708837                 \n",
      "    25     0.147895   0.919199   0.716279                  \n",
      "    26     0.146048   0.912853   0.709767                 \n",
      "    27     0.144414   0.933182   0.715349                  \n",
      "    28     0.142078   0.888901   0.72093                   \n",
      "    29     0.139966   0.954136   0.711628                  \n",
      "    30     0.135457   0.92563    0.717209                  \n",
      "    31     0.136447   0.944447   0.716279                  \n",
      "    32     0.133834   0.879245   0.725581                  \n",
      "    33     0.13109    0.9621     0.722791                  \n",
      "    34     0.131286   0.920702   0.726512                 \n",
      "    35     0.134855   0.878827   0.730233                 \n",
      "    36     0.134213   0.974166   0.710698                  \n",
      "    37     0.133287   0.927807   0.729302                  \n",
      "    38     0.12894    0.950616   0.724651                  \n",
      "    39     0.127216   0.903996   0.732093                  \n",
      "    40     0.126697   0.907137   0.727442                 \n",
      "    41     0.123457   0.969065   0.723721                 \n",
      "    42     0.120514   0.910447   0.727442                 \n",
      "    43     0.121959   0.898602   0.725581                 \n",
      "    44     0.119811   0.913905   0.736744                 \n",
      "    45     0.118488   0.927404   0.739535                 \n",
      "    46     0.118542   0.907524   0.713488                 \n",
      "    47     0.119031   0.918466   0.723721                 \n",
      "    48     0.120911   0.935633   0.72                     \n",
      "    49     0.12089    0.942752   0.71907                  \n",
      "    50     0.116552   0.98021    0.72                      \n",
      "    51     0.114552   0.921698   0.730233                 \n",
      "    52     0.110778   0.910872   0.72093                   \n",
      "    53     0.111684   0.917609   0.728372                 \n",
      "    54     0.113409   0.934624   0.727442                 \n",
      "    55     0.113592   0.928876   0.733953                 \n",
      "    56     0.113054   0.919133   0.729302                 \n",
      "    57     0.112849   0.962011   0.725581                 \n",
      "    58     0.112691   0.916328   0.72093                  \n",
      "    59     0.111786   0.946941   0.733953                 \n",
      "    60     0.110498   0.94078    0.723721                 \n",
      "    61     0.108083   0.953667   0.732093                 \n",
      "    62     0.107389   0.961826   0.727442                 \n",
      "    63     0.10442    1.010768   0.72                     \n",
      "    64     0.10786    0.945735   0.723721                 \n",
      "    65     0.106      0.938986   0.730233                  \n",
      "    66     0.106916   0.933323   0.733953                 \n",
      "    67     0.106293   0.95243    0.729302                  \n",
      "    68     0.104389   0.956875   0.735814                  \n",
      "    69     0.104081   0.944839   0.72186                  \n",
      "    70     0.10438    0.97646    0.728372                  \n",
      "    71     0.104315   0.944308   0.725581                 \n",
      "    72     0.101824   0.94872    0.734884                 \n",
      "    73     0.10223    0.975068   0.724651                  \n",
      "    74     0.100397   0.957995   0.732093                 \n",
      "    75     0.102163   0.956627   0.729302                   \n",
      "    76     0.101545   0.958375   0.733953                  \n",
      "    77     0.106209   0.95549    0.731163                 \n",
      "    78     0.104459   0.926835   0.733953                 \n",
      "    79     0.100939   0.991439   0.724651                 \n",
      "    80     0.101824   0.95105    0.726512                 \n",
      "    81     0.102298   0.934473   0.730233                 \n",
      "    82     0.101691   0.970253   0.727442                 \n",
      "    83     0.100387   0.948174   0.727442                 \n",
      "    84     0.100866   0.947636   0.730233                  \n",
      "    85     0.100453   0.968316   0.733023                  \n",
      "    86     0.102104   0.930755   0.735814                  \n",
      "    87     0.101846   0.966555   0.732093                  \n",
      "    88     0.103063   0.964466   0.732093                 \n",
      "    89     0.1034     0.992838   0.726512                 \n",
      "    90     0.101953   0.936794   0.726512                  \n",
      "    91     0.100031   0.963898   0.730233                  \n",
      "    92     0.09853    0.987674   0.730233                  \n",
      "    93     0.099214   0.936266   0.732093                  \n",
      "    94     0.099004   0.94578    0.737674                  \n",
      "    95     0.101962   0.953509   0.726512                  \n",
      "    96     0.09961    0.944089   0.729302                  \n",
      "    97     0.098194   0.955938   0.733023                  \n",
      "    98     0.09614    0.941469   0.734884                  \n",
      "    99     0.096771   0.938124   0.728372                  \n",
      "   100     0.095918   0.964343   0.72186                   \n",
      "   101     0.094301   0.949769   0.735814                  \n",
      "   102     0.095592   0.962981   0.729302                  \n",
      "   103     0.094803   0.98513    0.733023                  \n",
      "   104     0.094209   0.986944   0.727442                  \n",
      "   105     0.094192   0.938994   0.738605                   \n",
      "   106     0.094712   0.988946   0.724651                   \n",
      "   107     0.093899   1.003103   0.723721                  \n",
      "   108     0.093366   0.976499   0.731163                  \n",
      "   109     0.09387    0.966906   0.729302                  \n",
      "   110     0.095168   0.98483    0.72186                    \n",
      "   111     0.095451   0.989394   0.728372                   \n",
      "   112     0.096231   0.982763   0.728372                   \n",
      "   113     0.098393   0.962656   0.724651                   \n",
      "   114     0.097539   0.98136    0.726512                   \n",
      "   115     0.097451   0.947966   0.738605                  \n",
      "   116     0.09519    0.976398   0.723721                  \n",
      "   117     0.092203   0.95699    0.732093                  \n",
      "   118     0.094248   0.945189   0.737674                  \n",
      "   119     0.094604   0.979646   0.722791                   \n",
      "   120     0.095363   0.959077   0.730233                   \n",
      "   121     0.099541   0.973332   0.724651                   \n",
      "   122     0.100518   0.957906   0.735814                  \n",
      "   123     0.098243   0.956036   0.730233                  \n",
      "   124     0.096664   0.963613   0.730233                  \n",
      "   125     0.096054   0.957242   0.735814                  \n",
      "   126     0.0952     0.953836   0.737674                  \n",
      "   127     0.094707   0.969575   0.733023                  \n",
      "   128     0.093998   0.960371   0.733953                   \n",
      "   129     0.092629   0.969876   0.727442                   \n",
      "   130     0.094073   0.987223   0.729302                  \n",
      "   131     0.09577    1.017906   0.727442                   \n",
      "   132     0.095164   0.996011   0.736744                   \n",
      "   133     0.092425   0.989624   0.725581                  \n",
      "   134     0.094318   0.984495   0.723721                  \n",
      "   135     0.093473   0.974494   0.729302                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   136     0.092077   0.968265   0.729302                   \n",
      "   137     0.091319   0.966185   0.735814                   \n",
      "   138     0.092402   0.991395   0.729302                  \n",
      "   139     0.091438   0.961198   0.733953                   \n",
      "   140     0.091127   0.973941   0.732093                  \n",
      "   141     0.089884   0.957743   0.736744                  \n",
      "   142     0.087655   0.981176   0.727442                  \n",
      "   143     0.08882    0.967728   0.733023                  \n",
      "   144     0.088077   0.989076   0.730233                  \n",
      "   145     0.087182   0.995062   0.729302                  \n",
      "   146     0.088314   0.975528   0.732093                   \n",
      "   147     0.089485   1.014566   0.731163                   \n",
      "   148     0.090059   1.008623   0.72186                    \n",
      "   149     0.089471   1.005679   0.725581                   \n",
      "   150     0.088332   1.006471   0.729302                   \n",
      "   151     0.088388   0.992594   0.737674                  \n",
      "   152     0.087618   0.976936   0.732093                  \n",
      "   153     0.086879   0.989737   0.733953                  \n",
      "   154     0.086965   0.9899     0.729302                   \n",
      "   155     0.08866    0.989523   0.733953                   \n",
      "   156     0.087329   0.993598   0.732093                   \n",
      "   157     0.088422   0.984958   0.731163                   \n",
      "   158     0.088715   0.981776   0.734884                  \n",
      "   159     0.08816    0.97531    0.731163                  \n",
      "   160     0.087866   0.989042   0.735814                  \n",
      "   161     0.087442   0.985152   0.729302                  \n",
      "   162     0.088788   0.988474   0.735814                  \n",
      "   163     0.08815    0.987779   0.733023                   \n",
      "   164     0.088668   1.020539   0.723721                   \n",
      "   165     0.088509   0.994944   0.735814                  \n",
      "   166     0.08755    1.003081   0.730233                  \n",
      "   167     0.087913   0.976787   0.729302                  \n",
      "   168     0.087592   0.9819     0.734884                  \n",
      "   169     0.087582   0.995833   0.727442                  \n",
      "   170     0.086768   0.995284   0.731163                   \n",
      "   171     0.0872     0.979597   0.731163                   \n",
      "   172     0.086092   1.012861   0.728372                  \n",
      "   173     0.086981   0.987298   0.733023                  \n",
      "   174     0.086646   1.003884   0.732093                  \n",
      "   175     0.08701    1.001167   0.733953                  \n",
      "   176     0.089165   0.993528   0.735814                   \n",
      "   177     0.087555   0.976822   0.736744                  \n",
      "   178     0.087928   1.006997   0.729302                  \n",
      "   179     0.088539   0.98362    0.733953                  \n",
      "   180     0.087456   1.004712   0.728372                  \n",
      "   181     0.08715    0.978225   0.732093                  \n",
      "   182     0.087832   0.978225   0.736744                   \n",
      "   183     0.086754   0.965856   0.734884                   \n",
      "   184     0.08603    0.994417   0.730233                   \n",
      "   185     0.084597   0.985292   0.730233                  \n",
      "   186     0.085762   0.99759    0.730233                   \n",
      "   187     0.086839   0.996108   0.741395                   \n",
      "   188     0.088526   0.996273   0.735814                  \n",
      "   189     0.087591   1.001322   0.730233                  \n",
      "   190     0.08812    0.974172   0.736744                  \n",
      "   191     0.087836   0.983369   0.735814                  \n",
      "   192     0.087104   1.017973   0.729302                  \n",
      "   193     0.08716    0.991345   0.730233                  \n",
      "   194     0.087016   1.013127   0.728372                  \n",
      "   195     0.088332   0.992772   0.731163                  \n",
      "   196     0.089349   0.989911   0.722791                  \n",
      "   197     0.087872   0.985691   0.731163                  \n",
      "   198     0.086684   0.978997   0.736744                  \n",
      "   199     0.08846    1.024644   0.727442                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71ca9776017479e80510a6994adb14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d5264317a54018b14376938adb119a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.048344   0.904908   0.589767  \n",
      "    1      0.903681   0.825887   0.63907                   \n",
      "    2      0.813236   0.803121   0.644651                  \n",
      "    3      0.750448   0.760781   0.668837                 \n",
      "    4      0.69624    0.759492   0.668837                  \n",
      "    5      0.651102   0.747888   0.670698                  \n",
      "    6      0.613241   0.722921   0.705116                  \n",
      "    7      0.582077   0.708255   0.696744                  \n",
      "    8      0.547954   0.718797   0.714419                 \n",
      "    9      0.525183   0.706989   0.701395                  \n",
      "    10     0.496101   0.705474   0.717209                 \n",
      "    11     0.474321   0.705128   0.715349                 \n",
      "    12     0.454314   0.709777   0.72186                   \n",
      "    13     0.43422    0.715614   0.717209                  \n",
      "    14     0.415985   0.716511   0.715349                 \n",
      "    15     0.405369   0.696714   0.701395                 \n",
      "    16     0.388439   0.719997   0.715349                 \n",
      "    17     0.374107   0.704039   0.722791                 \n",
      "    18     0.358484   0.698784   0.728372                 \n",
      "    19     0.348393   0.711859   0.731163                 \n",
      "    20     0.335331   0.727545   0.732093                  \n",
      "    21     0.329524   0.713842   0.71907                  \n",
      "    22     0.320734   0.719582   0.725581                 \n",
      "    23     0.309839   0.763393   0.72                      \n",
      "    24     0.304128   0.749082   0.723721                  \n",
      "    25     0.291129   0.737833   0.727442                  \n",
      "    26     0.283186   0.742866   0.732093                  \n",
      "    27     0.270803   0.756465   0.72                      \n",
      "    28     0.269568   0.764385   0.734884                 \n",
      "    29     0.267327   0.763306   0.742326                  \n",
      "    30     0.26242    0.757484   0.741395                  \n",
      "    31     0.258027   0.77901    0.734884                 \n",
      "    32     0.254847   0.776737   0.729302                 \n",
      "    33     0.24596    0.82458    0.71814                  \n",
      "    34     0.244217   0.781185   0.728372                 \n",
      "    35     0.239637   0.77856    0.725581                 \n",
      "    36     0.234763   0.811541   0.727442                  \n",
      "    37     0.232224   0.76409    0.735814                 \n",
      "    38     0.227827   0.811935   0.725581                 \n",
      "    39     0.228217   0.799323   0.741395                 \n",
      "    40     0.228105   0.81736    0.736744                 \n",
      "    41     0.224139   0.803092   0.728372                  \n",
      "    42     0.22243    0.79684    0.739535                  \n",
      "    43     0.213929   0.782036   0.744186                  \n",
      "    44     0.209235   0.814095   0.742326                  \n",
      "    45     0.206912   0.794085   0.733023                 \n",
      "    46     0.204603   0.830333   0.725581                  \n",
      "    47     0.2018     0.817023   0.737674                  \n",
      "    48     0.202751   0.820282   0.733953                 \n",
      "    49     0.198162   0.846545   0.724651                 \n",
      "    50     0.195292   0.810317   0.723721                 \n",
      "    51     0.197686   0.859112   0.728372                  \n",
      "    52     0.197064   0.830116   0.738605                 \n",
      "    53     0.195648   0.821257   0.735814                  \n",
      "    54     0.190334   0.851262   0.734884                  \n",
      "    55     0.184804   0.875399   0.726512                  \n",
      "    56     0.184245   0.812392   0.741395                  \n",
      "    57     0.188191   0.842205   0.730233                  \n",
      "    58     0.180518   0.809819   0.736744                  \n",
      "    59     0.179487   0.826707   0.737674                 \n",
      "    60     0.174688   0.833266   0.733953                 \n",
      "    61     0.171226   0.847971   0.742326                  \n",
      "    62     0.169089   0.870787   0.734884                 \n",
      "    63     0.167596   0.859667   0.737674                 \n",
      "    64     0.165381   0.861718   0.739535                  \n",
      "    65     0.170435   0.866491   0.735814                 \n",
      "    66     0.17215    0.841028   0.735814                 \n",
      "    67     0.174062   0.848549   0.733953                 \n",
      "    68     0.170859   0.88295    0.732093                 \n",
      "    69     0.166177   0.853219   0.739535                 \n",
      "    70     0.167681   0.883395   0.738605                 \n",
      "    71     0.168082   0.861343   0.735814                 \n",
      "    72     0.164873   0.880807   0.737674                 \n",
      "    73     0.164275   0.878677   0.736744                 \n",
      "    74     0.163218   0.864282   0.741395                 \n",
      "    75     0.166676   0.863154   0.733023                  \n",
      "    76     0.16615    0.866967   0.744186                  \n",
      "    77     0.163079   0.857135   0.735814                 \n",
      "    78     0.160509   0.860981   0.744186                 \n",
      "    79     0.155705   0.852257   0.738605                 \n",
      "    80     0.156652   0.866841   0.732093                 \n",
      "    81     0.15466    0.87305    0.740465                 \n",
      "    82     0.156715   0.910335   0.742326                 \n",
      "    83     0.153832   0.911801   0.736744                 \n",
      "    84     0.153998   0.884132   0.731163                 \n",
      "    85     0.149589   0.867936   0.733953                 \n",
      "    86     0.147359   0.862667   0.739535                 \n",
      "    87     0.14709    0.883124   0.741395                 \n",
      "    88     0.143735   0.889425   0.733953                 \n",
      "    89     0.145159   0.893348   0.742326                 \n",
      "    90     0.145052   0.879477   0.733023                 \n",
      "    91     0.145781   0.88879    0.734884                 \n",
      "    92     0.14705    0.90765    0.737674                 \n",
      "    93     0.142695   0.889639   0.728372                  \n",
      "    94     0.144987   0.878135   0.747907                 \n",
      "    95     0.144756   0.884792   0.739535                 \n",
      "    96     0.142577   0.887016   0.744186                  \n",
      "    97     0.139708   0.892248   0.743256                  \n",
      "    98     0.137775   0.90994    0.727442                 \n",
      "    99     0.141322   0.945577   0.724651                 \n",
      "   100     0.13784    0.877727   0.741395                 \n",
      "   101     0.141754   0.903605   0.730233                 \n",
      "   102     0.140162   0.893827   0.743256                 \n",
      "   103     0.140113   0.875122   0.745116                 \n",
      "   104     0.135573   0.892207   0.737674                 \n",
      "   105     0.139576   0.910906   0.732093                 \n",
      "   106     0.138997   0.881228   0.742326                  \n",
      "   107     0.134259   0.905237   0.735814                  \n",
      "   108     0.136511   0.894608   0.742326                  \n",
      "   109     0.13678    0.891002   0.741395                 \n",
      "   110     0.138099   0.892498   0.744186                 \n",
      "   111     0.138267   0.887861   0.733953                 \n",
      "   112     0.138591   0.868634   0.735814                  \n",
      "   113     0.135996   0.905173   0.736744                  \n",
      "   114     0.135924   0.900842   0.739535                  \n",
      "   115     0.134436   0.9145     0.735814                  \n",
      "   116     0.132309   0.908406   0.740465                  \n",
      "   117     0.132279   0.906583   0.737674                 \n",
      "   118     0.135787   0.905269   0.737674                 \n",
      "   119     0.131849   0.897987   0.735814                 \n",
      "   120     0.134879   0.94004    0.744186                 \n",
      "   121     0.132546   0.893075   0.737674                 \n",
      "   122     0.130222   0.905741   0.736744                 \n",
      "   123     0.126745   0.916571   0.733023                 \n",
      "   124     0.126642   0.907153   0.743256                 \n",
      "   125     0.125812   0.922652   0.737674                 \n",
      "   126     0.125216   0.950167   0.737674                 \n",
      "   127     0.125581   0.921639   0.738605                  \n",
      "   128     0.124106   0.937807   0.737674                  \n",
      "   129     0.126173   0.93591    0.742326                  \n",
      "   130     0.125443   0.917205   0.746047                  \n",
      "   131     0.126639   0.943993   0.743256                  \n",
      "   132     0.127131   0.977878   0.729302                  \n",
      "   133     0.129668   0.933108   0.733023                 \n",
      "   134     0.129295   0.945651   0.738605                 \n",
      "   135     0.129539   0.936654   0.738605                 \n",
      "   136     0.12581    0.93489    0.746977                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.126167   0.929969   0.741395                 \n",
      "   138     0.126714   0.926401   0.737674                 \n",
      "   139     0.121405   0.939478   0.737674                 \n",
      "   140     0.12332    0.929406   0.732093                 \n",
      "   141     0.121988   0.940203   0.733023                 \n",
      "   142     0.122186   0.929713   0.744186                  \n",
      "   143     0.121553   0.914618   0.742326                  \n",
      "   144     0.122821   0.944583   0.737674                  \n",
      "   145     0.122906   0.943514   0.739535                 \n",
      "   146     0.121947   0.929732   0.741395                 \n",
      "   147     0.120036   0.943852   0.742326                 \n",
      "   148     0.119952   0.927662   0.740465                  \n",
      "   149     0.120419   0.943376   0.744186                  \n",
      "   150     0.119464   0.952295   0.743256                  \n",
      "   151     0.117394   0.929757   0.736744                 \n",
      "   152     0.119906   0.936081   0.736744                  \n",
      "   153     0.121499   0.944234   0.733023                  \n",
      "   154     0.123879   0.943571   0.739535                 \n",
      "   155     0.125976   0.934984   0.733953                  \n",
      "   156     0.126584   0.935236   0.745116                  \n",
      "   157     0.122986   0.964532   0.742326                  \n",
      "   158     0.119795   0.956925   0.738605                  \n",
      "   159     0.116352   0.927979   0.741395                  \n",
      "   160     0.117193   0.935528   0.739535                 \n",
      "   161     0.115902   0.935411   0.735814                 \n",
      "   162     0.11546    0.935959   0.741395                  \n",
      "   163     0.111654   0.935171   0.744186                  \n",
      "   164     0.115457   0.946547   0.743256                  \n",
      "   165     0.117409   0.949322   0.741395                  \n",
      "   166     0.117411   0.931495   0.739535                  \n",
      "   167     0.118886   0.929716   0.738605                 \n",
      "   168     0.119136   0.920498   0.749767                 \n",
      "   169     0.119398   0.932217   0.750698                 \n",
      "   170     0.115945   0.920753   0.733023                  \n",
      "   171     0.115292   0.939764   0.741395                  \n",
      "   172     0.115469   0.933657   0.738605                  \n",
      "   173     0.115079   0.93502    0.740465                  \n",
      "   174     0.11615    0.953616   0.745116                 \n",
      "   175     0.115843   0.954606   0.741395                 \n",
      "   176     0.113308   0.932857   0.743256                 \n",
      "   177     0.113667   0.944174   0.747907                 \n",
      "   178     0.117177   0.948995   0.734884                 \n",
      "   179     0.11149    0.936345   0.746047                 \n",
      "   180     0.113256   0.931173   0.742326                 \n",
      "   181     0.109914   0.954142   0.745116                 \n",
      "   182     0.1131     0.95105    0.741395                 \n",
      "   183     0.111783   0.961098   0.740465                  \n",
      "   184     0.114795   0.958587   0.741395                 \n",
      "   185     0.116506   0.954626   0.740465                 \n",
      "   186     0.114263   0.957687   0.739535                 \n",
      "   187     0.115294   0.952175   0.742326                 \n",
      "   188     0.111155   0.949855   0.738605                 \n",
      "   189     0.110619   0.968551   0.740465                 \n",
      "   190     0.111794   0.960672   0.733953                \n",
      "   191     0.113578   0.953023   0.737674                 \n",
      "   192     0.113151   0.94273    0.746977                  \n",
      "   193     0.111257   0.954168   0.744186                  \n",
      "   194     0.109754   0.956339   0.746046                  \n",
      "   195     0.114371   0.947014   0.749767                  \n",
      "   196     0.115037   0.939548   0.744186                 \n",
      "   197     0.113929   0.942886   0.748837                  \n",
      "   198     0.110396   0.940153   0.745116                 \n",
      "   199     0.110836   0.944241   0.746047                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c876c53a044298bab9644482870bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f478870093642aaa6d1b493834e2d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.056989   0.899738   0.597209  \n",
      "    1      0.907198   0.846327   0.631628                 \n",
      "    2      0.818235   0.788919   0.653023                 \n",
      "    3      0.75639    0.770096   0.661395                 \n",
      "    4      0.706775   0.750895   0.664186                 \n",
      "    5      0.660724   0.734085   0.67907                  \n",
      "    6      0.620112   0.715348   0.683721                 \n",
      "    7      0.583918   0.707131   0.691163                 \n",
      "    8      0.546263   0.714814   0.685581                 \n",
      "    9      0.515689   0.7016     0.700465                 \n",
      "    10     0.492439   0.693544   0.713488                 \n",
      "    11     0.472544   0.727645   0.709767                  \n",
      "    12     0.451543   0.709795   0.714419                 \n",
      "    13     0.430217   0.701511   0.705116                 \n",
      "    14     0.414269   0.707237   0.715349                 \n",
      "    15     0.393748   0.712855   0.725581                 \n",
      "    16     0.37814    0.71313    0.72093                  \n",
      "    17     0.3644     0.715969   0.722791                 \n",
      "    18     0.358798   0.722677   0.732093                 \n",
      "    19     0.347745   0.719003   0.714419                 \n",
      "    20     0.341165   0.7094     0.738605                 \n",
      "    21     0.333373   0.734601   0.735814                 \n",
      "    22     0.320266   0.743879   0.72                     \n",
      "    23     0.312503   0.730343   0.737674                 \n",
      "    24     0.304916   0.736344   0.727442                 \n",
      "    25     0.29565    0.741995   0.730233                  \n",
      "    26     0.282358   0.71899    0.735814                  \n",
      "    27     0.270173   0.750053   0.72                      \n",
      "    28     0.266971   0.741676   0.735814                  \n",
      "    29     0.261052   0.775251   0.71814                   \n",
      "    30     0.262454   0.789055   0.71907                   \n",
      "    31     0.253314   0.771697   0.736744                  \n",
      "    32     0.250757   0.775329   0.738605                 \n",
      "    33     0.245946   0.790655   0.722791                 \n",
      "    34     0.241773   0.779764   0.730233                 \n",
      "    35     0.232957   0.787985   0.736744                  \n",
      "    36     0.222708   0.77811    0.734884                  \n",
      "    37     0.225894   0.810507   0.725581                  \n",
      "    38     0.226536   0.796308   0.746047                 \n",
      "    39     0.226063   0.854147   0.731163                  \n",
      "    40     0.221787   0.825107   0.735814                  \n",
      "    41     0.215722   0.815893   0.739535                  \n",
      "    42     0.217332   0.836359   0.734884                 \n",
      "    43     0.218175   0.804403   0.737674                 \n",
      "    44     0.215847   0.837365   0.732093                  \n",
      "    45     0.209818   0.79618    0.743256                  \n",
      "    46     0.202322   0.813716   0.738605                  \n",
      "    47     0.200908   0.826106   0.730233                 \n",
      "    48     0.196968   0.813275   0.735814                  \n",
      "    49     0.196788   0.825265   0.735814                  \n",
      "    50     0.19799    0.83289    0.733953                  \n",
      "    51     0.191211   0.85564    0.726512                  \n",
      "    52     0.18895    0.817202   0.740465                  \n",
      "    53     0.185357   0.848074   0.733023                 \n",
      "    54     0.188856   0.84051    0.734884                 \n",
      "    55     0.185479   0.860398   0.737674                 \n",
      "    56     0.187467   0.8582     0.733953                 \n",
      "    57     0.180662   0.82447    0.736744                 \n",
      "    58     0.180373   0.872896   0.728372                 \n",
      "    59     0.175856   0.857839   0.726512                  \n",
      "    60     0.1773     0.873651   0.730233                  \n",
      "    61     0.178033   0.853825   0.735814                  \n",
      "    62     0.176823   0.860869   0.731163                  \n",
      "    63     0.180029   0.892882   0.727442                  \n",
      "    64     0.179871   0.850921   0.733953                  \n",
      "    65     0.173487   0.864099   0.731163                  \n",
      "    66     0.170009   0.8622     0.739535                  \n",
      "    67     0.1645     0.875959   0.734884                  \n",
      "    68     0.165678   0.847618   0.740465                  \n",
      "    69     0.168306   0.869495   0.742326                  \n",
      "    70     0.163295   0.887848   0.730233                  \n",
      "    71     0.162749   0.889136   0.730233                  \n",
      "    72     0.163624   0.882674   0.733953                  \n",
      "    73     0.16176    0.881587   0.734884                  \n",
      "    74     0.157798   0.863654   0.733023                 \n",
      "    75     0.161503   0.896878   0.733953                  \n",
      "    76     0.161096   0.854491   0.739535                 \n",
      "    77     0.163198   0.859252   0.728372                  \n",
      "    78     0.158944   0.869166   0.740465                  \n",
      "    79     0.156427   0.855002   0.729302                  \n",
      "    80     0.153266   0.880299   0.734884                  \n",
      "    81     0.15249    0.854576   0.733023                  \n",
      "    82     0.152413   0.916648   0.733023                 \n",
      "    83     0.153448   0.879241   0.724651                  \n",
      "    84     0.155413   0.892824   0.735814                  \n",
      "    85     0.15277    0.877406   0.733953                  \n",
      "    86     0.147161   0.91962    0.729302                  \n",
      "    87     0.146347   0.882158   0.733953                  \n",
      "    88     0.146511   0.90274    0.733953                 \n",
      "    89     0.144859   0.914282   0.727442                 \n",
      "    90     0.145069   0.916568   0.739535                 \n",
      "    91     0.146541   0.921286   0.733023                 \n",
      "    92     0.147182   0.88527    0.728372                 \n",
      "    93     0.144908   0.877027   0.733023                 \n",
      "    94     0.142885   0.890171   0.732093                 \n",
      "    95     0.141193   0.893811   0.737674                 \n",
      "    96     0.140388   0.897727   0.741395                 \n",
      "    97     0.139405   0.888488   0.737674                 \n",
      "    98     0.138398   0.897031   0.729302                 \n",
      "    99     0.142848   0.920861   0.736744                 \n",
      "   100     0.142929   0.936421   0.733953                 \n",
      "   101     0.142154   0.922072   0.729302                 \n",
      "   102     0.139941   0.912299   0.740465                 \n",
      "   103     0.139457   0.94665    0.726512                 \n",
      "   104     0.137366   0.922767   0.734884                 \n",
      "   105     0.135631   0.9374     0.737674                 \n",
      "   106     0.138099   0.918074   0.736744                 \n",
      "   107     0.136794   0.906354   0.740465                 \n",
      "   108     0.142475   0.923808   0.732093                  \n",
      "   109     0.141278   0.92795    0.731163                  \n",
      "   110     0.144403   0.92063    0.733023                 \n",
      "   111     0.143006   0.919855   0.734884                  \n",
      "   112     0.140685   0.887568   0.733023                 \n",
      "   113     0.138046   0.911964   0.744186                 \n",
      "   114     0.135598   0.89891    0.733023                 \n",
      "   115     0.134987   0.90336    0.733953                 \n",
      "   116     0.133527   0.89494    0.735814                 \n",
      "   117     0.135044   0.927604   0.728372                 \n",
      "   118     0.132739   0.916565   0.733953                 \n",
      "   119     0.133499   0.940845   0.725581                 \n",
      "   120     0.131952   0.911877   0.733953                 \n",
      "   121     0.130903   0.899729   0.739535                 \n",
      "   122     0.130117   0.897338   0.738605                 \n",
      "   123     0.126856   0.903476   0.742326                 \n",
      "   124     0.127322   0.926608   0.742326                 \n",
      "   125     0.127094   0.918805   0.735814                 \n",
      "   126     0.126286   0.909312   0.744186                 \n",
      "   127     0.126696   0.903354   0.738605                  \n",
      "   128     0.124539   0.924013   0.739535                  \n",
      "   129     0.124883   0.905903   0.744186                  \n",
      "   130     0.127723   0.911973   0.738605                  \n",
      "   131     0.130392   0.925143   0.738605                  \n",
      "   132     0.130827   0.912519   0.746047                  \n",
      "   133     0.127333   0.945141   0.736744                  \n",
      "   134     0.12638    0.935827   0.737674                  \n",
      "   135     0.125336   0.922483   0.745116                  \n",
      "   136     0.12362    0.945451   0.741395                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.125349   0.936705   0.740465                 \n",
      "   138     0.123554   0.965351   0.736744                 \n",
      "   139     0.123122   0.955865   0.737674                 \n",
      "   140     0.124654   0.960079   0.735814                 \n",
      "   141     0.123552   0.930993   0.738605                 \n",
      "   142     0.124143   0.908237   0.748837                  \n",
      "   143     0.121924   0.936006   0.740465                  \n",
      "   144     0.124308   0.955202   0.739535                 \n",
      "   145     0.125089   0.95522    0.733023                  \n",
      "   146     0.126543   0.940287   0.741395                  \n",
      "   147     0.124967   0.917387   0.746047                  \n",
      "   148     0.120546   0.91882    0.744186                  \n",
      "   149     0.118192   0.94922    0.742326                  \n",
      "   150     0.120582   0.937378   0.738605                 \n",
      "   151     0.121239   0.92755    0.744186                 \n",
      "   152     0.120944   0.951457   0.735814                  \n",
      "   153     0.116956   0.935271   0.742326                  \n",
      "   154     0.117747   0.940242   0.746977                  \n",
      "   155     0.121101   0.927758   0.738605                  \n",
      "   156     0.117759   0.932086   0.748837                  \n",
      "   157     0.114312   0.905157   0.738605                  \n",
      "   158     0.117146   0.925056   0.734884                 \n",
      "   159     0.116911   0.931119   0.735814                 \n",
      "   160     0.117259   0.946939   0.733023                 \n",
      "   161     0.117757   0.923931   0.745116                 \n",
      "   162     0.119505   0.9408     0.751628                 \n",
      "   163     0.11764    0.908832   0.738605                 \n",
      "   164     0.117051   0.920828   0.739535                 \n",
      "   165     0.11601    0.949775   0.738605                 \n",
      "   166     0.117321   0.941237   0.734884                 \n",
      "   167     0.118809   0.931124   0.734884                 \n",
      "   168     0.120677   0.951417   0.733023                 \n",
      "   169     0.11684    0.918016   0.744186                 \n",
      "   170     0.114009   0.910781   0.733954                  \n",
      "   171     0.113      0.948765   0.734884                  \n",
      "   172     0.116349   0.93487    0.739535                  \n",
      "   173     0.116309   0.947246   0.736744                  \n",
      "   174     0.116749   0.940006   0.734884                  \n",
      "   175     0.119121   0.931897   0.735814                 \n",
      "   176     0.117202   0.948757   0.736744                  \n",
      "   177     0.114871   0.954672   0.738605                  \n",
      "   178     0.111935   0.952772   0.733023                  \n",
      "   179     0.113363   0.97691    0.733953                  \n",
      "   180     0.113862   0.953283   0.735814                  \n",
      "   181     0.115342   0.933098   0.741395                 \n",
      "   182     0.113908   0.955105   0.729302                 \n",
      "   183     0.111402   0.925494   0.742326                 \n",
      "   184     0.112751   0.940023   0.750698                 \n",
      "   185     0.112121   0.937346   0.740465                 \n",
      "   186     0.10816    0.936422   0.735814                 \n",
      "   187     0.107606   0.946503   0.738605                 \n",
      "   188     0.110144   0.923594   0.736744                 \n",
      "   189     0.112145   0.95002    0.735814                 \n",
      "   190     0.115464   0.960083   0.746977                 \n",
      "   191     0.114847   0.932899   0.743256                 \n",
      "   192     0.113382   0.953085   0.739535                 \n",
      "   193     0.111789   0.960225   0.738605                 \n",
      "   194     0.111568   0.96052    0.745116                  \n",
      "   195     0.11143    0.984694   0.741395                  \n",
      "   196     0.113231   0.963873   0.743256                 \n",
      "   197     0.114397   0.962427   0.748837                  \n",
      "   198     0.116245   0.925336   0.747907                  \n",
      "   199     0.113297   0.931981   0.750698                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5096a81b4d4ca787602939127c8c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107f4292055b4d7ab605d99f5eec6675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.034849   0.908525   0.586047  \n",
      "    1      0.883341   0.839147   0.626047                 \n",
      "    2      0.799499   0.786893   0.661395                 \n",
      "    3      0.740358   0.764529   0.675349                 \n",
      "    4      0.693311   0.750364   0.687442                 \n",
      "    5      0.647464   0.739792   0.693953                 \n",
      "    6      0.613199   0.755607   0.668837                 \n",
      "    7      0.574496   0.727506   0.693953                 \n",
      "    8      0.550085   0.733572   0.697674                 \n",
      "    9      0.520468   0.716856   0.705116                 \n",
      "    10     0.494108   0.705585   0.72093                   \n",
      "    11     0.472175   0.748433   0.706977                  \n",
      "    12     0.450416   0.723127   0.701395                  \n",
      "    13     0.422446   0.726363   0.72186                  \n",
      "    14     0.408633   0.732113   0.72093                  \n",
      "    15     0.389086   0.714925   0.723721                 \n",
      "    16     0.374693   0.737857   0.72186                  \n",
      "    17     0.358904   0.718163   0.72186                  \n",
      "    18     0.34772    0.7435     0.71814                  \n",
      "    19     0.337566   0.762278   0.722791                  \n",
      "    20     0.331536   0.750708   0.725581                 \n",
      "    21     0.321366   0.76836    0.71907                  \n",
      "    22     0.312646   0.764762   0.724651                 \n",
      "    23     0.30773    0.752259   0.733953                 \n",
      "    24     0.29942    0.735886   0.731163                 \n",
      "    25     0.288298   0.742722   0.723721                  \n",
      "    26     0.285897   0.744182   0.746977                 \n",
      "    27     0.275384   0.749868   0.740465                  \n",
      "    28     0.268482   0.74883    0.731163                 \n",
      "    29     0.262317   0.782261   0.717209                 \n",
      "    30     0.262648   0.774771   0.729302                 \n",
      "    31     0.259115   0.766447   0.746047                 \n",
      "    32     0.247299   0.786553   0.730233                 \n",
      "    33     0.239649   0.749798   0.746047                 \n",
      "    34     0.235827   0.771707   0.729302                 \n",
      "    35     0.234607   0.782988   0.733023                  \n",
      "    36     0.229654   0.780372   0.742326                 \n",
      "    37     0.228527   0.80356    0.730233                  \n",
      "    38     0.232512   0.77369    0.741395                  \n",
      "    39     0.223854   0.794721   0.736744                  \n",
      "    40     0.217018   0.795748   0.739535                  \n",
      "    41     0.216701   0.780341   0.731163                  \n",
      "    42     0.212687   0.837793   0.723721                  \n",
      "    43     0.220229   0.824324   0.731163                  \n",
      "    44     0.217562   0.83218    0.737674                 \n",
      "    45     0.213798   0.855164   0.726512                 \n",
      "    46     0.210054   0.846231   0.730233                 \n",
      "    47     0.204225   0.827399   0.729302                 \n",
      "    48     0.200436   0.840705   0.742326                 \n",
      "    49     0.198422   0.829323   0.726512                 \n",
      "    50     0.192573   0.855327   0.729302                 \n",
      "    51     0.191489   0.839737   0.746977                  \n",
      "    52     0.188261   0.826467   0.746047                  \n",
      "    53     0.191468   0.84513    0.729302                  \n",
      "    54     0.191108   0.895412   0.714419                 \n",
      "    55     0.191071   0.849301   0.736744                 \n",
      "    56     0.186366   0.847701   0.733023                  \n",
      "    57     0.182165   0.837223   0.742326                  \n",
      "    58     0.17895    0.851496   0.743256                  \n",
      "    59     0.175151   0.91221    0.727442                 \n",
      "    60     0.178265   0.889459   0.740465                 \n",
      "    61     0.179069   0.886274   0.731163                  \n",
      "    62     0.175012   0.862907   0.731163                 \n",
      "    63     0.170142   0.876461   0.736744                  \n",
      "    64     0.164168   0.856681   0.739535                 \n",
      "    65     0.165722   0.866779   0.737674                 \n",
      "    66     0.162482   0.873613   0.739535                 \n",
      "    67     0.161399   0.842439   0.737674                 \n",
      "    68     0.165068   0.864121   0.731163                 \n",
      "    69     0.168621   0.871361   0.731163                 \n",
      "    70     0.16928    0.872933   0.739535                 \n",
      "    71     0.165864   0.908398   0.733023                 \n",
      "    72     0.164606   0.884276   0.739535                 \n",
      "    73     0.161097   0.886885   0.739535                 \n",
      "    74     0.1594     0.899213   0.724651                 \n",
      "    75     0.159227   0.894688   0.738605                 \n",
      "    76     0.158339   0.882469   0.749767                 \n",
      "    77     0.160797   0.885758   0.743256                 \n",
      "    78     0.159414   0.903557   0.740465                 \n",
      "    79     0.157811   0.903844   0.733023                 \n",
      "    80     0.158783   0.919046   0.732093                 \n",
      "    81     0.154319   0.886503   0.741395                 \n",
      "    82     0.152672   0.903424   0.741395                 \n",
      "    83     0.151008   0.892714   0.733023                 \n",
      "    84     0.15175    0.87746    0.737674                \n",
      "    85     0.148255   0.894043   0.740465                 \n",
      "    86     0.148958   0.904812   0.736744                 \n",
      "    87     0.151239   0.942849   0.717209                 \n",
      "    88     0.147521   0.888676   0.737674                \n",
      "    89     0.142273   0.888094   0.736744                  \n",
      "    90     0.141618   0.887204   0.739535                  \n",
      "    91     0.138749   0.917143   0.740465                  \n",
      "    92     0.141254   0.918455   0.740465                  \n",
      "    93     0.144435   0.915077   0.739535                 \n",
      "    94     0.143024   0.921706   0.743256                 \n",
      "    95     0.138283   0.929026   0.743256                 \n",
      "    96     0.139286   0.929526   0.745116                 \n",
      "    97     0.141022   0.924929   0.746977                 \n",
      "    98     0.138693   0.939523   0.735814                  \n",
      "    99     0.141262   0.911168   0.740465                  \n",
      "   100     0.141668   0.924502   0.742326                 \n",
      "   101     0.141818   0.937463   0.741395                 \n",
      "   102     0.141942   0.90504    0.742326                 \n",
      "   103     0.14246    0.919505   0.746977                 \n",
      "   104     0.141126   0.931558   0.740465                  \n",
      "   105     0.139085   0.948521   0.742326                  \n",
      "   106     0.140971   0.940154   0.742326                 \n",
      "   107     0.140328   0.915636   0.737674                  \n",
      "   108     0.138927   0.918402   0.745116                  \n",
      "   109     0.135718   0.922508   0.743256                  \n",
      "   110     0.133131   0.936619   0.741395                  \n",
      "   111     0.135994   0.937237   0.736744                 \n",
      "   112     0.133914   0.928818   0.741395                 \n",
      "   113     0.133458   0.962944   0.738605                 \n",
      "   114     0.136716   0.915179   0.747907                 \n",
      "   115     0.136077   0.951863   0.736744                 \n",
      "   116     0.136507   0.913378   0.746977                 \n",
      "   117     0.13779    0.940863   0.745116                 \n",
      "   118     0.137034   0.923436   0.743256                 \n",
      "   119     0.137044   0.941117   0.739535                 \n",
      "   120     0.134669   0.929148   0.739535                 \n",
      "   121     0.134214   0.928345   0.739535                 \n",
      "   122     0.131518   0.912387   0.744186                 \n",
      "   123     0.130709   0.91915    0.739535                 \n",
      "   124     0.125379   0.912183   0.735814                 \n",
      "   125     0.13021    0.941449   0.742326                 \n",
      "   126     0.132072   0.928305   0.736744                 \n",
      "   127     0.129545   0.902737   0.742326                 \n",
      "   128     0.124951   0.928346   0.744186                 \n",
      "   129     0.125851   0.933494   0.749767                 \n",
      "   130     0.128048   0.909716   0.747907                  \n",
      "   131     0.131937   0.90776    0.733023                  \n",
      "   132     0.129414   0.909127   0.754419                  \n",
      "   133     0.128587   0.927912   0.741395                  \n",
      "   134     0.126437   0.917251   0.748837                  \n",
      "   135     0.125324   0.931531   0.738605                  \n",
      "   136     0.12918    0.909169   0.752558                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.126043   0.937138   0.745116                  \n",
      "   138     0.126439   0.918287   0.751628                 \n",
      "   139     0.124618   0.957131   0.745116                 \n",
      "   140     0.125836   0.950633   0.745116                 \n",
      "   141     0.12535    0.928214   0.751628                  \n",
      "   142     0.126075   0.92019    0.753488                 \n",
      "   143     0.126864   0.920123   0.746977                 \n",
      "   144     0.123881   0.926172   0.738605                  \n",
      "   145     0.12269    0.919522   0.756279                  \n",
      "   146     0.11938    0.93914    0.738605                 \n",
      "   147     0.121231   0.923492   0.753488                  \n",
      "   148     0.120662   0.934693   0.748837                  \n",
      "   149     0.121765   0.930724   0.746047                  \n",
      "   150     0.119799   0.935057   0.738605                 \n",
      "   151     0.121371   0.928426   0.753488                 \n",
      "   152     0.123531   0.919321   0.743256                 \n",
      "   153     0.126271   0.914771   0.746977                 \n",
      "   154     0.125709   0.924295   0.750698                 \n",
      "   155     0.126382   0.935012   0.746047                 \n",
      "   156     0.124719   0.909498   0.746047                 \n",
      "   157     0.124116   0.915415   0.743256                 \n",
      "   158     0.120401   0.936805   0.746977                 \n",
      "   159     0.118099   0.921177   0.746046                 \n",
      "   160     0.116091   0.948067   0.744186                  \n",
      "   161     0.118622   0.92981    0.749767                  \n",
      "   162     0.116945   0.932971   0.748837                  \n",
      "   163     0.11865    0.926624   0.749767                 \n",
      "   164     0.118955   0.919755   0.750698                 \n",
      "   165     0.117176   0.948782   0.744186                 \n",
      "   166     0.119113   0.918487   0.750698                 \n",
      "   167     0.119599   0.942192   0.742326                 \n",
      "   168     0.118757   0.920026   0.756279                  \n",
      "   169     0.119284   0.945051   0.746977                 \n",
      "   170     0.120166   0.919921   0.749767                \n",
      "   171     0.118405   0.926676   0.751628                 \n",
      "   172     0.115908   0.929303   0.752558                 \n",
      "   173     0.11628    0.924825   0.756279                  \n",
      "   174     0.116704   0.927877   0.743256                 \n",
      "   175     0.116193   0.95163    0.743256                 \n",
      "   176     0.115146   0.96229    0.745116                 \n",
      "   177     0.113276   0.943657   0.746047                 \n",
      "   178     0.116744   0.966536   0.746977                 \n",
      "   179     0.117308   0.95084    0.753488                 \n",
      "   180     0.116515   0.953169   0.746977                 \n",
      "   181     0.115072   0.933839   0.749767                  \n",
      "   182     0.112853   0.938549   0.742326                  \n",
      "   183     0.111771   0.942983   0.744186                 \n",
      "   184     0.114696   0.950196   0.744186                 \n",
      "   185     0.116707   0.934751   0.742326                 \n",
      "   186     0.113905   0.971043   0.746047                 \n",
      "   187     0.113636   0.989035   0.730233                 \n",
      "   188     0.113768   0.938508   0.743256                  \n",
      "   189     0.114567   0.940857   0.743256                  \n",
      "   190     0.112478   0.929269   0.746977                 \n",
      "   191     0.112887   0.92958    0.746977                 \n",
      "   192     0.1134     0.946951   0.741395                  \n",
      "   193     0.110883   0.959771   0.743256                  \n",
      "   194     0.109764   0.963893   0.742326                 \n",
      "   195     0.109689   0.956399   0.749767                  \n",
      "   196     0.106657   0.953075   0.746046                  \n",
      "   197     0.109149   0.964294   0.746977                  \n",
      "   198     0.110254   0.973322   0.749767                  \n",
      "   199     0.110548   0.946815   0.751628                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11a1d0daf294e79ab959a89424b1bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464fc10dd70842dea5058ade427e8862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.110152   0.921717   0.566512  \n",
      "    1      0.973497   0.868315   0.610233                \n",
      "    2      0.900606   0.835363   0.624186                 \n",
      "    3      0.849173   0.815308   0.632558                 \n",
      "    4      0.809031   0.793035   0.662326                 \n",
      "    5      0.781293   0.78314    0.664186                 \n",
      "    6      0.75578    0.778201   0.655814                 \n",
      "    7      0.735832   0.775821   0.659535                 \n",
      "    8      0.718845   0.760639   0.671628                  \n",
      "    9      0.695298   0.732159   0.694884                  \n",
      "    10     0.675533   0.742846   0.674419                 \n",
      "    11     0.65889    0.726868   0.691163                 \n",
      "    12     0.649705   0.728218   0.686512                 \n",
      "    13     0.639708   0.7328     0.687442                 \n",
      "    14     0.625095   0.700923   0.706047                 \n",
      "    15     0.60682    0.709523   0.695814                 \n",
      "    16     0.600486   0.709625   0.702326                 \n",
      "    17     0.591383   0.708489   0.698605                 \n",
      "    18     0.582657   0.716993   0.703256                 \n",
      "    19     0.576103   0.702836   0.708837                 \n",
      "    20     0.566094   0.710919   0.707907                  \n",
      "    21     0.553487   0.694786   0.716279                  \n",
      "    22     0.54365    0.690229   0.706977                  \n",
      "    23     0.531332   0.701104   0.712558                 \n",
      "    24     0.522883   0.686629   0.714419                  \n",
      "    25     0.515393   0.687684   0.714419                  \n",
      "    26     0.512197   0.697334   0.71814                   \n",
      "    27     0.509399   0.690573   0.707907                  \n",
      "    28     0.499186   0.684293   0.707907                  \n",
      "    29     0.493623   0.680618   0.717209                  \n",
      "    30     0.484822   0.680588   0.723721                  \n",
      "    31     0.476843   0.679115   0.708837                 \n",
      "    32     0.475455   0.691384   0.714419                 \n",
      "    33     0.4723     0.673496   0.731163                 \n",
      "    34     0.467304   0.683856   0.72                     \n",
      "    35     0.452844   0.662123   0.725581                  \n",
      "    36     0.447179   0.673834   0.729302                 \n",
      "    37     0.441935   0.685145   0.727442                  \n",
      "    38     0.434531   0.685442   0.71907                   \n",
      "    39     0.436709   0.675186   0.725581                  \n",
      "    40     0.431668   0.687314   0.723721                 \n",
      "    41     0.426485   0.683872   0.739535                 \n",
      "    42     0.421441   0.674801   0.736744                 \n",
      "    43     0.41591    0.680359   0.733953                 \n",
      "    44     0.405905   0.69429    0.72                     \n",
      "    45     0.406342   0.693894   0.72186                   \n",
      "    46     0.40328    0.682285   0.729302                 \n",
      "    47     0.391811   0.700783   0.729302                  \n",
      "    48     0.388117   0.700343   0.733953                  \n",
      "    49     0.387499   0.691925   0.737674                  \n",
      "    50     0.380849   0.698299   0.733953                 \n",
      "    51     0.372004   0.685464   0.734884                  \n",
      "    52     0.374405   0.695047   0.735814                  \n",
      "    53     0.369468   0.704383   0.731163                  \n",
      "    54     0.365038   0.70499    0.738605                  \n",
      "    55     0.359023   0.710718   0.731163                  \n",
      "    56     0.358951   0.704739   0.730233                 \n",
      "    57     0.362089   0.688566   0.735814                 \n",
      "    58     0.364957   0.690445   0.737674                 \n",
      "    59     0.358512   0.696845   0.734884                  \n",
      "    60     0.353718   0.687783   0.734884                 \n",
      "    61     0.346956   0.693102   0.739535                 \n",
      "    62     0.339356   0.713661   0.749767                 \n",
      "    63     0.347685   0.709183   0.739535                 \n",
      "    64     0.338814   0.70915    0.733023                 \n",
      "    65     0.33476    0.713608   0.745116                 \n",
      "    66     0.329738   0.716893   0.740465                 \n",
      "    67     0.326713   0.704833   0.745116                 \n",
      "    68     0.325228   0.702351   0.746977                 \n",
      "    69     0.323315   0.704263   0.746047                 \n",
      "    70     0.320336   0.720397   0.739535                  \n",
      "    71     0.318648   0.699841   0.746047                 \n",
      "    72     0.316696   0.707838   0.741395                 \n",
      "    73     0.313836   0.713585   0.742326                 \n",
      "    74     0.311197   0.728246   0.744186                 \n",
      "    75     0.307456   0.716529   0.732093                  \n",
      "    76     0.306787   0.73519    0.736744                  \n",
      "    77     0.304087   0.728071   0.733953                  \n",
      "    78     0.300251   0.71714    0.742326                 \n",
      "    79     0.300457   0.710982   0.743256                  \n",
      "    80     0.300016   0.7287     0.737674                 \n",
      "    81     0.294502   0.739175   0.735814                 \n",
      "    82     0.293072   0.718665   0.737674                 \n",
      "    83     0.286783   0.731745   0.741395                 \n",
      "    84     0.284185   0.736205   0.736744                 \n",
      "    85     0.284983   0.740411   0.738605                 \n",
      "    86     0.282861   0.73433    0.744186                 \n",
      "    87     0.277477   0.741174   0.745116                 \n",
      "    88     0.281211   0.744276   0.743256                 \n",
      "    89     0.276826   0.750586   0.743256                 \n",
      "    90     0.281582   0.734985   0.746047                 \n",
      "    91     0.276227   0.735516   0.737674                 \n",
      "    92     0.271093   0.737596   0.749767                 \n",
      "    93     0.271991   0.743812   0.756279                 \n",
      "    94     0.269266   0.759647   0.744186                 \n",
      "    95     0.27061    0.762261   0.749767                 \n",
      "    96     0.268351   0.75507    0.742326                 \n",
      "    97     0.266186   0.758381   0.746977                  \n",
      "    98     0.269784   0.758008   0.733953                 \n",
      "    99     0.264013   0.752055   0.745116                 \n",
      "   100     0.262965   0.745459   0.744186                 \n",
      "   101     0.260831   0.763056   0.742326                 \n",
      "   102     0.255      0.762456   0.734884                 \n",
      "   103     0.249629   0.766708   0.738605                 \n",
      "   104     0.252312   0.771128   0.735814                 \n",
      "   105     0.252446   0.762815   0.745116                 \n",
      "   106     0.249606   0.760032   0.735814                 \n",
      "   107     0.251887   0.765526   0.741395                 \n",
      "   108     0.247662   0.762044   0.746047                  \n",
      "   109     0.247676   0.758082   0.733023                  \n",
      "   110     0.24612    0.785522   0.735814                  \n",
      "   111     0.244386   0.780169   0.738605                  \n",
      "   112     0.245386   0.762381   0.733023                 \n",
      "   113     0.238707   0.774213   0.733023                 \n",
      "   114     0.237166   0.761658   0.744186                  \n",
      "   115     0.235557   0.776807   0.740465                 \n",
      "   116     0.24146    0.765259   0.738605                 \n",
      "   117     0.239176   0.760036   0.742326                  \n",
      "   118     0.239077   0.752047   0.738605                  \n",
      "   119     0.236631   0.769745   0.745116                 \n",
      "   120     0.236959   0.780825   0.736744                 \n",
      "   121     0.236558   0.779782   0.742326                  \n",
      "   122     0.232704   0.786326   0.747907                  \n",
      "   123     0.234183   0.776663   0.736744                  \n",
      "   124     0.233791   0.781778   0.734884                  \n",
      "   125     0.232675   0.76858    0.741395                  \n",
      "   126     0.230584   0.795829   0.734884                 \n",
      "   127     0.228907   0.76833    0.746047                  \n",
      "   128     0.22857    0.76628    0.741395                  \n",
      "   129     0.221364   0.776002   0.737674                  \n",
      "   130     0.216963   0.778664   0.742326                  \n",
      "   131     0.215059   0.788163   0.743256                 \n",
      "   132     0.215848   0.77573    0.736744                  \n",
      "   133     0.219854   0.770878   0.743256                  \n",
      "   134     0.216422   0.785208   0.744186                  \n",
      "   135     0.213523   0.784665   0.745116                  \n",
      "   136     0.209812   0.804014   0.738605                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.208356   0.807297   0.738605                 \n",
      "   138     0.208974   0.791653   0.746047                 \n",
      "   139     0.212694   0.787743   0.741395                  \n",
      "   140     0.213549   0.787025   0.744186                  \n",
      "   141     0.208542   0.804415   0.738605                  \n",
      "   142     0.204594   0.795734   0.743256                 \n",
      "   143     0.205979   0.792095   0.748837                 \n",
      "   144     0.210433   0.808556   0.746046                 \n",
      "   145     0.210873   0.794323   0.749767                \n",
      "   146     0.207092   0.805487   0.734884                 \n",
      "   147     0.203297   0.810661   0.750698                 \n",
      "   148     0.199153   0.812324   0.743256                 \n",
      "   149     0.202249   0.804213   0.745116                 \n",
      "   150     0.205102   0.821313   0.742326                 \n",
      "   151     0.201769   0.803835   0.741395                 \n",
      "   152     0.198805   0.83242    0.736744                 \n",
      "   153     0.200834   0.815854   0.741395                 \n",
      "   154     0.203336   0.812499   0.735814               \n",
      "   155     0.202041   0.817892   0.738605                 \n",
      "   156     0.199278   0.803643   0.738605                 \n",
      "   157     0.198915   0.806041   0.744186                 \n",
      "   158     0.195732   0.796402   0.739535                 \n",
      "   159     0.195284   0.810705   0.741395                 \n",
      "   160     0.196885   0.792542   0.739535                 \n",
      "   161     0.194621   0.819779   0.739535                  \n",
      "   162     0.190846   0.811721   0.749767                 \n",
      "   163     0.192523   0.824101   0.741395                 \n",
      "   164     0.193931   0.828105   0.745116                  \n",
      "   165     0.194989   0.836444   0.749767                 \n",
      "   166     0.19549    0.818367   0.746977                  \n",
      "   167     0.190219   0.833513   0.739535                  \n",
      "   168     0.189968   0.839896   0.737674                  \n",
      "   169     0.192801   0.829756   0.741395                  \n",
      "   170     0.191365   0.831921   0.742326                  \n",
      "   171     0.188455   0.81868    0.746047                 \n",
      "   172     0.189925   0.840178   0.746047                  \n",
      "   173     0.185615   0.826358   0.749767                  \n",
      "   174     0.181874   0.83308    0.746977                 \n",
      "   175     0.179531   0.829782   0.751628                 \n",
      "   176     0.178282   0.824308   0.750698                 \n",
      "   177     0.181182   0.832261   0.750698                 \n",
      "   178     0.183177   0.821612   0.746977                 \n",
      "   179     0.182731   0.832352   0.746047                 \n",
      "   180     0.17859    0.864069   0.746977                 \n",
      "   181     0.180908   0.844464   0.744186                  \n",
      "   182     0.181875   0.836913   0.748837                  \n",
      "   183     0.180094   0.838017   0.750698                  \n",
      "   184     0.177684   0.846158   0.746977                  \n",
      "   185     0.17461    0.851972   0.749767                 \n",
      "   186     0.175102   0.847416   0.746047                 \n",
      "   187     0.179683   0.850024   0.751628                  \n",
      "   188     0.176135   0.82729    0.749767                 \n",
      "   189     0.178127   0.836653   0.748837                 \n",
      "   190     0.174707   0.824008   0.748837                 \n",
      "   191     0.173309   0.836831   0.756279                 \n",
      "   192     0.173843   0.839533   0.749767                 \n",
      "   193     0.177376   0.840329   0.747907                 \n",
      "   194     0.180382   0.826603   0.749767                 \n",
      "   195     0.174818   0.856294   0.746977                 \n",
      "   196     0.174064   0.832022   0.740465                 \n",
      "   197     0.174624   0.833947   0.740465                 \n",
      "   198     0.175201   0.848964   0.740465                  \n",
      "   199     0.169204   0.84721    0.737674                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacdc4f564724e30b4052ae72ec5c998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0caf3d1f0a46e6b4a80e311256251c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.090505   0.901989   0.590698  \n",
      "    1      0.96797    0.86654    0.592558                \n",
      "    2      0.898789   0.82183    0.627907                 \n",
      "    3      0.850652   0.798394   0.643721                 \n",
      "    4      0.818542   0.783724   0.643721                 \n",
      "    5      0.786948   0.774489   0.655814                 \n",
      "    6      0.763274   0.76977    0.669767                 \n",
      "    7      0.740497   0.753734   0.666046                 \n",
      "    8      0.720032   0.759365   0.663256                 \n",
      "    9      0.701579   0.736537   0.683721                \n",
      "    10     0.680784   0.721249   0.693953                  \n",
      "    11     0.663431   0.730511   0.67814                   \n",
      "    12     0.649687   0.724809   0.686512                  \n",
      "    13     0.639798   0.707358   0.708837                 \n",
      "    14     0.623395   0.712856   0.696744                  \n",
      "    15     0.61412    0.712783   0.703256                  \n",
      "    16     0.597103   0.698183   0.706047                  \n",
      "    17     0.586881   0.702494   0.709767                 \n",
      "    18     0.580028   0.71633    0.694884                 \n",
      "    19     0.569104   0.707109   0.703256                 \n",
      "    20     0.563725   0.693371   0.706047                 \n",
      "    21     0.558682   0.70269    0.708837                  \n",
      "    22     0.547714   0.688211   0.711628                  \n",
      "    23     0.538736   0.688244   0.703256                 \n",
      "    24     0.534032   0.683429   0.711628                  \n",
      "    25     0.525941   0.698227   0.707907                 \n",
      "    26     0.512334   0.692805   0.716279                  \n",
      "    27     0.503795   0.68545    0.72093                  \n",
      "    28     0.49463    0.691911   0.715349                 \n",
      "    29     0.493276   0.686289   0.71814                   \n",
      "    30     0.488627   0.682412   0.717209                  \n",
      "    31     0.480487   0.695886   0.726512                 \n",
      "    32     0.475929   0.697551   0.714419                  \n",
      "    33     0.47079    0.686666   0.732093                 \n",
      "    34     0.462423   0.701139   0.706977                  \n",
      "    35     0.45954    0.685085   0.716279                  \n",
      "    36     0.454925   0.678016   0.72093                   \n",
      "    37     0.449288   0.679198   0.723721                  \n",
      "    38     0.445213   0.690924   0.715349                  \n",
      "    39     0.438873   0.683388   0.72                     \n",
      "    40     0.429903   0.678772   0.726512                  \n",
      "    41     0.426928   0.679619   0.727442                  \n",
      "    42     0.424705   0.669585   0.729302                 \n",
      "    43     0.414974   0.679911   0.727442                 \n",
      "    44     0.407379   0.691288   0.723721                 \n",
      "    45     0.403863   0.683936   0.717209                 \n",
      "    46     0.404788   0.692696   0.72093                  \n",
      "    47     0.396158   0.695386   0.732093                 \n",
      "    48     0.389323   0.693898   0.722791                 \n",
      "    49     0.389592   0.697539   0.734884                 \n",
      "    50     0.390213   0.677178   0.737674                 \n",
      "    51     0.383202   0.683581   0.732093                  \n",
      "    52     0.38324    0.693842   0.733953                  \n",
      "    53     0.375001   0.706074   0.724651                  \n",
      "    54     0.378215   0.693633   0.731163                  \n",
      "    55     0.372024   0.717368   0.737674                 \n",
      "    56     0.366932   0.695834   0.739535                 \n",
      "    57     0.36732    0.688091   0.732093                 \n",
      "    58     0.364175   0.680294   0.730233                 \n",
      "    59     0.35668    0.689882   0.741395                 \n",
      "    60     0.353619   0.702233   0.734884                 \n",
      "    61     0.352414   0.699581   0.733023                 \n",
      "    62     0.346834   0.69545    0.741395                 \n",
      "    63     0.345293   0.683652   0.745116                 \n",
      "    64     0.341677   0.693287   0.744186                 \n",
      "    65     0.339284   0.676084   0.739535                 \n",
      "    66     0.333492   0.691549   0.735814                 \n",
      "    67     0.335493   0.706041   0.742326                 \n",
      "    68     0.331721   0.702028   0.735814                  \n",
      "    69     0.325184   0.703889   0.734884                  \n",
      "    70     0.320232   0.701818   0.734884                  \n",
      "    71     0.318946   0.727326   0.728372                 \n",
      "    72     0.312253   0.718634   0.738605                  \n",
      "    73     0.311812   0.73463    0.736744                 \n",
      "    74     0.309907   0.724831   0.741395                 \n",
      "    75     0.307877   0.714857   0.735814                 \n",
      "    76     0.305897   0.726367   0.740465                 \n",
      "    77     0.305184   0.715929   0.735814                 \n",
      "    78     0.302743   0.743633   0.731163                 \n",
      "    79     0.305775   0.71827    0.733953                 \n",
      "    80     0.306779   0.712701   0.749767                 \n",
      "    81     0.297113   0.734629   0.739535                 \n",
      "    82     0.293219   0.721407   0.736744                 \n",
      "    83     0.290353   0.728883   0.734884                 \n",
      "    84     0.288384   0.737594   0.745116                 \n",
      "    85     0.290597   0.739056   0.736744                 \n",
      "    86     0.288191   0.758128   0.738605                \n",
      "    87     0.283633   0.735261   0.733023                 \n",
      "    88     0.284285   0.716951   0.749767                 \n",
      "    89     0.286478   0.722437   0.744186                 \n",
      "    90     0.284908   0.734205   0.737674                 \n",
      "    91     0.283505   0.718304   0.752558                  \n",
      "    92     0.27846    0.726626   0.740465                  \n",
      "    93     0.280782   0.724125   0.733953                 \n",
      "    94     0.274553   0.728256   0.753488                 \n",
      "    95     0.270057   0.748416   0.740465                 \n",
      "    96     0.268656   0.747026   0.738605                 \n",
      "    97     0.270023   0.743606   0.738605                 \n",
      "    98     0.268692   0.747571   0.740465                 \n",
      "    99     0.267022   0.752684   0.735814                 \n",
      "   100     0.261425   0.736418   0.736744                  \n",
      "   101     0.26062    0.741442   0.743256                \n",
      "   102     0.248552   0.731004   0.742326                 \n",
      "   103     0.252363   0.742652   0.741395                 \n",
      "   104     0.254428   0.761539   0.736744                 \n",
      "   105     0.256288   0.746414   0.744186                 \n",
      "   106     0.254082   0.74295    0.746047                  \n",
      "   107     0.252778   0.753992   0.742326                 \n",
      "   108     0.248629   0.755568   0.741395                  \n",
      "   109     0.247624   0.757143   0.739535                 \n",
      "   110     0.247835   0.752479   0.746047                 \n",
      "   111     0.24829    0.754558   0.739535                 \n",
      "   112     0.248802   0.750595   0.739535                 \n",
      "   113     0.246461   0.755642   0.743256                 \n",
      "   114     0.240315   0.753102   0.748837                 \n",
      "   115     0.241958   0.751552   0.742326                 \n",
      "   116     0.233654   0.748777   0.742326                 \n",
      "   117     0.235211   0.742177   0.750698                 \n",
      "   118     0.232697   0.744226   0.747907                 \n",
      "   119     0.230731   0.759461   0.739535                 \n",
      "   120     0.231771   0.767105   0.746977                 \n",
      "   121     0.233184   0.754885   0.746047                 \n",
      "   122     0.237601   0.758274   0.746977                 \n",
      "   123     0.234183   0.765945   0.743256                 \n",
      "   124     0.231177   0.768865   0.745116                 \n",
      "   125     0.232722   0.766022   0.737674                 \n",
      "   126     0.229496   0.766939   0.744186                 \n",
      "   127     0.229583   0.764525   0.743256                 \n",
      "   128     0.22616    0.790307   0.730233                 \n",
      "   129     0.226747   0.769905   0.737674                 \n",
      "   130     0.224604   0.78399    0.742326                 \n",
      "   131     0.228277   0.768704   0.741395                 \n",
      "   132     0.223303   0.785778   0.744186                 \n",
      "   133     0.225693   0.807373   0.733953                 \n",
      "   134     0.225962   0.794309   0.740465                 \n",
      "   135     0.22585    0.79692    0.736744                 \n",
      "   136     0.220043   0.782196   0.734884                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.218872   0.792435   0.743256                 \n",
      "   138     0.212864   0.80332    0.739535                  \n",
      "   139     0.212999   0.778108   0.746977                  \n",
      "   140     0.215412   0.781958   0.742326                  \n",
      "   141     0.213701   0.771126   0.744186                  \n",
      "   142     0.206148   0.770292   0.739535                  \n",
      "   143     0.204442   0.786113   0.746047                  \n",
      "   144     0.204539   0.777607   0.743256                  \n",
      "   145     0.206301   0.78433    0.746047                  \n",
      "   146     0.209932   0.793493   0.745116                  \n",
      "   147     0.207399   0.783024   0.743256                  \n",
      "   148     0.203586   0.800206   0.738605                  \n",
      "   149     0.20508    0.797697   0.745116                  \n",
      "   150     0.200477   0.783608   0.745116                 \n",
      "   151     0.197231   0.77735    0.746047                  \n",
      "   152     0.19926    0.780702   0.749767                  \n",
      "   153     0.199551   0.78791    0.745116                 \n",
      "   154     0.199942   0.797005   0.739535                 \n",
      "   155     0.202098   0.787117   0.735814                 \n",
      "   156     0.198657   0.797124   0.736744                 \n",
      "   157     0.195109   0.78487    0.746047                 \n",
      "   158     0.193675   0.796811   0.746977                 \n",
      "   159     0.190298   0.808497   0.744186                 \n",
      "   160     0.193575   0.808997   0.738605                  \n",
      "   161     0.194633   0.809012   0.746047                 \n",
      "   162     0.196936   0.797478   0.745116                 \n",
      "   163     0.196193   0.810932   0.742326                 \n",
      "   164     0.19805    0.801472   0.746046                 \n",
      "   165     0.194848   0.787797   0.740465                 \n",
      "   166     0.191125   0.791491   0.746047                 \n",
      "   167     0.190922   0.815239   0.740465                  \n",
      "   168     0.192686   0.814181   0.744186                 \n",
      "   169     0.198077   0.806698   0.736744                  \n",
      "   170     0.192129   0.813822   0.746047                  \n",
      "   171     0.189066   0.811114   0.749767                  \n",
      "   172     0.187828   0.836048   0.745116                  \n",
      "   173     0.187922   0.827606   0.749767                  \n",
      "   174     0.188777   0.820528   0.747907                  \n",
      "   175     0.192394   0.813278   0.755349                 \n",
      "   176     0.189167   0.815037   0.748837                 \n",
      "   177     0.184803   0.822709   0.748837                 \n",
      "   178     0.181813   0.820623   0.751628                  \n",
      "   179     0.179479   0.819121   0.752558                  \n",
      "   180     0.181092   0.834516   0.746047                  \n",
      "   181     0.178798   0.828461   0.751628                  \n",
      "   182     0.182068   0.828749   0.746977                  \n",
      "   183     0.178817   0.826801   0.752558                  \n",
      "   184     0.180328   0.838307   0.745116                  \n",
      "   185     0.180226   0.832081   0.749767                  \n",
      "   186     0.179785   0.824168   0.751628                  \n",
      "   187     0.181425   0.831388   0.746047                 \n",
      "   188     0.180435   0.830525   0.747907                 \n",
      "   189     0.183752   0.828384   0.757209                 \n",
      "   190     0.179224   0.827291   0.752558                 \n",
      "   191     0.178102   0.834999   0.749767                  \n",
      "   192     0.179658   0.836836   0.751628                  \n",
      "   193     0.180045   0.831467   0.744186                  \n",
      "   194     0.180963   0.830244   0.747907                  \n",
      "   195     0.181219   0.836567   0.743256                  \n",
      "   196     0.180544   0.832357   0.751628                 \n",
      "   197     0.180017   0.845065   0.745116                 \n",
      "   198     0.179385   0.82001    0.749767                  \n",
      "   199     0.181033   0.827069   0.750698                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde56eadcc7a426eaaa81c5a0a383065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f425c612017a40d5979c8a5966c4600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.104236   0.905147   0.581395  \n",
      "    1      0.967796   0.860516   0.613953                \n",
      "    2      0.894448   0.854143   0.615814                 \n",
      "    3      0.843534   0.816633   0.632558                 \n",
      "    4      0.809574   0.79952    0.643721                 \n",
      "    5      0.781922   0.78324    0.655814                 \n",
      "    6      0.757489   0.773981   0.665116                 \n",
      "    7      0.736232   0.762812   0.660465                 \n",
      "    8      0.712212   0.74923    0.674419                 \n",
      "    9      0.694229   0.752424   0.661395                  \n",
      "    10     0.679856   0.756006   0.658605                  \n",
      "    11     0.665093   0.750009   0.67814                   \n",
      "    12     0.650106   0.723468   0.688372                  \n",
      "    13     0.631648   0.721199   0.696744                 \n",
      "    14     0.622361   0.72517    0.686512                 \n",
      "    15     0.612506   0.725148   0.689302                  \n",
      "    16     0.60533    0.698193   0.703256                  \n",
      "    17     0.595929   0.699337   0.705116                  \n",
      "    18     0.586266   0.702663   0.708837                 \n",
      "    19     0.570962   0.701248   0.700465                  \n",
      "    20     0.563522   0.684101   0.711628                 \n",
      "    21     0.556206   0.689527   0.707907                 \n",
      "    22     0.553263   0.710649   0.700465                 \n",
      "    23     0.545841   0.681764   0.712558                 \n",
      "    24     0.533985   0.684121   0.723721                 \n",
      "    25     0.518961   0.691197   0.715349                  \n",
      "    26     0.507672   0.692458   0.713488                 \n",
      "    27     0.505109   0.679494   0.711628                 \n",
      "    28     0.500933   0.68218    0.709767                 \n",
      "    29     0.498228   0.690506   0.722791                 \n",
      "    30     0.489416   0.690801   0.714419                  \n",
      "    31     0.480929   0.692598   0.704186                  \n",
      "    32     0.474552   0.676679   0.710698                 \n",
      "    33     0.465992   0.674332   0.722791                  \n",
      "    34     0.453906   0.677873   0.730233                 \n",
      "    35     0.454562   0.66855    0.727442                  \n",
      "    36     0.451646   0.676623   0.72186                   \n",
      "    37     0.440952   0.679208   0.727442                  \n",
      "    38     0.439343   0.679625   0.72                      \n",
      "    39     0.440071   0.678737   0.729302                  \n",
      "    40     0.431541   0.686123   0.724651                  \n",
      "    41     0.426493   0.689225   0.726512                 \n",
      "    42     0.419636   0.672904   0.734884                 \n",
      "    43     0.418658   0.693643   0.724651                 \n",
      "    44     0.415464   0.685234   0.732093                 \n",
      "    45     0.405646   0.686633   0.731163                 \n",
      "    46     0.398532   0.697414   0.723721                 \n",
      "    47     0.390108   0.680267   0.724651                 \n",
      "    48     0.38955    0.675576   0.729302                 \n",
      "    49     0.387366   0.687077   0.734884                 \n",
      "    50     0.385555   0.704441   0.711628                 \n",
      "    51     0.379987   0.703364   0.723721                 \n",
      "    52     0.374283   0.678898   0.72186                  \n",
      "    53     0.374128   0.674986   0.723721                 \n",
      "    54     0.375214   0.694847   0.717209                 \n",
      "    55     0.369789   0.698117   0.731163                 \n",
      "    56     0.370406   0.693553   0.723721                 \n",
      "    57     0.36002    0.698069   0.722791                 \n",
      "    58     0.358529   0.69898    0.72186                  \n",
      "    59     0.356752   0.691058   0.723721                  \n",
      "    60     0.352172   0.695721   0.724651                  \n",
      "    61     0.351948   0.6969     0.736744                  \n",
      "    62     0.347212   0.695868   0.733953                 \n",
      "    63     0.342393   0.693457   0.733953                 \n",
      "    64     0.340547   0.711047   0.722791                 \n",
      "    65     0.333866   0.716062   0.727442                  \n",
      "    66     0.329754   0.706785   0.725581                  \n",
      "    67     0.330994   0.695      0.717209                 \n",
      "    68     0.326643   0.70078    0.733953                 \n",
      "    69     0.324486   0.705466   0.732093                  \n",
      "    70     0.323439   0.746375   0.72186                  \n",
      "    71     0.321793   0.71839    0.728372                 \n",
      "    72     0.319417   0.710554   0.730233                 \n",
      "    73     0.325275   0.714781   0.729302                 \n",
      "    74     0.317204   0.709821   0.734884                 \n",
      "    75     0.314011   0.721246   0.734884                 \n",
      "    76     0.308494   0.717299   0.736744                 \n",
      "    77     0.307649   0.710355   0.731163                 \n",
      "    78     0.310539   0.719854   0.733953                  \n",
      "    79     0.303701   0.730568   0.733953                 \n",
      "    80     0.295259   0.718991   0.728372                  \n",
      "    81     0.293175   0.712236   0.732093                  \n",
      "    82     0.296517   0.745227   0.733023                  \n",
      "    83     0.292911   0.699004   0.737674                 \n",
      "    84     0.289759   0.714788   0.738605                 \n",
      "    85     0.284632   0.726127   0.738605                  \n",
      "    86     0.284032   0.718133   0.740465                  \n",
      "    87     0.282012   0.726253   0.734884                 \n",
      "    88     0.279168   0.743375   0.733953                 \n",
      "    89     0.280012   0.730813   0.742326                 \n",
      "    90     0.278804   0.726549   0.735814                 \n",
      "    91     0.275667   0.723159   0.746046                 \n",
      "    92     0.269901   0.734201   0.731163                 \n",
      "    93     0.269863   0.743464   0.729302                \n",
      "    94     0.269637   0.748237   0.730233                 \n",
      "    95     0.265204   0.734997   0.743256                 \n",
      "    96     0.269995   0.751705   0.736744                 \n",
      "    97     0.26614    0.72984    0.739535                 \n",
      "    98     0.264159   0.74212    0.733023                  \n",
      "    99     0.261421   0.758767   0.744186                  \n",
      "   100     0.263484   0.73138    0.741395                  \n",
      "   101     0.25783    0.724428   0.731163                  \n",
      "   102     0.256494   0.742344   0.731163                  \n",
      "   103     0.261922   0.749768   0.735814                  \n",
      "   104     0.259309   0.738497   0.733953                 \n",
      "   105     0.255774   0.74913    0.740465                  \n",
      "   106     0.251969   0.744858   0.731163                 \n",
      "   107     0.25654    0.761737   0.734884                 \n",
      "   108     0.249395   0.759064   0.732093                 \n",
      "   109     0.243529   0.780349   0.737674                 \n",
      "   110     0.243754   0.790929   0.729302                  \n",
      "   111     0.243225   0.778603   0.730233                 \n",
      "   112     0.245021   0.756356   0.731163                 \n",
      "   113     0.242191   0.769385   0.731163                 \n",
      "   114     0.240991   0.761382   0.738605                 \n",
      "   115     0.239066   0.783039   0.733023                 \n",
      "   116     0.238616   0.775422   0.738605                 \n",
      "   117     0.235977   0.779546   0.737674                 \n",
      "   118     0.232762   0.777078   0.743256                 \n",
      "   119     0.22925    0.775563   0.737674                 \n",
      "   120     0.228975   0.778739   0.745116                  \n",
      "   121     0.225538   0.758364   0.741395                  \n",
      "   122     0.226626   0.780479   0.741395                  \n",
      "   123     0.230201   0.762109   0.736744                  \n",
      "   124     0.230942   0.765856   0.733953                  \n",
      "   125     0.23105    0.754172   0.736744                  \n",
      "   126     0.226392   0.787335   0.733023                  \n",
      "   127     0.228214   0.768723   0.733023                  \n",
      "   128     0.225081   0.754891   0.731163                 \n",
      "   129     0.223512   0.768162   0.737674                  \n",
      "   130     0.226234   0.7786     0.740465                  \n",
      "   131     0.226366   0.762343   0.741395                  \n",
      "   132     0.228833   0.791533   0.734884                  \n",
      "   133     0.221379   0.77757    0.733953                  \n",
      "   134     0.221159   0.779228   0.734884                  \n",
      "   135     0.222234   0.793632   0.737674                 \n",
      "   136     0.216794   0.778671   0.743256                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.215243   0.784172   0.744186                 \n",
      "   138     0.214752   0.788724   0.742326                 \n",
      "   139     0.210353   0.788118   0.741395                 \n",
      "   140     0.207449   0.809384   0.737674                 \n",
      "   141     0.209979   0.814006   0.740465                 \n",
      "   142     0.206637   0.828395   0.739535                 \n",
      "   143     0.21305    0.800323   0.736744                 \n",
      "   144     0.209786   0.809353   0.733023                 \n",
      "   145     0.211258   0.834749   0.740465                \n",
      "   146     0.210368   0.804417   0.737674                 \n",
      "   147     0.200742   0.80135    0.742326                  \n",
      "   148     0.202129   0.806494   0.743256                  \n",
      "   149     0.203741   0.796991   0.739535                  \n",
      "   150     0.205977   0.809077   0.741395                  \n",
      "   151     0.201846   0.795186   0.736744                 \n",
      "   152     0.195398   0.813016   0.738605                 \n",
      "   153     0.195635   0.805231   0.744186                 \n",
      "   154     0.196193   0.814157   0.729302                 \n",
      "   155     0.200705   0.820984   0.726512                 \n",
      "   156     0.197451   0.811852   0.743256                 \n",
      "   157     0.194695   0.814921   0.734884                 \n",
      "   158     0.19407    0.805272   0.734884                 \n",
      "   159     0.193099   0.819276   0.745116                 \n",
      "   160     0.193139   0.823769   0.742326                  \n",
      "   161     0.188095   0.830147   0.737674                  \n",
      "   162     0.189364   0.813566   0.746977                 \n",
      "   163     0.19       0.804485   0.751628                 \n",
      "   164     0.18828    0.820501   0.743256                  \n",
      "   165     0.188859   0.804993   0.743256                 \n",
      "   166     0.192295   0.79626    0.750698                  \n",
      "   167     0.190031   0.792084   0.747907                 \n",
      "   168     0.188972   0.803776   0.744186                 \n",
      "   169     0.189573   0.806989   0.752558                 \n",
      "   170     0.189266   0.83288    0.736744                 \n",
      "   171     0.191831   0.819658   0.747907                 \n",
      "   172     0.186212   0.82927    0.736744                  \n",
      "   173     0.182887   0.812176   0.746977                  \n",
      "   174     0.186224   0.825595   0.741395                 \n",
      "   175     0.185685   0.839075   0.750698                  \n",
      "   176     0.188321   0.818164   0.748837                  \n",
      "   177     0.187352   0.823808   0.746047                  \n",
      "   178     0.188211   0.82452    0.738605                 \n",
      "   179     0.185427   0.828452   0.737674                 \n",
      "   180     0.189078   0.821568   0.751628                  \n",
      "   181     0.188748   0.847766   0.740465                 \n",
      "   182     0.188611   0.837378   0.748837                 \n",
      "   183     0.187449   0.823014   0.743256                  \n",
      "   184     0.183181   0.832798   0.746046                  \n",
      "   185     0.181363   0.821275   0.747907                  \n",
      "   186     0.179088   0.808058   0.737674                  \n",
      "   187     0.180325   0.815708   0.744186                  \n",
      "   188     0.177369   0.819805   0.750698                  \n",
      "   189     0.178253   0.813529   0.752558                  \n",
      "   190     0.175809   0.815548   0.744186                  \n",
      "   191     0.171954   0.825563   0.743256                  \n",
      "   192     0.17391    0.809124   0.746047                  \n",
      "   193     0.169451   0.826376   0.751628                 \n",
      "   194     0.169952   0.821188   0.748837                 \n",
      "   195     0.17428    0.811665   0.750698                  \n",
      "   196     0.179671   0.808912   0.748837                 \n",
      "   197     0.176441   0.813617   0.746977                 \n",
      "   198     0.173571   0.81245    0.747907                 \n",
      "   199     0.168776   0.833565   0.748837                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa62fba03d4495f8df4def3b89c799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475289c8c1534931b2f6b0199eb351c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.150973   0.940996   0.546977  \n",
      "    1      1.029926   0.89169    0.591628                \n",
      "    2      0.951375   0.859552   0.613953                  \n",
      "    3      0.902696   0.832889   0.634419                  \n",
      "    4      0.871975   0.831967   0.626047                  \n",
      "    5      0.84741    0.815171   0.64                     \n",
      "    6      0.821352   0.809067   0.63814                  \n",
      "    7      0.803995   0.799234   0.64186                  \n",
      "    8      0.785718   0.784086   0.653953                 \n",
      "    9      0.771021   0.779878   0.653023                  \n",
      "    10     0.760069   0.765784   0.667907                  \n",
      "    11     0.749198   0.759222   0.666977                  \n",
      "    12     0.74018    0.763735   0.658605                 \n",
      "    13     0.731205   0.767211   0.666977                 \n",
      "    14     0.719402   0.74348    0.67814                  \n",
      "    15     0.70639    0.745144   0.670698                 \n",
      "    16     0.698116   0.743984   0.67907                  \n",
      "    17     0.686683   0.73427    0.68                     \n",
      "    18     0.682832   0.725026   0.68093                   \n",
      "    19     0.67464    0.732972   0.68093                  \n",
      "    20     0.667207   0.716726   0.687442                  \n",
      "    21     0.661623   0.723053   0.690233                  \n",
      "    22     0.650255   0.732478   0.683721                  \n",
      "    23     0.639051   0.719601   0.689302                  \n",
      "    24     0.630979   0.715316   0.696744                  \n",
      "    25     0.628015   0.713757   0.690233                  \n",
      "    26     0.625543   0.709893   0.696744                  \n",
      "    27     0.615486   0.704062   0.707907                 \n",
      "    28     0.610892   0.699796   0.707907                 \n",
      "    29     0.612863   0.686153   0.708837                 \n",
      "    30     0.605978   0.692804   0.706047                 \n",
      "    31     0.599421   0.687603   0.711628                 \n",
      "    32     0.59133    0.702006   0.706977                 \n",
      "    33     0.586494   0.694069   0.707907                 \n",
      "    34     0.587359   0.691137   0.716279                  \n",
      "    35     0.578687   0.708965   0.697674                 \n",
      "    36     0.57378    0.71031    0.697674                  \n",
      "    37     0.568754   0.694999   0.706977                 \n",
      "    38     0.565813   0.678997   0.722791                  \n",
      "    39     0.564757   0.67979    0.715349                  \n",
      "    40     0.560475   0.682514   0.714419                 \n",
      "    41     0.549431   0.681995   0.701395                 \n",
      "    42     0.551128   0.674116   0.712558                 \n",
      "    43     0.54914    0.67303    0.705116                 \n",
      "    44     0.545603   0.682061   0.711628                  \n",
      "    45     0.539356   0.668186   0.72093                   \n",
      "    46     0.532029   0.679548   0.72093                  \n",
      "    47     0.534683   0.67026    0.722791                 \n",
      "    48     0.526592   0.692834   0.714419                 \n",
      "    49     0.524651   0.663768   0.717209                 \n",
      "    50     0.52234    0.664211   0.72                     \n",
      "    51     0.518006   0.674466   0.717209                 \n",
      "    52     0.521347   0.676655   0.717209                 \n",
      "    53     0.51276    0.677664   0.724651                 \n",
      "    54     0.507101   0.669529   0.72093                  \n",
      "    55     0.505723   0.665245   0.72093                  \n",
      "    56     0.49394    0.662573   0.71814                  \n",
      "    57     0.485027   0.666955   0.722791                 \n",
      "    58     0.48579    0.672495   0.731163                 \n",
      "    59     0.485873   0.676212   0.715349                 \n",
      "    60     0.482399   0.692698   0.72                     \n",
      "    61     0.481178   0.678506   0.727442                  \n",
      "    62     0.480385   0.667735   0.730233                  \n",
      "    63     0.475143   0.663437   0.732093                  \n",
      "    64     0.472047   0.671188   0.729302                  \n",
      "    65     0.471127   0.669291   0.722791                  \n",
      "    66     0.468917   0.658616   0.725581                  \n",
      "    67     0.465567   0.676849   0.713488                  \n",
      "    68     0.461232   0.668377   0.72                     \n",
      "    69     0.45924    0.664057   0.723721                 \n",
      "    70     0.458698   0.66181    0.727442                 \n",
      "    71     0.458316   0.663664   0.717209                 \n",
      "    72     0.460753   0.659918   0.72                     \n",
      "    73     0.454545   0.656443   0.72186                  \n",
      "    74     0.449804   0.661326   0.723721                 \n",
      "    75     0.447927   0.658947   0.724651                 \n",
      "    76     0.444045   0.662521   0.728372                 \n",
      "    77     0.438282   0.664681   0.727442                 \n",
      "    78     0.439032   0.65782    0.725581                 \n",
      "    79     0.433412   0.656884   0.732093                 \n",
      "    80     0.431286   0.665313   0.733953                 \n",
      "    81     0.430271   0.666068   0.735814                 \n",
      "    82     0.428299   0.67468    0.724651                 \n",
      "    83     0.427067   0.664722   0.730233                  \n",
      "    84     0.423379   0.662844   0.731163                 \n",
      "    85     0.417904   0.668186   0.729302                  \n",
      "    86     0.423239   0.665475   0.727442                  \n",
      "    87     0.418224   0.670219   0.723721                  \n",
      "    88     0.412725   0.658196   0.740465                  \n",
      "    89     0.408188   0.665452   0.727442                  \n",
      "    90     0.410894   0.654134   0.726512                  \n",
      "    91     0.410545   0.67082    0.734884                  \n",
      "    92     0.410968   0.672725   0.727442                 \n",
      "    93     0.412828   0.670616   0.726512                 \n",
      "    94     0.410453   0.666063   0.729302                 \n",
      "    95     0.403178   0.6518     0.733953                 \n",
      "    96     0.407274   0.673656   0.729302                 \n",
      "    97     0.39726    0.660728   0.733023                  \n",
      "    98     0.392517   0.664537   0.730233                  \n",
      "    99     0.385483   0.659642   0.743256                  \n",
      "   100     0.381609   0.670061   0.736744                 \n",
      "   101     0.382106   0.667409   0.732093                  \n",
      "   102     0.383303   0.672902   0.736744                  \n",
      "   103     0.380281   0.668689   0.728372                  \n",
      "   104     0.380034   0.66672    0.731163                  \n",
      "   105     0.380377   0.668079   0.728372                 \n",
      "   106     0.369423   0.67328    0.726512                  \n",
      "   107     0.361687   0.67603    0.72186                   \n",
      "   108     0.360636   0.676193   0.732093                 \n",
      "   109     0.364663   0.677106   0.735814                 \n",
      "   110     0.363303   0.675038   0.729302                 \n",
      "   111     0.363989   0.671882   0.734884                 \n",
      "   112     0.363717   0.675205   0.733023                 \n",
      "   113     0.364119   0.679282   0.735814                  \n",
      "   114     0.360117   0.683241   0.734884                 \n",
      "   115     0.357632   0.676      0.733953                 \n",
      "   116     0.357795   0.682603   0.732093                 \n",
      "   117     0.362889   0.675008   0.735814                 \n",
      "   118     0.356659   0.666608   0.743256                 \n",
      "   119     0.356484   0.673476   0.739535                 \n",
      "   120     0.352712   0.669213   0.742326                 \n",
      "   121     0.348473   0.674842   0.748837                 \n",
      "   122     0.341145   0.668303   0.746977                 \n",
      "   123     0.340601   0.675605   0.738605                 \n",
      "   124     0.344387   0.661125   0.739535                 \n",
      "   125     0.346009   0.67151    0.734884                 \n",
      "   126     0.338503   0.68543    0.731163                  \n",
      "   127     0.332507   0.671417   0.741395                  \n",
      "   128     0.330616   0.689133   0.735814                  \n",
      "   129     0.330236   0.681552   0.741395                 \n",
      "   130     0.329977   0.693368   0.743256                  \n",
      "   131     0.333781   0.677622   0.742326                 \n",
      "   132     0.330025   0.679056   0.746046                 \n",
      "   133     0.325058   0.696278   0.735814                 \n",
      "   134     0.321371   0.700615   0.743256                 \n",
      "   135     0.322435   0.69131    0.740465                 \n",
      "   136     0.320189   0.711727   0.745116                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.324683   0.701397   0.744186                 \n",
      "   138     0.32758    0.682995   0.746977                 \n",
      "   139     0.322423   0.692164   0.748837                 \n",
      "   140     0.320898   0.695841   0.738605                 \n",
      "   141     0.31862    0.687446   0.747907                 \n",
      "   142     0.313716   0.686867   0.744186                 \n",
      "   143     0.315759   0.684574   0.746977                 \n",
      "   144     0.313859   0.685972   0.742326                 \n",
      "   145     0.306163   0.698306   0.748837                 \n",
      "   146     0.30591    0.699298   0.746977                 \n",
      "   147     0.306439   0.689059   0.747907                 \n",
      "   148     0.306249   0.710654   0.733023                 \n",
      "   149     0.306679   0.693019   0.743256                 \n",
      "   150     0.300382   0.697539   0.741395                 \n",
      "   151     0.295653   0.685174   0.737674                 \n",
      "   152     0.292489   0.691397   0.742326                 \n",
      "   153     0.297776   0.703815   0.737674                 \n",
      "   154     0.2979     0.695907   0.747907                 \n",
      "   155     0.303038   0.715453   0.735814                  \n",
      "   156     0.295565   0.690749   0.742326                 \n",
      "   157     0.294777   0.705902   0.742326                 \n",
      "   158     0.299098   0.697306   0.747907                 \n",
      "   159     0.297875   0.691902   0.747907                 \n",
      "   160     0.292995   0.698208   0.742326                 \n",
      "   161     0.29483    0.709877   0.746047                  \n",
      "   162     0.291527   0.709235   0.735814                 \n",
      "   163     0.293387   0.707997   0.735814                 \n",
      "   164     0.287581   0.719133   0.740465                  \n",
      "   165     0.289359   0.697717   0.734884                 \n",
      "   166     0.277144   0.701553   0.743256                  \n",
      "   167     0.284962   0.705149   0.741395                  \n",
      "   168     0.278819   0.706746   0.736744                  \n",
      "   169     0.286031   0.708144   0.740465                  \n",
      "   170     0.27717    0.712496   0.746047                  \n",
      "   171     0.281233   0.726054   0.743256                  \n",
      "   172     0.282597   0.716722   0.749767                  \n",
      "   173     0.28463    0.707155   0.747907                  \n",
      "   174     0.282724   0.714761   0.737674                  \n",
      "   175     0.281441   0.709349   0.746047                 \n",
      "   176     0.284099   0.712666   0.749767                  \n",
      "   177     0.276282   0.721526   0.742326                  \n",
      "   178     0.278959   0.720694   0.739535                  \n",
      "   179     0.276967   0.722609   0.729302                  \n",
      "   180     0.271565   0.728566   0.733953                  \n",
      "   181     0.270228   0.728257   0.744186                  \n",
      "   182     0.26769    0.714944   0.744186                 \n",
      "   183     0.267009   0.724001   0.734884                 \n",
      "   184     0.264531   0.709516   0.739535                 \n",
      "   185     0.264854   0.712568   0.739535                 \n",
      "   186     0.262228   0.714457   0.741395                 \n",
      "   187     0.265622   0.725517   0.746047                  \n",
      "   188     0.265632   0.724533   0.738605                 \n",
      "   189     0.261286   0.721395   0.751628                 \n",
      "   190     0.258242   0.7213     0.747907                 \n",
      "   191     0.260286   0.727139   0.745116                  \n",
      "   192     0.260082   0.738712   0.748837                  \n",
      "   193     0.26147    0.713971   0.743256                  \n",
      "   194     0.260171   0.725434   0.749767                  \n",
      "   195     0.260926   0.720348   0.747907                  \n",
      "   196     0.253785   0.719815   0.741395                  \n",
      "   197     0.25731    0.717973   0.745116                  \n",
      "   198     0.256229   0.726527   0.743256                  \n",
      "   199     0.25244    0.732892   0.739535                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139c9b380f3d4cbe8f4812a05fc9c4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d9333ff75b405ebb5b8205207509ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.134566   0.9243     0.546977  \n",
      "    1      1.014498   0.900417   0.587907                \n",
      "    2      0.94848    0.856381   0.613953                  \n",
      "    3      0.898938   0.838069   0.621395                  \n",
      "    4      0.866853   0.826871   0.634419                 \n",
      "    5      0.841578   0.808134   0.637209                  \n",
      "    6      0.818671   0.798828   0.634419                  \n",
      "    7      0.797978   0.798409   0.646512                  \n",
      "    8      0.778042   0.789499   0.64186                   \n",
      "    9      0.767717   0.788496   0.650233                  \n",
      "    10     0.756099   0.788157   0.653023                  \n",
      "    11     0.747188   0.769857   0.662326                  \n",
      "    12     0.739067   0.768882   0.652093                  \n",
      "    13     0.72888    0.772992   0.671628                  \n",
      "    14     0.724571   0.751945   0.673488                 \n",
      "    15     0.712101   0.746191   0.67814                  \n",
      "    16     0.709069   0.739717   0.67814                   \n",
      "    17     0.693576   0.733774   0.68186                   \n",
      "    18     0.686645   0.73621    0.693953                 \n",
      "    19     0.678791   0.739473   0.67907                  \n",
      "    20     0.667145   0.730694   0.67814                   \n",
      "    21     0.663018   0.73191    0.677209                 \n",
      "    22     0.658415   0.725222   0.68093                   \n",
      "    23     0.652539   0.727375   0.68                      \n",
      "    24     0.640081   0.711448   0.685581                  \n",
      "    25     0.6355     0.706198   0.693023                  \n",
      "    26     0.627373   0.714852   0.68093                   \n",
      "    27     0.619649   0.70531    0.691163                  \n",
      "    28     0.616379   0.707458   0.699535                  \n",
      "    29     0.61143    0.706345   0.692093                 \n",
      "    30     0.608303   0.70435    0.688372                  \n",
      "    31     0.605792   0.714193   0.699535                  \n",
      "    32     0.602809   0.705436   0.693953                  \n",
      "    33     0.5989     0.701021   0.693023                 \n",
      "    34     0.58882    0.699229   0.697674                 \n",
      "    35     0.580757   0.695657   0.702326                 \n",
      "    36     0.573744   0.702657   0.703256                 \n",
      "    37     0.569651   0.698125   0.702326                 \n",
      "    38     0.568123   0.701019   0.694884                 \n",
      "    39     0.565013   0.71754    0.695814                 \n",
      "    40     0.559228   0.706963   0.698605                 \n",
      "    41     0.549984   0.697676   0.704186                 \n",
      "    42     0.547213   0.687417   0.710698                 \n",
      "    43     0.551421   0.681513   0.708837                 \n",
      "    44     0.549257   0.690203   0.705116                 \n",
      "    45     0.547488   0.687673   0.708837                 \n",
      "    46     0.543524   0.701231   0.715349                  \n",
      "    47     0.536879   0.687834   0.705116                 \n",
      "    48     0.529934   0.692806   0.711628                 \n",
      "    49     0.526421   0.707883   0.711628                 \n",
      "    50     0.519757   0.694369   0.706047                 \n",
      "    51     0.520394   0.670089   0.717209                 \n",
      "    52     0.5155     0.676068   0.709767                 \n",
      "    53     0.509331   0.676881   0.706977                 \n",
      "    54     0.509283   0.680696   0.709767                 \n",
      "    55     0.50856    0.666221   0.71814                  \n",
      "    56     0.503105   0.673123   0.713488                  \n",
      "    57     0.499059   0.674389   0.713488                 \n",
      "    58     0.499951   0.689951   0.711628                 \n",
      "    59     0.502092   0.674294   0.716279                 \n",
      "    60     0.493354   0.673693   0.706047                  \n",
      "    61     0.484718   0.66789    0.72093                  \n",
      "    62     0.478767   0.674172   0.716279                  \n",
      "    63     0.480053   0.676726   0.71814                   \n",
      "    64     0.472407   0.667399   0.712558                  \n",
      "    65     0.471287   0.680672   0.717209                  \n",
      "    66     0.467138   0.679129   0.715349                  \n",
      "    67     0.467452   0.675992   0.72                      \n",
      "    68     0.466613   0.66849    0.72093                  \n",
      "    69     0.470774   0.667044   0.715349                 \n",
      "    70     0.471524   0.66552    0.71814                   \n",
      "    71     0.463492   0.672719   0.710698                  \n",
      "    72     0.452657   0.67631    0.715349                  \n",
      "    73     0.445222   0.670222   0.713488                  \n",
      "    74     0.447474   0.669477   0.71814                   \n",
      "    75     0.446744   0.6643     0.72                      \n",
      "    76     0.443488   0.665826   0.71814                  \n",
      "    77     0.4435     0.663757   0.72093                   \n",
      "    78     0.438559   0.667461   0.713488                  \n",
      "    79     0.437218   0.671367   0.712558                 \n",
      "    80     0.436908   0.679664   0.712558                 \n",
      "    81     0.432409   0.680427   0.726512                 \n",
      "    82     0.430448   0.68065    0.723721                 \n",
      "    83     0.432588   0.674066   0.71907                  \n",
      "    84     0.426571   0.671226   0.717209                 \n",
      "    85     0.423212   0.669312   0.717209                 \n",
      "    86     0.416289   0.669847   0.725581                 \n",
      "    87     0.41525    0.684644   0.72186                  \n",
      "    88     0.40726    0.674716   0.729302                 \n",
      "    89     0.413807   0.671164   0.736744                 \n",
      "    90     0.412138   0.661513   0.730233                 \n",
      "    91     0.412081   0.679286   0.734884                 \n",
      "    92     0.4071     0.681935   0.72186                  \n",
      "    93     0.403618   0.680757   0.730233                 \n",
      "    94     0.401004   0.675563   0.732093                 \n",
      "    95     0.405124   0.677691   0.727442                 \n",
      "    96     0.4021     0.666252   0.730233                 \n",
      "    97     0.401393   0.686501   0.72186                  \n",
      "    98     0.406895   0.679115   0.727442                 \n",
      "    99     0.401616   0.678531   0.724651                 \n",
      "   100     0.400404   0.674261   0.722791                 \n",
      "   101     0.391372   0.678384   0.723721                 \n",
      "   102     0.38177    0.688335   0.726512                 \n",
      "   103     0.38104    0.676853   0.727442                 \n",
      "   104     0.379927   0.681788   0.723721                 \n",
      "   105     0.378264   0.679279   0.733023                \n",
      "   106     0.374846   0.681179   0.727442                 \n",
      "   107     0.376275   0.701936   0.71907                  \n",
      "   108     0.371609   0.692812   0.733023                 \n",
      "   109     0.379105   0.681894   0.735814                 \n",
      "   110     0.374634   0.688439   0.730233                 \n",
      "   111     0.369761   0.67748    0.737674                 \n",
      "   112     0.367284   0.678241   0.733953                 \n",
      "   113     0.364807   0.685287   0.733023                  \n",
      "   114     0.366642   0.680789   0.732093                 \n",
      "   115     0.360639   0.677769   0.736744                 \n",
      "   116     0.35745    0.672468   0.734884                 \n",
      "   117     0.35819    0.694759   0.737674                 \n",
      "   118     0.350237   0.699017   0.727442                 \n",
      "   119     0.350845   0.694817   0.728372                 \n",
      "   120     0.351661   0.688281   0.733023                 \n",
      "   121     0.353421   0.687794   0.743256                 \n",
      "   122     0.351056   0.694079   0.739535                 \n",
      "   123     0.347965   0.693616   0.733023                 \n",
      "   124     0.346351   0.6941     0.734884                  \n",
      "   125     0.34771    0.686325   0.729302                  \n",
      "   126     0.348418   0.691566   0.732093                 \n",
      "   127     0.344479   0.696756   0.731163                 \n",
      "   128     0.339064   0.706281   0.728372                 \n",
      "   129     0.340694   0.702417   0.730233                  \n",
      "   130     0.339871   0.697532   0.731163                  \n",
      "   131     0.339605   0.697627   0.738605                  \n",
      "   132     0.336203   0.709698   0.728372                 \n",
      "   133     0.333168   0.714755   0.726512                 \n",
      "   134     0.333319   0.723796   0.726512                 \n",
      "   135     0.333025   0.712405   0.729302                  \n",
      "   136     0.336843   0.706615   0.733023                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.339547   0.699906   0.733953                 \n",
      "   138     0.327863   0.709354   0.730233                 \n",
      "   139     0.32217    0.703036   0.732093                 \n",
      "   140     0.318526   0.710518   0.733023                 \n",
      "   141     0.315453   0.710977   0.733953                 \n",
      "   142     0.311712   0.698035   0.734884                 \n",
      "   143     0.3151     0.710241   0.739535                  \n",
      "   144     0.311166   0.710643   0.733953                 \n",
      "   145     0.312889   0.710334   0.737674                  \n",
      "   146     0.311117   0.722952   0.741395                 \n",
      "   147     0.31277    0.711446   0.727442                  \n",
      "   148     0.314004   0.710358   0.740465                 \n",
      "   149     0.312425   0.713546   0.730233                 \n",
      "   150     0.310061   0.708601   0.739535                 \n",
      "   151     0.308942   0.720654   0.736744                 \n",
      "   152     0.307779   0.701131   0.740465                 \n",
      "   153     0.308883   0.705568   0.736744                 \n",
      "   154     0.305121   0.717984   0.733023                 \n",
      "   155     0.310393   0.70927    0.727442                 \n",
      "   156     0.311769   0.720411   0.738605                 \n",
      "   157     0.305489   0.720885   0.741395                 \n",
      "   158     0.306764   0.707272   0.737674                 \n",
      "   159     0.29912    0.711009   0.737674                 \n",
      "   160     0.301797   0.722896   0.732093                 \n",
      "   161     0.296566   0.721272   0.733023                 \n",
      "   162     0.29305    0.726671   0.746977                 \n",
      "   163     0.290186   0.727795   0.726512                 \n",
      "   164     0.284537   0.719958   0.732093                 \n",
      "   165     0.284644   0.742517   0.72                      \n",
      "   166     0.281597   0.731433   0.733023                  \n",
      "   167     0.289279   0.714017   0.730233                  \n",
      "   168     0.282073   0.72494    0.735814                  \n",
      "   169     0.282067   0.717769   0.738605                 \n",
      "   170     0.278692   0.704805   0.731163                 \n",
      "   171     0.281964   0.727369   0.732093                 \n",
      "   172     0.280466   0.728075   0.727442                 \n",
      "   173     0.2806     0.724137   0.729302                 \n",
      "   174     0.278429   0.742313   0.731163                 \n",
      "   175     0.277973   0.724617   0.732093                 \n",
      "   176     0.285044   0.739516   0.724651                 \n",
      "   177     0.278958   0.733134   0.735814                 \n",
      "   178     0.278271   0.739818   0.725581                 \n",
      "   179     0.278618   0.742957   0.726512                 \n",
      "   180     0.271649   0.736111   0.731163                 \n",
      "   181     0.274673   0.740056   0.739535                  \n",
      "   182     0.273584   0.748027   0.733023                  \n",
      "   183     0.265007   0.740567   0.737674                  \n",
      "   184     0.262327   0.741666   0.732093                 \n",
      "   185     0.260293   0.725426   0.746046                 \n",
      "   186     0.26385    0.72477    0.736744                  \n",
      "   187     0.266782   0.725033   0.733953                  \n",
      "   188     0.26455    0.722808   0.746977                 \n",
      "   189     0.266104   0.721889   0.740465                  \n",
      "   190     0.263104   0.742654   0.732093                 \n",
      "   191     0.264382   0.739542   0.733023                  \n",
      "   192     0.262208   0.734278   0.731163                  \n",
      "   193     0.262216   0.741549   0.742326                  \n",
      "   194     0.26126    0.749545   0.736744                 \n",
      "   195     0.264754   0.73787    0.734884                  \n",
      "   196     0.264918   0.737796   0.732093                  \n",
      "   197     0.259466   0.732469   0.742326                  \n",
      "   198     0.258422   0.737415   0.739535                  \n",
      "   199     0.260832   0.745099   0.746977                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed477a55d944dad9adf73e5d2697e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb13a25f96f47a5805eba27254efa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.155974   0.945604   0.533953  \n",
      "    1      1.024836   0.878152   0.59907                 \n",
      "    2      0.956555   0.877377   0.596279                 \n",
      "    3      0.910201   0.834623   0.626977                 \n",
      "    4      0.87582    0.818285   0.634419                  \n",
      "    5      0.853071   0.811375   0.628837                  \n",
      "    6      0.829624   0.792894   0.653953                  \n",
      "    7      0.807661   0.789316   0.648372                  \n",
      "    8      0.785316   0.79176    0.651163                  \n",
      "    9      0.77357    0.779938   0.656744                  \n",
      "    10     0.759302   0.771792   0.665116                  \n",
      "    11     0.749506   0.775359   0.659535                  \n",
      "    12     0.738595   0.764827   0.666977                  \n",
      "    13     0.726282   0.765033   0.656744                  \n",
      "    14     0.716737   0.762882   0.665116                  \n",
      "    15     0.712259   0.74219    0.68                      \n",
      "    16     0.703278   0.745776   0.677209                  \n",
      "    17     0.697154   0.745065   0.68                      \n",
      "    18     0.688118   0.74019    0.67907                   \n",
      "    19     0.677048   0.73736    0.685581                 \n",
      "    20     0.667742   0.732657   0.685581                 \n",
      "    21     0.667385   0.726945   0.698605                 \n",
      "    22     0.661001   0.721275   0.693023                 \n",
      "    23     0.652269   0.719704   0.698605                  \n",
      "    24     0.642901   0.737824   0.68                     \n",
      "    25     0.633471   0.71989    0.687442                  \n",
      "    26     0.631976   0.718848   0.690233                 \n",
      "    27     0.62889    0.730267   0.684651                 \n",
      "    28     0.621233   0.706282   0.706977                 \n",
      "    29     0.612683   0.719259   0.696744                 \n",
      "    30     0.605531   0.701481   0.701395                 \n",
      "    31     0.598283   0.695312   0.701395                 \n",
      "    32     0.590997   0.697423   0.707907                 \n",
      "    33     0.593637   0.700034   0.697674                 \n",
      "    34     0.587311   0.694237   0.709767                  \n",
      "    35     0.587917   0.690303   0.714419                 \n",
      "    36     0.58903    0.700526   0.701395                 \n",
      "    37     0.581101   0.689901   0.706047                 \n",
      "    38     0.575815   0.697524   0.699535                 \n",
      "    39     0.569549   0.687485   0.705116                 \n",
      "    40     0.560256   0.689122   0.711628                 \n",
      "    41     0.55666    0.67767    0.72                     \n",
      "    42     0.555216   0.677667   0.714419                 \n",
      "    43     0.54961    0.673978   0.717209                 \n",
      "    44     0.546972   0.677004   0.717209                 \n",
      "    45     0.537099   0.679274   0.716279                 \n",
      "    46     0.537117   0.677815   0.714419                 \n",
      "    47     0.535873   0.675763   0.71814                  \n",
      "    48     0.53003    0.679537   0.714419                 \n",
      "    49     0.530704   0.673396   0.71907                  \n",
      "    50     0.519939   0.681033   0.714419                 \n",
      "    51     0.521874   0.671939   0.712558                 \n",
      "    52     0.514014   0.679647   0.717209                 \n",
      "    53     0.508573   0.672935   0.72186                  \n",
      "    54     0.501728   0.671491   0.72186                  \n",
      "    55     0.497003   0.669812   0.725581                  \n",
      "    56     0.496301   0.671164   0.715349                  \n",
      "    57     0.49676    0.676065   0.72093                   \n",
      "    58     0.498423   0.683545   0.716279                  \n",
      "    59     0.493579   0.687331   0.71814                  \n",
      "    60     0.48663    0.672349   0.715349                 \n",
      "    61     0.479009   0.680355   0.72                     \n",
      "    62     0.486912   0.666863   0.724651                 \n",
      "    63     0.480593   0.672667   0.71814                  \n",
      "    64     0.47869    0.662774   0.72186                  \n",
      "    65     0.47122    0.658702   0.724651                 \n",
      "    66     0.47493    0.666139   0.724651                 \n",
      "    67     0.472245   0.671943   0.716279                 \n",
      "    68     0.469521   0.660434   0.726512                 \n",
      "    69     0.463633   0.659101   0.723721                  \n",
      "    70     0.457968   0.659793   0.728372                  \n",
      "    71     0.458699   0.665291   0.728372                  \n",
      "    72     0.452277   0.670475   0.730233                 \n",
      "    73     0.454182   0.652727   0.732093                 \n",
      "    74     0.451721   0.661692   0.730233                 \n",
      "    75     0.447011   0.660724   0.729302                 \n",
      "    76     0.441514   0.653777   0.733953                 \n",
      "    77     0.437419   0.660433   0.731163                 \n",
      "    78     0.434792   0.661924   0.736744                 \n",
      "    79     0.429784   0.670578   0.730233                 \n",
      "    80     0.43798    0.67734    0.723721                 \n",
      "    81     0.440314   0.663925   0.723721                 \n",
      "    82     0.433947   0.681549   0.724651                 \n",
      "    83     0.424828   0.658103   0.733023                 \n",
      "    84     0.420443   0.667516   0.724651                 \n",
      "    85     0.415134   0.669422   0.723721                  \n",
      "    86     0.417061   0.66925    0.728372                  \n",
      "    87     0.41879    0.660068   0.730233                  \n",
      "    88     0.420054   0.671514   0.728372                  \n",
      "    89     0.420157   0.672058   0.725581                  \n",
      "    90     0.414142   0.667038   0.736744                  \n",
      "    91     0.415337   0.663545   0.725581                  \n",
      "    92     0.414235   0.65701    0.737674                  \n",
      "    93     0.40915    0.672227   0.735814                 \n",
      "    94     0.408532   0.673973   0.72186                   \n",
      "    95     0.402471   0.668007   0.725581                 \n",
      "    96     0.399988   0.682579   0.722791                 \n",
      "    97     0.395194   0.671722   0.722791                 \n",
      "    98     0.396198   0.667309   0.735814                 \n",
      "    99     0.392742   0.661697   0.735814                 \n",
      "   100     0.392877   0.658482   0.736744                 \n",
      "   101     0.38976    0.665256   0.729302                  \n",
      "   102     0.389687   0.663505   0.735814                  \n",
      "   103     0.386253   0.662958   0.730233                  \n",
      "   104     0.385092   0.672831   0.727442                  \n",
      "   105     0.386253   0.674046   0.728372                  \n",
      "   106     0.384038   0.688896   0.729302                  \n",
      "   107     0.382116   0.669546   0.731163                  \n",
      "   108     0.372836   0.671329   0.730233                  \n",
      "   109     0.368734   0.669129   0.741395                 \n",
      "   110     0.363348   0.670424   0.723721                  \n",
      "   111     0.357343   0.664079   0.734884                 \n",
      "   112     0.357523   0.668707   0.733953                  \n",
      "   113     0.356086   0.681746   0.725581                  \n",
      "   114     0.355985   0.681348   0.722791                  \n",
      "   115     0.350905   0.672628   0.741395                 \n",
      "   116     0.353995   0.672502   0.733953                 \n",
      "   117     0.352345   0.681759   0.731163                 \n",
      "   118     0.346562   0.688554   0.733023                 \n",
      "   119     0.341676   0.682074   0.733023                  \n",
      "   120     0.345728   0.697861   0.734884                  \n",
      "   121     0.344762   0.690166   0.734884                 \n",
      "   122     0.341657   0.701525   0.730233                 \n",
      "   123     0.34149    0.682737   0.734884                 \n",
      "   124     0.340715   0.691883   0.732093                 \n",
      "   125     0.338718   0.693736   0.724651                  \n",
      "   126     0.341434   0.68423    0.733023                  \n",
      "   127     0.338907   0.680532   0.736744                 \n",
      "   128     0.339761   0.679094   0.745116                 \n",
      "   129     0.339256   0.67726    0.742326                 \n",
      "   130     0.34102    0.686056   0.731163                 \n",
      "   131     0.342436   0.682779   0.727442                 \n",
      "   132     0.336722   0.68635    0.730233                 \n",
      "   133     0.334449   0.686661   0.733023                 \n",
      "   134     0.330405   0.691959   0.727442                 \n",
      "   135     0.326967   0.689006   0.732093                  \n",
      "   136     0.3268     0.683682   0.731163                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.32375    0.687612   0.736744                  \n",
      "   138     0.324856   0.684114   0.738605                  \n",
      "   139     0.320866   0.688952   0.740465                  \n",
      "   140     0.316201   0.688023   0.733953                  \n",
      "   141     0.318002   0.702826   0.725581                  \n",
      "   142     0.317057   0.707748   0.741395                 \n",
      "   143     0.320236   0.70094    0.734884                 \n",
      "   144     0.314915   0.698145   0.735814                  \n",
      "   145     0.315406   0.699749   0.737674                 \n",
      "   146     0.314537   0.701487   0.737674                  \n",
      "   147     0.312894   0.705907   0.746977                  \n",
      "   148     0.310421   0.708968   0.740465                 \n",
      "   149     0.30549    0.699249   0.743256                 \n",
      "   150     0.302556   0.716825   0.730233                 \n",
      "   151     0.303573   0.709082   0.732093                 \n",
      "   152     0.307829   0.707648   0.744186                 \n",
      "   153     0.302406   0.700419   0.733953                 \n",
      "   154     0.300696   0.700217   0.741395                 \n",
      "   155     0.301408   0.706312   0.736744                 \n",
      "   156     0.302042   0.713679   0.737674                 \n",
      "   157     0.305583   0.701064   0.745116                 \n",
      "   158     0.30101    0.706286   0.734884                 \n",
      "   159     0.304211   0.707061   0.743256                 \n",
      "   160     0.29794    0.714949   0.741395                 \n",
      "   161     0.292916   0.70858    0.742326                 \n",
      "   162     0.292646   0.712536   0.745116                 \n",
      "   163     0.29088    0.722333   0.738605                 \n",
      "   164     0.286944   0.714479   0.738605                 \n",
      "   165     0.283787   0.716381   0.733953                 \n",
      "   166     0.282649   0.709543   0.746977                 \n",
      "   167     0.28517    0.711968   0.741395                  \n",
      "   168     0.282398   0.724704   0.746047                 \n",
      "   169     0.284861   0.713919   0.741395                 \n",
      "   170     0.282474   0.723332   0.741395                 \n",
      "   171     0.283099   0.719285   0.742326                  \n",
      "   172     0.283897   0.728516   0.738605                 \n",
      "   173     0.285563   0.713875   0.742326                 \n",
      "   174     0.288309   0.73185    0.734884                 \n",
      "   175     0.282689   0.716863   0.735814                 \n",
      "   176     0.277716   0.729343   0.733953                 \n",
      "   177     0.27553    0.705884   0.741395                 \n",
      "   178     0.277527   0.715509   0.738605                 \n",
      "   179     0.272707   0.721587   0.736744                  \n",
      "   180     0.272272   0.725384   0.740465                 \n",
      "   181     0.269242   0.72852    0.737674                 \n",
      "   182     0.269348   0.736262   0.738605                 \n",
      "   183     0.265761   0.736095   0.732093                 \n",
      "   184     0.266406   0.745095   0.734884                  \n",
      "   185     0.265527   0.744172   0.737674                 \n",
      "   186     0.268045   0.731489   0.735814                 \n",
      "   187     0.264265   0.730244   0.733023                 \n",
      "   188     0.264986   0.738716   0.735814                 \n",
      "   189     0.266243   0.724987   0.742326                 \n",
      "   190     0.269499   0.734184   0.740465                 \n",
      "   191     0.26767    0.733512   0.735814                 \n",
      "   192     0.264107   0.733764   0.736744                 \n",
      "   193     0.260316   0.734307   0.738605                 \n",
      "   194     0.260621   0.736278   0.733953                 \n",
      "   195     0.256347   0.736832   0.738605                 \n",
      "   196     0.252085   0.735393   0.738605                 \n",
      "   197     0.258604   0.750614   0.733023                 \n",
      "   198     0.253196   0.736418   0.736744                 \n",
      "   199     0.25205    0.765974   0.733953                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3648ffbae39409bb7a19c37e3a0df35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592403343dd34c8e960adc03acfed106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.217676   0.919497   0.573953  \n",
      "    1      1.073613   0.881669   0.603721                 \n",
      "    2      1.003161   0.873329   0.6                      \n",
      "    3      0.956765   0.860598   0.605581                  \n",
      "    4      0.923977   0.853871   0.615814                  \n",
      "    5      0.904878   0.841225   0.627907                  \n",
      "    6      0.889341   0.842788   0.631628                  \n",
      "    7      0.872626   0.838237   0.618605                  \n",
      "    8      0.855063   0.820779   0.626047                  \n",
      "    9      0.84361    0.813244   0.637209                  \n",
      "    10     0.837585   0.813784   0.63907                  \n",
      "    11     0.823448   0.811075   0.635349                 \n",
      "    12     0.817205   0.807151   0.630698                  \n",
      "    13     0.805955   0.807854   0.63814                   \n",
      "    14     0.799675   0.811024   0.637209                 \n",
      "    15     0.79443    0.798917   0.64                     \n",
      "    16     0.788667   0.787227   0.651163                 \n",
      "    17     0.775247   0.785376   0.657674                 \n",
      "    18     0.772155   0.788514   0.650233                  \n",
      "    19     0.767797   0.779191   0.645581                  \n",
      "    20     0.763758   0.774288   0.657674                  \n",
      "    21     0.759537   0.772077   0.659535                  \n",
      "    22     0.755337   0.763569   0.671628                  \n",
      "    23     0.749242   0.776499   0.646512                 \n",
      "    24     0.752486   0.774681   0.650233                 \n",
      "    25     0.748798   0.766241   0.666977                \n",
      "    26     0.738451   0.769985   0.670698                 \n",
      "    27     0.741398   0.762832   0.661395                 \n",
      "    28     0.733752   0.756972   0.667907                 \n",
      "    29     0.727278   0.754052   0.668837                 \n",
      "    30     0.717704   0.75891    0.667907                 \n",
      "    31     0.716707   0.758139   0.667907                  \n",
      "    32     0.710712   0.75192    0.666977                  \n",
      "    33     0.705845   0.743333   0.68186                  \n",
      "    34     0.704818   0.746498   0.664186                 \n",
      "    35     0.701819   0.734614   0.666047                 \n",
      "    36     0.705096   0.739484   0.677209                 \n",
      "    37     0.704674   0.739838   0.671628                 \n",
      "    38     0.702477   0.740626   0.671628                 \n",
      "    39     0.695763   0.736118   0.675349                 \n",
      "    40     0.690568   0.730781   0.672558                 \n",
      "    41     0.685294   0.727944   0.67907                  \n",
      "    42     0.685103   0.725539   0.685581                 \n",
      "    43     0.678953   0.722945   0.683721                  \n",
      "    44     0.67112    0.726562   0.67814                  \n",
      "    45     0.671218   0.724946   0.675349                 \n",
      "    46     0.668916   0.725734   0.689302                 \n",
      "    47     0.668752   0.716635   0.687442                  \n",
      "    48     0.663766   0.713313   0.694884                  \n",
      "    49     0.658763   0.71347    0.687442                  \n",
      "    50     0.650889   0.715236   0.68186                   \n",
      "    51     0.649429   0.712509   0.682791                 \n",
      "    52     0.64678    0.709388   0.68186                   \n",
      "    53     0.64595    0.710206   0.687442                  \n",
      "    54     0.647703   0.709864   0.685581                  \n",
      "    55     0.647001   0.710532   0.684651                 \n",
      "    56     0.646684   0.698431   0.699535                  \n",
      "    57     0.638538   0.706193   0.688372                  \n",
      "    58     0.634208   0.707099   0.687442                  \n",
      "    59     0.632816   0.708807   0.690233                 \n",
      "    60     0.62962    0.698877   0.690233                 \n",
      "    61     0.631214   0.701924   0.694884                 \n",
      "    62     0.625268   0.702123   0.688372                 \n",
      "    63     0.623189   0.692196   0.698605                  \n",
      "    64     0.627808   0.696318   0.693953                 \n",
      "    65     0.623917   0.697005   0.693023                 \n",
      "    66     0.619907   0.70311    0.685581                 \n",
      "    67     0.618382   0.692733   0.693953                 \n",
      "    68     0.612649   0.695927   0.692093                  \n",
      "    69     0.610677   0.690361   0.696744                  \n",
      "    70     0.607309   0.685518   0.697674                  \n",
      "    71     0.608978   0.688496   0.697674                 \n",
      "    72     0.60395    0.687687   0.706977                  \n",
      "    73     0.598452   0.690699   0.699535                  \n",
      "    74     0.600568   0.692048   0.693023                  \n",
      "    75     0.59983    0.684421   0.695814                  \n",
      "    76     0.598446   0.694223   0.695814                  \n",
      "    77     0.595824   0.688805   0.693953                  \n",
      "    78     0.592361   0.687079   0.693953                 \n",
      "    79     0.586794   0.687369   0.694884                 \n",
      "    80     0.584104   0.67431    0.706047                 \n",
      "    81     0.578966   0.677853   0.703256                 \n",
      "    82     0.578156   0.69029    0.698605                 \n",
      "    83     0.582626   0.686686   0.694884                \n",
      "    84     0.575802   0.679059   0.714419                 \n",
      "    85     0.57562    0.684971   0.697674                 \n",
      "    86     0.571712   0.673717   0.707907                  \n",
      "    87     0.572324   0.674372   0.713488                  \n",
      "    88     0.568719   0.677141   0.711628                 \n",
      "    89     0.566723   0.689664   0.700465                  \n",
      "    90     0.570559   0.688124   0.694884                 \n",
      "    91     0.569997   0.674072   0.713488                  \n",
      "    92     0.561076   0.675697   0.706977                  \n",
      "    93     0.557722   0.688345   0.701395                  \n",
      "    94     0.555792   0.673012   0.714419                  \n",
      "    95     0.560639   0.676868   0.712558                  \n",
      "    96     0.557976   0.672019   0.709767                 \n",
      "    97     0.558108   0.67204    0.71814                  \n",
      "    98     0.550577   0.680956   0.713488                 \n",
      "    99     0.549612   0.68055    0.702326                 \n",
      "   100     0.546577   0.672735   0.706977                  \n",
      "   101     0.544261   0.673422   0.710698                  \n",
      "   102     0.545596   0.672189   0.709767                 \n",
      "   103     0.540615   0.668359   0.707907                 \n",
      "   104     0.53727    0.679475   0.710698                 \n",
      "   105     0.533873   0.670292   0.714419                  \n",
      "   106     0.532767   0.676214   0.713488                  \n",
      "   107     0.532651   0.667249   0.713488                  \n",
      "   108     0.530236   0.674692   0.71907                   \n",
      "   109     0.526998   0.672321   0.708837                 \n",
      "   110     0.526096   0.669465   0.716279                  \n",
      "   111     0.526085   0.664006   0.716279                 \n",
      "   112     0.526987   0.66827    0.71907                   \n",
      "   113     0.525405   0.672887   0.710698                  \n",
      "   114     0.528735   0.670442   0.72                      \n",
      "   115     0.521195   0.673697   0.72                      \n",
      "   116     0.519817   0.665747   0.712558                  \n",
      "   117     0.51931    0.670413   0.714419                 \n",
      "   118     0.515938   0.665197   0.712558                  \n",
      "   119     0.513475   0.667129   0.710698                 \n",
      "   120     0.510484   0.664896   0.716279                  \n",
      "   121     0.508119   0.654858   0.714419                  \n",
      "   122     0.506812   0.663878   0.717209                 \n",
      "   123     0.507864   0.675688   0.72093                  \n",
      "   124     0.49928    0.664033   0.712558                 \n",
      "   125     0.498966   0.660728   0.71814                   \n",
      "   126     0.500885   0.669859   0.715349                 \n",
      "   127     0.496829   0.664392   0.724651                 \n",
      "   128     0.49575    0.675171   0.711628                 \n",
      "   129     0.489773   0.665734   0.712558                 \n",
      "   130     0.490317   0.665626   0.71814                  \n",
      "   131     0.49225    0.666659   0.71814                   \n",
      "   132     0.495454   0.663494   0.71814                   \n",
      "   133     0.494357   0.671569   0.72186                  \n",
      "   134     0.494402   0.667551   0.712558                 \n",
      "   135     0.492912   0.663666   0.722791                 \n",
      "   136     0.496271   0.661697   0.726512                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.496349   0.671172   0.726512                  \n",
      "   138     0.486182   0.668688   0.712558                 \n",
      "   139     0.485785   0.661361   0.725581                 \n",
      "   140     0.478612   0.662492   0.72                     \n",
      "   141     0.484435   0.664819   0.71907                  \n",
      "   142     0.48218    0.665954   0.715349                  \n",
      "   143     0.481136   0.66113    0.72093                   \n",
      "   144     0.481541   0.663775   0.722791                  \n",
      "   145     0.479219   0.661479   0.730233                  \n",
      "   146     0.475304   0.666234   0.72                      \n",
      "   147     0.468668   0.662164   0.72093                   \n",
      "   148     0.468841   0.676312   0.717209                  \n",
      "   149     0.471494   0.657476   0.737674                 \n",
      "   150     0.464107   0.666092   0.731163                  \n",
      "   151     0.462251   0.664027   0.725581                  \n",
      "   152     0.465122   0.664979   0.729302                  \n",
      "   153     0.464824   0.659063   0.729302                 \n",
      "   154     0.466913   0.663889   0.726512                  \n",
      "   155     0.463901   0.657304   0.733953                  \n",
      "   156     0.462143   0.670124   0.731163                  \n",
      "   157     0.467239   0.663852   0.72186                   \n",
      "   158     0.456231   0.65936    0.723721                  \n",
      "   159     0.450284   0.671808   0.715349                  \n",
      "   160     0.450206   0.661543   0.72093                   \n",
      "   161     0.442415   0.662899   0.730233                  \n",
      "   162     0.445376   0.656352   0.732093                  \n",
      "   163     0.446049   0.667982   0.725581                 \n",
      "   164     0.443362   0.664743   0.731163                 \n",
      "   165     0.437569   0.667308   0.723721                 \n",
      "   166     0.440815   0.673716   0.731163                  \n",
      "   167     0.434486   0.669926   0.71814                   \n",
      "   168     0.43318    0.669553   0.728372                 \n",
      "   169     0.428101   0.668557   0.724651                 \n",
      "   170     0.428959   0.671246   0.72093                  \n",
      "   171     0.426973   0.668067   0.725581                 \n",
      "   172     0.430022   0.6767     0.727442                  \n",
      "   173     0.432833   0.659594   0.722791                  \n",
      "   174     0.427786   0.660724   0.727442                 \n",
      "   175     0.427901   0.668121   0.730233                 \n",
      "   176     0.426976   0.668316   0.723721                  \n",
      "   177     0.428232   0.669513   0.722791                  \n",
      "   178     0.433277   0.679562   0.717209                  \n",
      "   179     0.431609   0.668201   0.71814                   \n",
      "   180     0.431792   0.664488   0.717209                  \n",
      "   181     0.432079   0.672622   0.723721                  \n",
      "   182     0.425221   0.673924   0.722791                 \n",
      "   183     0.428479   0.661686   0.730233                 \n",
      "   184     0.429118   0.662916   0.724651                 \n",
      "   185     0.427457   0.665114   0.730233                 \n",
      "   186     0.427042   0.675602   0.727442                 \n",
      "   187     0.422333   0.667685   0.728372                 \n",
      "   188     0.418628   0.676526   0.725581                 \n",
      "   189     0.426034   0.668429   0.723721                  \n",
      "   190     0.418479   0.672617   0.734884                  \n",
      "   191     0.41202    0.665834   0.724651                 \n",
      "   192     0.415108   0.674977   0.722791                  \n",
      "   193     0.41825    0.673135   0.723721                  \n",
      "   194     0.418897   0.672965   0.71814                  \n",
      "   195     0.419742   0.658237   0.731163                  \n",
      "   196     0.416533   0.665178   0.71907                  \n",
      "   197     0.411838   0.66156    0.737674                 \n",
      "   198     0.408503   0.670142   0.728372                 \n",
      "   199     0.411074   0.661781   0.734884                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4921192a37c34316a9c0599e383c562f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346b5606e894c208a10429be5d0f02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.244904   0.920825   0.573023  \n",
      "    1      1.086649   0.896591   0.579535                \n",
      "    2      1.006884   0.872228   0.60093                 \n",
      "    3      0.953955   0.859146   0.613953                 \n",
      "    4      0.928569   0.850664   0.613953                  \n",
      "    5      0.902689   0.844384   0.621395                  \n",
      "    6      0.884264   0.833391   0.633488                 \n",
      "    7      0.867217   0.830661   0.628837                 \n",
      "    8      0.852925   0.828148   0.628837                  \n",
      "    9      0.844458   0.816923   0.629767                 \n",
      "    10     0.837401   0.814777   0.64093                  \n",
      "    11     0.818898   0.814627   0.63814                   \n",
      "    12     0.815246   0.802977   0.635349                  \n",
      "    13     0.805124   0.801985   0.63814                  \n",
      "    14     0.799468   0.797033   0.63814                  \n",
      "    15     0.792879   0.781989   0.656744                  \n",
      "    16     0.785425   0.784785   0.647442                  \n",
      "    17     0.779879   0.783356   0.651163                 \n",
      "    18     0.77664    0.775284   0.653953                  \n",
      "    19     0.769734   0.772061   0.662326                 \n",
      "    20     0.766166   0.762931   0.660465                 \n",
      "    21     0.762575   0.7676     0.659535                 \n",
      "    22     0.761509   0.760989   0.659535                 \n",
      "    23     0.748798   0.758586   0.656744                  \n",
      "    24     0.746087   0.759457   0.658605                  \n",
      "    25     0.74233    0.748392   0.673488                 \n",
      "    26     0.738718   0.758954   0.657674                  \n",
      "    27     0.727247   0.75438    0.670698                 \n",
      "    28     0.717323   0.760719   0.653023                 \n",
      "    29     0.718317   0.751056   0.665116                 \n",
      "    30     0.712532   0.741365   0.683721                  \n",
      "    31     0.712189   0.737011   0.666977                  \n",
      "    32     0.711568   0.738818   0.68093                   \n",
      "    33     0.703217   0.735137   0.675349                  \n",
      "    34     0.704741   0.732243   0.672558                  \n",
      "    35     0.697964   0.737256   0.671628                 \n",
      "    36     0.69281    0.732152   0.682791                 \n",
      "    37     0.685078   0.7301     0.683721                 \n",
      "    38     0.684135   0.730345   0.67814                   \n",
      "    39     0.678309   0.719204   0.685581                 \n",
      "    40     0.678398   0.730533   0.685581                 \n",
      "    41     0.678927   0.725288   0.68186                  \n",
      "    42     0.677574   0.720182   0.686512                  \n",
      "    43     0.676985   0.706682   0.692093                 \n",
      "    44     0.672886   0.71799    0.683721                 \n",
      "    45     0.667629   0.725759   0.67907                   \n",
      "    46     0.666517   0.714203   0.686512                  \n",
      "    47     0.664051   0.71394    0.688372                  \n",
      "    48     0.657883   0.712557   0.688372                  \n",
      "    49     0.65126    0.720122   0.685581                  \n",
      "    50     0.647626   0.706735   0.699535                  \n",
      "    51     0.650281   0.710629   0.688372                 \n",
      "    52     0.647683   0.709174   0.682791                  \n",
      "    53     0.649657   0.703948   0.695814                 \n",
      "    54     0.644663   0.695574   0.701395                  \n",
      "    55     0.64383    0.702991   0.701395                 \n",
      "    56     0.639427   0.701284   0.693953                  \n",
      "    57     0.63488    0.69093    0.692093                 \n",
      "    58     0.630471   0.70074    0.698605                 \n",
      "    59     0.630676   0.696475   0.706977                 \n",
      "    60     0.627197   0.699144   0.696744                  \n",
      "    61     0.623637   0.697283   0.695814                  \n",
      "    62     0.615505   0.695577   0.700465                  \n",
      "    63     0.617481   0.704923   0.687442                 \n",
      "    64     0.618002   0.687295   0.696744                 \n",
      "    65     0.614276   0.707273   0.697674                 \n",
      "    66     0.613034   0.68896    0.697674                 \n",
      "    67     0.619298   0.690538   0.699535                 \n",
      "    68     0.614343   0.696221   0.694884                  \n",
      "    69     0.607406   0.69042    0.701395                  \n",
      "    70     0.60108    0.688769   0.699535                  \n",
      "    71     0.602021   0.674836   0.709767                 \n",
      "    72     0.601794   0.678638   0.704186                  \n",
      "    73     0.601446   0.68412    0.696744                 \n",
      "    74     0.600824   0.679636   0.704186                 \n",
      "    75     0.597915   0.68163    0.705116                 \n",
      "    76     0.594866   0.675468   0.706047                 \n",
      "    77     0.591457   0.67915    0.703256                  \n",
      "    78     0.587526   0.665479   0.717209                  \n",
      "    79     0.583299   0.689559   0.704186                 \n",
      "    80     0.575469   0.6739     0.705116                 \n",
      "    81     0.577041   0.675064   0.705116                 \n",
      "    82     0.57323    0.667961   0.709767                  \n",
      "    83     0.571433   0.668708   0.712558                 \n",
      "    84     0.57295    0.673931   0.708837                  \n",
      "    85     0.573684   0.684341   0.705116                 \n",
      "    86     0.568394   0.67641    0.708837                 \n",
      "    87     0.568924   0.677184   0.706046                 \n",
      "    88     0.56621    0.668145   0.71814                  \n",
      "    89     0.563396   0.683606   0.716279                 \n",
      "    90     0.556082   0.669423   0.716279                 \n",
      "    91     0.555728   0.676293   0.704186                 \n",
      "    92     0.553045   0.671581   0.716279                  \n",
      "    93     0.551403   0.671821   0.71907                  \n",
      "    94     0.54781    0.671534   0.71907                  \n",
      "    95     0.543541   0.670704   0.72                     \n",
      "    96     0.542446   0.679604   0.716279                 \n",
      "    97     0.540182   0.661592   0.722791                 \n",
      "    98     0.545441   0.675868   0.717209                 \n",
      "    99     0.543399   0.66914    0.72                     \n",
      "   100     0.542789   0.662959   0.72093                  \n",
      "   101     0.541472   0.682083   0.72                     \n",
      "   102     0.537361   0.666696   0.729302                 \n",
      "   103     0.53157    0.664695   0.733023                 \n",
      "   104     0.53262    0.664112   0.728372                 \n",
      "   105     0.52922    0.66381    0.722791                 \n",
      "   106     0.537256   0.662185   0.726512                 \n",
      "   107     0.534594   0.670049   0.72                     \n",
      "   108     0.534342   0.660939   0.724651                 \n",
      "   109     0.528682   0.657318   0.727442                 \n",
      "   110     0.524866   0.668405   0.722791                 \n",
      "   111     0.523196   0.656202   0.729302                 \n",
      "   112     0.523858   0.659411   0.727442                 \n",
      "   113     0.52052    0.664795   0.728372                 \n",
      "   114     0.517407   0.663627   0.722791                 \n",
      "   115     0.516273   0.667439   0.722791                  \n",
      "   116     0.514004   0.653737   0.725581                 \n",
      "   117     0.512789   0.659899   0.722791                 \n",
      "   118     0.513018   0.656478   0.729302                 \n",
      "   119     0.508416   0.658233   0.722791                 \n",
      "   120     0.510419   0.662548   0.723721                 \n",
      "   121     0.505664   0.653577   0.722791                  \n",
      "   122     0.509261   0.662207   0.72                      \n",
      "   123     0.505021   0.644646   0.733953                  \n",
      "   124     0.499769   0.653043   0.722791                  \n",
      "   125     0.497857   0.658036   0.725581                 \n",
      "   126     0.500705   0.648266   0.728372                 \n",
      "   127     0.496807   0.659058   0.724651                  \n",
      "   128     0.497376   0.656532   0.72093                  \n",
      "   129     0.494252   0.658244   0.733953                  \n",
      "   130     0.496745   0.656796   0.723721                  \n",
      "   131     0.496678   0.655175   0.730233                 \n",
      "   132     0.49114    0.653501   0.733023                 \n",
      "   133     0.48338    0.651755   0.734884                 \n",
      "   134     0.483788   0.659521   0.733023                 \n",
      "   135     0.487917   0.655753   0.728372                 \n",
      "   136     0.494301   0.656374   0.733023                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.490168   0.660338   0.725581                 \n",
      "   138     0.487204   0.655703   0.72                     \n",
      "   139     0.48551    0.660004   0.728372                 \n",
      "   140     0.484983   0.664929   0.726512                 \n",
      "   141     0.478451   0.656033   0.732093                 \n",
      "   142     0.476417   0.658744   0.731163                 \n",
      "   143     0.476458   0.659541   0.735814                 \n",
      "   144     0.47153    0.656112   0.722791                 \n",
      "   145     0.466538   0.648769   0.732093                 \n",
      "   146     0.464111   0.65409    0.728372                 \n",
      "   147     0.463457   0.662176   0.733023                 \n",
      "   148     0.462402   0.644391   0.736744                  \n",
      "   149     0.468138   0.656016   0.736744                  \n",
      "   150     0.466092   0.655592   0.734884                  \n",
      "   151     0.467913   0.655123   0.741395                 \n",
      "   152     0.466331   0.648659   0.733953                 \n",
      "   153     0.462848   0.647065   0.735814                 \n",
      "   154     0.46248    0.645614   0.731163                 \n",
      "   155     0.46357    0.648071   0.739535                 \n",
      "   156     0.458133   0.664789   0.730233                 \n",
      "   157     0.456563   0.649105   0.738605                  \n",
      "   158     0.448578   0.649312   0.730233                  \n",
      "   159     0.453615   0.659271   0.738605                  \n",
      "   160     0.44935    0.65223    0.736744                 \n",
      "   161     0.448349   0.655204   0.735814                 \n",
      "   162     0.443742   0.656034   0.735814                 \n",
      "   163     0.440745   0.65476    0.736744                 \n",
      "   164     0.438497   0.671441   0.732093                 \n",
      "   165     0.444557   0.660173   0.732093                 \n",
      "   166     0.443891   0.651812   0.737674                 \n",
      "   167     0.442968   0.658303   0.729302                 \n",
      "   168     0.441105   0.66502    0.738605                 \n",
      "   169     0.443397   0.656844   0.728372                 \n",
      "   170     0.438722   0.654277   0.741395                  \n",
      "   171     0.438436   0.655797   0.734884                  \n",
      "   172     0.435319   0.647507   0.737674                  \n",
      "   173     0.436432   0.649721   0.737674                  \n",
      "   174     0.430069   0.65333    0.740465                  \n",
      "   175     0.425492   0.65675    0.736744                  \n",
      "   176     0.426472   0.644659   0.744186                  \n",
      "   177     0.419623   0.657146   0.732093                 \n",
      "   178     0.421664   0.651812   0.737674                 \n",
      "   179     0.425077   0.66551    0.734884                 \n",
      "   180     0.421487   0.661512   0.739535                 \n",
      "   181     0.417864   0.666264   0.733953                 \n",
      "   182     0.416695   0.666535   0.734884                 \n",
      "   183     0.419369   0.667398   0.733023                 \n",
      "   184     0.420846   0.667816   0.731163                 \n",
      "   185     0.420073   0.674375   0.729302                 \n",
      "   186     0.41919    0.666079   0.737674                 \n",
      "   187     0.41802    0.66827    0.740465                 \n",
      "   188     0.412923   0.662132   0.740465                 \n",
      "   189     0.411449   0.653551   0.740465                  \n",
      "   190     0.409144   0.656243   0.739535                 \n",
      "   191     0.406961   0.654632   0.740465                 \n",
      "   192     0.402054   0.657641   0.743256                 \n",
      "   193     0.405881   0.658074   0.738605                 \n",
      "   194     0.410797   0.662693   0.737674                 \n",
      "   195     0.414622   0.657123   0.734884                 \n",
      "   196     0.408082   0.677677   0.737674                 \n",
      "   197     0.405498   0.657424   0.737674                  \n",
      "   198     0.408969   0.661887   0.739535                  \n",
      "   199     0.408626   0.663657   0.734884                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1556ce4969824501815ef9de4ec7f590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4ec7809d0a429888c398ce32232867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.200081   0.932614   0.547907  \n",
      "    1      1.066524   0.895047   0.591628                \n",
      "    2      0.995481   0.886449   0.586977                \n",
      "    3      0.947606   0.86654    0.610233                \n",
      "    4      0.920654   0.856385   0.627907                  \n",
      "    5      0.902324   0.851145   0.613953                  \n",
      "    6      0.885162   0.841591   0.624186                  \n",
      "    7      0.869018   0.836678   0.624186                 \n",
      "    8      0.85637    0.82984    0.619535                  \n",
      "    9      0.842927   0.818796   0.632558                  \n",
      "    10     0.832619   0.813939   0.635349                  \n",
      "    11     0.828694   0.811162   0.64                     \n",
      "    12     0.825622   0.815147   0.633488                 \n",
      "    13     0.815965   0.803414   0.64186                  \n",
      "    14     0.806895   0.798163   0.645581                  \n",
      "    15     0.8011     0.792366   0.646512                  \n",
      "    16     0.792737   0.789986   0.642791                  \n",
      "    17     0.784274   0.789455   0.64                     \n",
      "    18     0.775851   0.776015   0.659535                 \n",
      "    19     0.77268    0.775221   0.661395                 \n",
      "    20     0.770497   0.775088   0.657674                 \n",
      "    21     0.764882   0.781258   0.649302                 \n",
      "    22     0.76786    0.766452   0.656744                  \n",
      "    23     0.763738   0.77084    0.656744                  \n",
      "    24     0.757081   0.757006   0.667907                 \n",
      "    25     0.753838   0.75902    0.662326                 \n",
      "    26     0.741866   0.756739   0.655814                 \n",
      "    27     0.737778   0.753262   0.664186                 \n",
      "    28     0.733187   0.747642   0.662326                 \n",
      "    29     0.726502   0.751721   0.658605                 \n",
      "    30     0.719839   0.745445   0.665116                 \n",
      "    31     0.717117   0.742492   0.663256                 \n",
      "    32     0.71612    0.742053   0.671628                 \n",
      "    33     0.709367   0.749936   0.658605                 \n",
      "    34     0.708206   0.741122   0.676279                 \n",
      "    35     0.707366   0.73623    0.677209                 \n",
      "    36     0.702867   0.728672   0.683721                  \n",
      "    37     0.696339   0.731391   0.670698                  \n",
      "    38     0.695231   0.72334    0.683721                 \n",
      "    39     0.690602   0.735554   0.667907                 \n",
      "    40     0.68951    0.725399   0.68                     \n",
      "    41     0.689207   0.721814   0.67814                  \n",
      "    42     0.684047   0.723675   0.685581                 \n",
      "    43     0.678334   0.721381   0.684651                 \n",
      "    44     0.681848   0.722664   0.67814                   \n",
      "    45     0.677997   0.722696   0.677209                  \n",
      "    46     0.670699   0.714484   0.694884                  \n",
      "    47     0.663023   0.719279   0.689302                  \n",
      "    48     0.66262    0.717605   0.684651                 \n",
      "    49     0.66148    0.708451   0.698605                 \n",
      "    50     0.657059   0.703957   0.693953                 \n",
      "    51     0.658688   0.708168   0.689302                 \n",
      "    52     0.650757   0.708561   0.684651                 \n",
      "    53     0.650694   0.712209   0.694884                 \n",
      "    54     0.652044   0.700146   0.696744                 \n",
      "    55     0.650581   0.702083   0.693953                 \n",
      "    56     0.638522   0.700519   0.700465                  \n",
      "    57     0.638026   0.710241   0.688372                 \n",
      "    58     0.636686   0.697144   0.696744                 \n",
      "    59     0.634786   0.70191    0.690233                  \n",
      "    60     0.625964   0.694903   0.706977                  \n",
      "    61     0.624455   0.703202   0.694884                  \n",
      "    62     0.628063   0.704294   0.692093                  \n",
      "    63     0.624008   0.696761   0.696744                 \n",
      "    64     0.625632   0.685955   0.716279                 \n",
      "    65     0.624411   0.683669   0.702326                 \n",
      "    66     0.614638   0.685342   0.704186                 \n",
      "    67     0.610231   0.685975   0.706047                 \n",
      "    68     0.605018   0.689969   0.706977                 \n",
      "    69     0.602093   0.691871   0.699535                 \n",
      "    70     0.598336   0.678591   0.712558                 \n",
      "    71     0.598338   0.686616   0.692093                 \n",
      "    72     0.596591   0.681356   0.708837                 \n",
      "    73     0.594435   0.68219    0.711628                 \n",
      "    74     0.586681   0.687971   0.707907                  \n",
      "    75     0.587082   0.68772    0.713488                 \n",
      "    76     0.590118   0.687388   0.711628                 \n",
      "    77     0.582242   0.680811   0.713488                 \n",
      "    78     0.587679   0.687645   0.707907                 \n",
      "    79     0.585093   0.677428   0.709767                 \n",
      "    80     0.586144   0.676356   0.712558                 \n",
      "    81     0.583095   0.681718   0.710698                 \n",
      "    82     0.58678    0.67891    0.711628                 \n",
      "    83     0.579295   0.678376   0.71814                  \n",
      "    84     0.573394   0.670901   0.705116                 \n",
      "    85     0.57583    0.677736   0.707907                 \n",
      "    86     0.573658   0.680385   0.702326                  \n",
      "    87     0.572012   0.679753   0.705116                 \n",
      "    88     0.568174   0.680098   0.708837                 \n",
      "    89     0.567157   0.679705   0.706047                 \n",
      "    90     0.562894   0.677491   0.708837                 \n",
      "    91     0.557462   0.673763   0.714419                 \n",
      "    92     0.55453    0.672175   0.714419                 \n",
      "    93     0.554219   0.671255   0.710698                  \n",
      "    94     0.556715   0.663355   0.72093                   \n",
      "    95     0.554673   0.67294    0.710698                  \n",
      "    96     0.558563   0.666834   0.708837                  \n",
      "    97     0.556927   0.671201   0.713488                 \n",
      "    98     0.550907   0.671148   0.711628                 \n",
      "    99     0.545185   0.669175   0.715349                 \n",
      "   100     0.53837    0.667186   0.71814                  \n",
      "   101     0.536762   0.67357    0.706047                 \n",
      "   102     0.540988   0.666597   0.707907                 \n",
      "   103     0.53762    0.672226   0.714419                 \n",
      "   104     0.541945   0.661556   0.711628                 \n",
      "   105     0.53904    0.665089   0.710698                 \n",
      "   106     0.536336   0.666325   0.715349                  \n",
      "   107     0.53224    0.663748   0.71814                   \n",
      "   108     0.525483   0.66726    0.713488                 \n",
      "   109     0.524942   0.674376   0.709767                 \n",
      "   110     0.524889   0.669592   0.72093                  \n",
      "   111     0.525003   0.662954   0.728372                 \n",
      "   112     0.52487    0.662113   0.722791                 \n",
      "   113     0.518813   0.667607   0.729302                 \n",
      "   114     0.515973   0.669285   0.717209                 \n",
      "   115     0.516844   0.670037   0.72093                  \n",
      "   116     0.51357    0.670797   0.714419                 \n",
      "   117     0.515323   0.666745   0.723721                 \n",
      "   118     0.515063   0.676605   0.713488                 \n",
      "   119     0.510257   0.664729   0.726512                 \n",
      "   120     0.50807    0.667597   0.724651                 \n",
      "   121     0.508537   0.660068   0.72186                  \n",
      "   122     0.505933   0.661454   0.72186                  \n",
      "   123     0.499399   0.669036   0.724651                 \n",
      "   124     0.497418   0.672421   0.722791                 \n",
      "   125     0.494011   0.658027   0.72186                  \n",
      "   126     0.495854   0.671484   0.727442                  \n",
      "   127     0.496764   0.661707   0.728372                  \n",
      "   128     0.493468   0.661481   0.71907                   \n",
      "   129     0.488799   0.664981   0.72                     \n",
      "   130     0.49221    0.666045   0.726512                 \n",
      "   131     0.489031   0.664002   0.722791                  \n",
      "   132     0.487917   0.666951   0.723721                 \n",
      "   133     0.485633   0.662183   0.727442                 \n",
      "   134     0.486192   0.662362   0.728372                  \n",
      "   135     0.479128   0.659839   0.728372                 \n",
      "   136     0.478147   0.663458   0.715349                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.484988   0.655091   0.723721                  \n",
      "   138     0.490552   0.654299   0.731163                 \n",
      "   139     0.485617   0.647288   0.734884                  \n",
      "   140     0.480163   0.66271    0.722791                  \n",
      "   141     0.476742   0.654035   0.732093                  \n",
      "   142     0.480062   0.654253   0.726512                  \n",
      "   143     0.47838    0.65855    0.727442                  \n",
      "   144     0.481001   0.645046   0.729302                  \n",
      "   145     0.481341   0.643531   0.735814                  \n",
      "   146     0.469563   0.652776   0.731163                 \n",
      "   147     0.474214   0.656387   0.723721                 \n",
      "   148     0.465198   0.654035   0.724651                 \n",
      "   149     0.466448   0.652379   0.728372                 \n",
      "   150     0.463921   0.65068    0.733023                  \n",
      "   151     0.463806   0.650396   0.727442                  \n",
      "   152     0.46364    0.645063   0.723721                 \n",
      "   153     0.459365   0.657237   0.727442                 \n",
      "   154     0.458535   0.655115   0.732093                 \n",
      "   155     0.466619   0.648815   0.729302                 \n",
      "   156     0.466815   0.644271   0.730233                 \n",
      "   157     0.463646   0.655417   0.723721                 \n",
      "   158     0.457243   0.651877   0.729302                  \n",
      "   159     0.456642   0.647289   0.734884                  \n",
      "   160     0.45492    0.651207   0.727442                  \n",
      "   161     0.458152   0.646915   0.729302                 \n",
      "   162     0.445642   0.661839   0.72093                  \n",
      "   163     0.44349    0.660275   0.725581                  \n",
      "   164     0.443868   0.650774   0.726512                 \n",
      "   165     0.448736   0.653694   0.726512                  \n",
      "   166     0.447609   0.656887   0.731163                  \n",
      "   167     0.445149   0.655035   0.729302                  \n",
      "   168     0.443595   0.656416   0.732093                  \n",
      "   169     0.442128   0.654032   0.733953                 \n",
      "   170     0.43978    0.658281   0.740465                 \n",
      "   171     0.435086   0.656896   0.730233                 \n",
      "   172     0.443552   0.660593   0.732093                 \n",
      "   173     0.443657   0.650831   0.724651                 \n",
      "   174     0.431509   0.648689   0.740465                 \n",
      "   175     0.428145   0.656476   0.726512                 \n",
      "   176     0.421833   0.66227    0.733023                 \n",
      "   177     0.427966   0.654518   0.733953                 \n",
      "   178     0.42706    0.663281   0.738605                 \n",
      "   179     0.429292   0.648773   0.737674                  \n",
      "   180     0.430401   0.657961   0.735814                  \n",
      "   181     0.427134   0.662432   0.746047                  \n",
      "   182     0.42653    0.655548   0.735814                 \n",
      "   183     0.423849   0.655423   0.737674                 \n",
      "   184     0.419762   0.65448    0.736744                 \n",
      "   185     0.417662   0.652007   0.746977                 \n",
      "   186     0.424077   0.649242   0.741395                 \n",
      "   187     0.424999   0.659877   0.733023                 \n",
      "   188     0.42265    0.643524   0.746977                  \n",
      "   189     0.421388   0.650374   0.746977                  \n",
      "   190     0.41406    0.652265   0.732093                  \n",
      "   191     0.413829   0.653899   0.733023                  \n",
      "   192     0.41142    0.657026   0.735814                  \n",
      "   193     0.408174   0.661552   0.737674                 \n",
      "   194     0.412175   0.657997   0.744186                 \n",
      "   195     0.41322    0.657155   0.737674                  \n",
      "   196     0.408217   0.663555   0.736744                  \n",
      "   197     0.405115   0.661521   0.743256                  \n",
      "   198     0.404373   0.655852   0.745116                  \n",
      "   199     0.399263   0.657905   0.737674                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8b66820cd545a4a014993eb4038f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9084ce1c2ae4d50b725735759c12e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.482401   1.000772   0.498605  \n",
      "    1      1.260751   0.94413    0.52186                 \n",
      "    2      1.165166   0.943462   0.526512                \n",
      "    3      1.105413   0.947865   0.51907                 \n",
      "    4      1.068895   0.94372    0.545116                 \n",
      "    5      1.044296   0.935887   0.541395                 \n",
      "    6      1.027249   0.929692   0.542326                \n",
      "    7      1.014559   0.925196   0.544186                \n",
      "    8      1.005219   0.917281   0.533023                \n",
      "    9      0.99916    0.920296   0.546047                \n",
      "    10     0.993578   0.927868   0.547907                  \n",
      "    11     0.984616   0.908704   0.55907                   \n",
      "    12     0.982931   0.913367   0.546047                  \n",
      "    13     0.980697   0.914153   0.551628                  \n",
      "    14     0.979772   0.908605   0.557209                  \n",
      "    15     0.972605   0.922784   0.549767                  \n",
      "    16     0.966831   0.902293   0.563721                  \n",
      "    17     0.96194    0.916167   0.567442                 \n",
      "    18     0.960207   0.907708   0.56                     \n",
      "    19     0.957489   0.882814   0.580465                 \n",
      "    20     0.954568   0.893632   0.581395                 \n",
      "    21     0.952085   0.902776   0.568372                 \n",
      "    22     0.951148   0.901868   0.570233                 \n",
      "    23     0.950085   0.892272   0.576744                 \n",
      "    24     0.943983   0.890328   0.573023                 \n",
      "    25     0.94499    0.912808   0.554419                 \n",
      "    26     0.946793   0.887502   0.593488                 \n",
      "    27     0.941416   0.870468   0.592558                 \n",
      "    28     0.941353   0.87545    0.591628                  \n",
      "    29     0.935794   0.899162   0.578605                  \n",
      "    30     0.932311   0.873231   0.586977                  \n",
      "    31     0.932712   0.877438   0.584186                 \n",
      "    32     0.931572   0.883674   0.594419                  \n",
      "    33     0.926427   0.865703   0.608372                  \n",
      "    34     0.923843   0.869782   0.605581                  \n",
      "    35     0.921404   0.881693   0.588837                 \n",
      "    36     0.923283   0.865429   0.611163                 \n",
      "    37     0.920829   0.872482   0.604651                 \n",
      "    38     0.914042   0.878273   0.588837                 \n",
      "    39     0.913828   0.874944   0.592558                 \n",
      "    40     0.914012   0.868219   0.593488                 \n",
      "    41     0.914153   0.863578   0.612093                 \n",
      "    42     0.905031   0.871205   0.59907                  \n",
      "    43     0.909536   0.864925   0.602791                 \n",
      "    44     0.914495   0.860825   0.604651                \n",
      "    45     0.910991   0.85602    0.610233                  \n",
      "    46     0.905691   0.86801    0.605581                 \n",
      "    47     0.90155    0.85715    0.614884                 \n",
      "    48     0.902715   0.855114   0.611163                 \n",
      "    49     0.903772   0.865042   0.609302                 \n",
      "    50     0.898859   0.857771   0.610233                 \n",
      "    51     0.893476   0.858666   0.618605                 \n",
      "    52     0.898593   0.853212   0.613023                 \n",
      "    53     0.891913   0.858928   0.613023                 \n",
      "    54     0.892683   0.846803   0.607442                 \n",
      "    55     0.888596   0.856488   0.612093                 \n",
      "    56     0.894583   0.849828   0.616744                 \n",
      "    57     0.890134   0.843844   0.613953                 \n",
      "    58     0.88374    0.849196   0.620465                 \n",
      "    59     0.883691   0.845742   0.626977                 \n",
      "    60     0.887395   0.844718   0.626047                 \n",
      "    61     0.88614    0.846994   0.615814                 \n",
      "    62     0.889813   0.838617   0.618605                 \n",
      "    63     0.889031   0.840562   0.626977                 \n",
      "    64     0.883503   0.837455   0.617674                 \n",
      "    65     0.880529   0.84745    0.621395                 \n",
      "    66     0.879507   0.843113   0.621395                 \n",
      "    67     0.882199   0.851239   0.612093                 \n",
      "    68     0.873783   0.847275   0.614884                 \n",
      "    69     0.873033   0.839027   0.616744                  \n",
      "    70     0.876795   0.84831    0.607442                  \n",
      "    71     0.876324   0.847804   0.616744                  \n",
      "    72     0.880147   0.839332   0.622326                  \n",
      "    73     0.87414    0.837738   0.613953                 \n",
      "    74     0.869306   0.837771   0.618605                 \n",
      "    75     0.874188   0.842023   0.616744                  \n",
      "    76     0.869968   0.836081   0.627907                 \n",
      "    77     0.867924   0.834589   0.618605                 \n",
      "    78     0.871266   0.838924   0.624186                  \n",
      "    79     0.870066   0.841282   0.624186                  \n",
      "    80     0.86688    0.831076   0.629767                  \n",
      "    81     0.863235   0.837326   0.617674                  \n",
      "    82     0.865108   0.832992   0.621395                  \n",
      "    83     0.864086   0.828618   0.629767                  \n",
      "    84     0.863887   0.830076   0.622326                  \n",
      "    85     0.860916   0.834835   0.627907                 \n",
      "    86     0.85736    0.825498   0.632558                 \n",
      "    87     0.856012   0.831895   0.624186                 \n",
      "    88     0.852624   0.83548    0.625116                 \n",
      "    89     0.851731   0.831127   0.618605                 \n",
      "    90     0.846113   0.825805   0.633488                  \n",
      "    91     0.846613   0.82722    0.627907                  \n",
      "    92     0.849955   0.821833   0.629767                 \n",
      "    93     0.85057    0.815119   0.627907                 \n",
      "    94     0.850511   0.828799   0.623256                  \n",
      "    95     0.848074   0.822253   0.625116                  \n",
      "    96     0.848898   0.829923   0.628837                  \n",
      "    97     0.849009   0.816912   0.632558                 \n",
      "    98     0.848627   0.821815   0.630698                 \n",
      "    99     0.845082   0.822893   0.634419                 \n",
      "   100     0.845448   0.821515   0.615814                 \n",
      "   101     0.84942    0.813881   0.627907                 \n",
      "   102     0.844641   0.826637   0.626047                  \n",
      "   103     0.83981    0.81003    0.627907                  \n",
      "   104     0.838867   0.824763   0.622326                  \n",
      "   105     0.844056   0.810665   0.633488                \n",
      "   106     0.845477   0.816723   0.636279                 \n",
      "   107     0.845386   0.820044   0.623256                 \n",
      "   108     0.84382    0.817771   0.628837                 \n",
      "   109     0.847805   0.809369   0.628837                  \n",
      "   110     0.839253   0.813887   0.629767                  \n",
      "   111     0.839098   0.809253   0.635349                  \n",
      "   112     0.839273   0.806996   0.629767                 \n",
      "   113     0.841763   0.807818   0.632558                 \n",
      "   114     0.838166   0.815302   0.630698                 \n",
      "   115     0.840685   0.805555   0.637209                 \n",
      "   116     0.837592   0.815316   0.634419                 \n",
      "   117     0.8398     0.818508   0.630698                 \n",
      "   118     0.840583   0.809459   0.635349                 \n",
      "   119     0.835381   0.812036   0.627907                 \n",
      "   120     0.832827   0.806925   0.632558                 \n",
      "   121     0.82934    0.808379   0.625116                 \n",
      "   122     0.834024   0.806633   0.627907                 \n",
      "   123     0.83755    0.81684    0.626977                 \n",
      "   124     0.834843   0.812917   0.63814                  \n",
      "   125     0.827299   0.807551   0.629767                 \n",
      "   126     0.824346   0.804055   0.628837                  \n",
      "   127     0.831935   0.807516   0.63814                  \n",
      "   128     0.833677   0.805431   0.635349                 \n",
      "   129     0.8331     0.814518   0.627907                 \n",
      "   130     0.828295   0.805944   0.64186                   \n",
      "   131     0.833365   0.805414   0.63907                  \n",
      "   132     0.823615   0.805644   0.637209                  \n",
      "   133     0.820381   0.802117   0.64093                  \n",
      "   134     0.81663    0.801186   0.64186                  \n",
      "   135     0.817138   0.806282   0.635349                 \n",
      "   136     0.815145   0.798872   0.636279                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.816621   0.806188   0.634419                  \n",
      "   138     0.816157   0.803176   0.624186                  \n",
      "   139     0.815535   0.80147    0.634419                  \n",
      "   140     0.815573   0.808451   0.634419                  \n",
      "   141     0.81979    0.796814   0.645581                  \n",
      "   142     0.814336   0.80835    0.63814                   \n",
      "   143     0.819391   0.799448   0.63907                   \n",
      "   144     0.817253   0.797465   0.642791                  \n",
      "   145     0.815709   0.790809   0.649302                  \n",
      "   146     0.812327   0.789999   0.648372                  \n",
      "   147     0.814775   0.790657   0.644651                  \n",
      "   148     0.815095   0.801291   0.631628                  \n",
      "   149     0.813909   0.797732   0.64                      \n",
      "   150     0.807521   0.796268   0.64186                   \n",
      "   151     0.812785   0.79462    0.636279                 \n",
      "   152     0.813932   0.804483   0.63814                  \n",
      "   153     0.815516   0.789302   0.643721                 \n",
      "   154     0.813618   0.794005   0.633488                 \n",
      "   155     0.807563   0.791096   0.651163                 \n",
      "   156     0.808463   0.79186    0.636279                  \n",
      "   157     0.803657   0.78837    0.64186                  \n",
      "   158     0.804917   0.790682   0.645581                 \n",
      "   159     0.80576    0.79072    0.64186                  \n",
      "   160     0.804131   0.795961   0.637209                  \n",
      "   161     0.805283   0.788927   0.647442                 \n",
      "   162     0.80838    0.789329   0.643721                 \n",
      "   163     0.800532   0.789042   0.646512                 \n",
      "   164     0.799647   0.784348   0.647442                 \n",
      "   165     0.802136   0.793448   0.643721                 \n",
      "   166     0.804501   0.790133   0.63907                  \n",
      "   167     0.80527    0.781882   0.644651                  \n",
      "   168     0.800918   0.792222   0.643721                  \n",
      "   169     0.79869    0.789042   0.644651               \n",
      "   170     0.797536   0.780477   0.652093                  \n",
      "   171     0.792237   0.780391   0.647442                  \n",
      "   172     0.791872   0.782194   0.643721                  \n",
      "   173     0.792834   0.781916   0.647442                 \n",
      "   174     0.796286   0.784453   0.64186                  \n",
      "   175     0.792408   0.783548   0.653953                 \n",
      "   176     0.793037   0.78009    0.646512                 \n",
      "   177     0.794024   0.78507    0.644651                 \n",
      "   178     0.79662    0.779029   0.650233                  \n",
      "   179     0.795573   0.776873   0.651163                  \n",
      "   180     0.796773   0.778842   0.653023                 \n",
      "   181     0.793233   0.784155   0.649302                 \n",
      "   182     0.792133   0.786082   0.649302                 \n",
      "   183     0.79075    0.78154    0.649302                 \n",
      "   184     0.786763   0.779353   0.642791                 \n",
      "   185     0.787872   0.784927   0.642791                 \n",
      "   186     0.790641   0.781694   0.643721                 \n",
      "   187     0.791904   0.777932   0.647442                  \n",
      "   188     0.793869   0.785376   0.647442                  \n",
      "   189     0.790925   0.783536   0.652093                 \n",
      "   190     0.790913   0.783028   0.643721                 \n",
      "   191     0.793743   0.777387   0.647442                  \n",
      "   192     0.790111   0.788991   0.649302                  \n",
      "   193     0.78757    0.784378   0.656744                  \n",
      "   194     0.786126   0.776934   0.653953                 \n",
      "   195     0.786123   0.775644   0.648372                 \n",
      "   196     0.786528   0.780882   0.644651                 \n",
      "   197     0.784511   0.775326   0.654884                 \n",
      "   198     0.783843   0.7792     0.653023                 \n",
      "   199     0.781483   0.772121   0.655814                 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d43a112d9c41de8e8c1975a385505f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdffa1868a5642e6a38dcac2ea8cee11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.433133   1.017674   0.494884  \n",
      "    1      1.240191   0.986397   0.504186                \n",
      "    2      1.145162   0.960639   0.52186                  \n",
      "    3      1.110524   0.951477   0.524651                 \n",
      "    4      1.074957   0.946807   0.517209                \n",
      "    5      1.045149   0.944522   0.52186                 \n",
      "    6      1.031478   0.932223   0.533023                 \n",
      "    7      1.020832   0.93812    0.533953                 \n",
      "    8      1.008343   0.921268   0.556279                \n",
      "    9      0.994239   0.914407   0.563721                  \n",
      "    10     0.992079   0.921398   0.541395                  \n",
      "    11     0.982828   0.917017   0.55907                   \n",
      "    12     0.979013   0.928719   0.540465                  \n",
      "    13     0.973977   0.925108   0.553488                 \n",
      "    14     0.973619   0.909087   0.565581                  \n",
      "    15     0.969365   0.900769   0.569302                 \n",
      "    16     0.964671   0.912961   0.557209                  \n",
      "    17     0.956906   0.910238   0.56093                  \n",
      "    18     0.95698    0.893963   0.579535                 \n",
      "    19     0.955853   0.905856   0.571163                 \n",
      "    20     0.954375   0.904014   0.579535                 \n",
      "    21     0.947769   0.886483   0.583256                 \n",
      "    22     0.946901   0.905669   0.573023                 \n",
      "    23     0.941318   0.890713   0.590698                 \n",
      "    24     0.941959   0.891822   0.586047                 \n",
      "    25     0.937917   0.882673   0.584186                 \n",
      "    26     0.936658   0.895916   0.581395                  \n",
      "    27     0.935061   0.898403   0.583256                  \n",
      "    28     0.932327   0.88322    0.589767                 \n",
      "    29     0.928053   0.891247   0.591628                  \n",
      "    30     0.921623   0.875363   0.605581                  \n",
      "    31     0.922949   0.875587   0.59907                   \n",
      "    32     0.924863   0.880745   0.59907                  \n",
      "    33     0.921607   0.88063    0.586047                 \n",
      "    34     0.925107   0.869881   0.607442                 \n",
      "    35     0.921861   0.892074   0.596279                 \n",
      "    36     0.916669   0.869798   0.59814                   \n",
      "    37     0.910445   0.88193    0.586977                 \n",
      "    38     0.911623   0.862854   0.59907                  \n",
      "    39     0.91451    0.873384   0.60186                  \n",
      "    40     0.915645   0.870642   0.60093                  \n",
      "    41     0.912099   0.877785   0.592558                 \n",
      "    42     0.911104   0.870199   0.590698                 \n",
      "    43     0.910293   0.868253   0.606512                 \n",
      "    44     0.910992   0.880731   0.591628                 \n",
      "    45     0.908119   0.861273   0.607442                 \n",
      "    46     0.907968   0.868576   0.60093                  \n",
      "    47     0.907133   0.863674   0.606512                 \n",
      "    48     0.902537   0.862904   0.610233                  \n",
      "    49     0.898846   0.870234   0.59907                  \n",
      "    50     0.899625   0.862955   0.597209                 \n",
      "    51     0.896235   0.861494   0.613023                 \n",
      "    52     0.891491   0.863202   0.59907                  \n",
      "    53     0.894023   0.856936   0.612093                 \n",
      "    54     0.88808    0.866495   0.602791                 \n",
      "    55     0.889282   0.863898   0.610233                 \n",
      "    56     0.883845   0.851173   0.617674                 \n",
      "    57     0.885862   0.856462   0.612093                 \n",
      "    58     0.890839   0.854661   0.613953                 \n",
      "    59     0.887213   0.859178   0.618605                 \n",
      "    60     0.887883   0.855439   0.613953                 \n",
      "    61     0.883689   0.854469   0.610233                 \n",
      "    62     0.878121   0.849783   0.615814                 \n",
      "    63     0.87938    0.855604   0.604651                 \n",
      "    64     0.87939    0.850314   0.614884                 \n",
      "    65     0.879974   0.844967   0.613023                \n",
      "    66     0.883417   0.845926   0.613953                 \n",
      "    67     0.881021   0.844693   0.621395                 \n",
      "    68     0.876328   0.840225   0.622326                 \n",
      "    69     0.871568   0.842018   0.614884                 \n",
      "    70     0.876846   0.83221    0.624186                 \n",
      "    71     0.869511   0.839588   0.622326                 \n",
      "    72     0.865837   0.84422    0.618605                 \n",
      "    73     0.872156   0.836244   0.627907                 \n",
      "    74     0.867701   0.835403   0.621395                 \n",
      "    75     0.866148   0.838836   0.626047                  \n",
      "    76     0.867513   0.84545    0.624186                 \n",
      "    77     0.865776   0.839318   0.615814                 \n",
      "    78     0.86834    0.836125   0.624186                 \n",
      "    79     0.866934   0.831153   0.624186                 \n",
      "    80     0.864788   0.83274    0.626977                  \n",
      "    81     0.863794   0.844558   0.625116                  \n",
      "    82     0.862567   0.834467   0.622326                 \n",
      "    83     0.85987    0.828275   0.630698                 \n",
      "    84     0.861417   0.834239   0.624186                  \n",
      "    85     0.861729   0.830149   0.632558                  \n",
      "    86     0.864515   0.83626    0.627907                  \n",
      "    87     0.863163   0.83372    0.626977                  \n",
      "    88     0.861022   0.832068   0.628837                  \n",
      "    89     0.860518   0.829459   0.621395                 \n",
      "    90     0.85325    0.830626   0.630698                  \n",
      "    91     0.853147   0.828115   0.628837                 \n",
      "    92     0.85531    0.831251   0.626977                 \n",
      "    93     0.85425    0.826005   0.630698                 \n",
      "    94     0.852975   0.822542   0.629767                 \n",
      "    95     0.848754   0.82064    0.631628                 \n",
      "    96     0.850203   0.822772   0.63907                  \n",
      "    97     0.851714   0.817196   0.64093                  \n",
      "    98     0.852546   0.821272   0.622326                 \n",
      "    99     0.848594   0.824519   0.632558                 \n",
      "   100     0.845244   0.818585   0.634419                 \n",
      "   101     0.844825   0.818905   0.633488                 \n",
      "   102     0.841996   0.817364   0.634419                 \n",
      "   103     0.844916   0.832212   0.620465                  \n",
      "   104     0.844211   0.823888   0.634419                  \n",
      "   105     0.840874   0.822795   0.636279                  \n",
      "   106     0.839947   0.811589   0.636279                  \n",
      "   107     0.839735   0.820633   0.628837                  \n",
      "   108     0.840157   0.825931   0.630698                  \n",
      "   109     0.840948   0.817673   0.64186                  \n",
      "   110     0.844586   0.817254   0.630698                  \n",
      "   111     0.8397     0.813714   0.63907                  \n",
      "   112     0.838748   0.80998    0.642791                 \n",
      "   113     0.835739   0.814536   0.636279                  \n",
      "   114     0.836556   0.814262   0.632558                 \n",
      "   115     0.837156   0.823478   0.63814                   \n",
      "   116     0.835992   0.817241   0.633488                  \n",
      "   117     0.832205   0.821781   0.636279                  \n",
      "   118     0.83328    0.810036   0.646512                  \n",
      "   119     0.83292    0.818499   0.64                      \n",
      "   120     0.832849   0.811704   0.633488                 \n",
      "   121     0.83328    0.817747   0.637209                 \n",
      "   122     0.83149    0.823311   0.629767                 \n",
      "   123     0.831322   0.805966   0.63814                  \n",
      "   124     0.82831    0.807293   0.63814                  \n",
      "   125     0.828333   0.806748   0.63814                  \n",
      "   126     0.830858   0.810459   0.64186                  \n",
      "   127     0.827367   0.816782   0.630698                 \n",
      "   128     0.828873   0.812683   0.632558                 \n",
      "   129     0.822248   0.808962   0.631628                 \n",
      "   130     0.818472   0.807395   0.63814                  \n",
      "   131     0.815938   0.802822   0.631628                  \n",
      "   132     0.819702   0.803043   0.63907                  \n",
      "   133     0.816905   0.803213   0.64093                 \n",
      "   134     0.820201   0.811538   0.637209                 \n",
      "   135     0.819172   0.807415   0.636279                 \n",
      "   136     0.820834   0.805314   0.64                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.817988   0.801861   0.636279                \n",
      "   138     0.813352   0.805201   0.650233                 \n",
      "   139     0.813587   0.804117   0.64                      \n",
      "   140     0.81601    0.805634   0.642791                  \n",
      "   141     0.817039   0.798422   0.643721                  \n",
      "   142     0.812342   0.796168   0.64093                   \n",
      "   143     0.809308   0.803083   0.64                     \n",
      "   144     0.810057   0.801161   0.63814                  \n",
      "   145     0.811438   0.80192    0.645581                 \n",
      "   146     0.810829   0.801909   0.645581                 \n",
      "   147     0.81253    0.801853   0.645581                 \n",
      "   148     0.811579   0.797813   0.647442                  \n",
      "   149     0.813605   0.794485   0.642791                  \n",
      "   150     0.812917   0.794993   0.64093                  \n",
      "   151     0.813292   0.793478   0.642791                  \n",
      "   152     0.809693   0.797838   0.643721                  \n",
      "   153     0.805795   0.80255    0.645581                  \n",
      "   154     0.805794   0.795545   0.64186                   \n",
      "   155     0.806078   0.790251   0.646512                 \n",
      "   156     0.807294   0.79267    0.643721                 \n",
      "   157     0.806839   0.795051   0.645581                  \n",
      "   158     0.804472   0.796449   0.64                      \n",
      "   159     0.803328   0.786837   0.649302                  \n",
      "   160     0.806232   0.790414   0.654884                  \n",
      "   161     0.806144   0.790906   0.651163                  \n",
      "   162     0.806165   0.792312   0.653023                  \n",
      "   163     0.809687   0.793422   0.647442                 \n",
      "   164     0.807125   0.795191   0.645581                  \n",
      "   165     0.807748   0.791434   0.654884                  \n",
      "   166     0.807671   0.784335   0.648372                 \n",
      "   167     0.808242   0.789039   0.648372                 \n",
      "   168     0.799143   0.7857     0.650233                 \n",
      "   169     0.798148   0.786963   0.655814                 \n",
      "   170     0.797588   0.785228   0.653953                 \n",
      "   171     0.797962   0.793049   0.64186                  \n",
      "   172     0.799373   0.794253   0.652093                 \n",
      "   173     0.797392   0.787033   0.645581                  \n",
      "   174     0.798507   0.78531    0.650233                 \n",
      "   175     0.7959     0.787242   0.647442                  \n",
      "   176     0.797993   0.785473   0.653023                  \n",
      "   177     0.801215   0.78864    0.653953                 \n",
      "   178     0.798582   0.787143   0.653023                 \n",
      "   179     0.790644   0.789531   0.652093                 \n",
      "   180     0.786337   0.786613   0.650233                 \n",
      "   181     0.792254   0.786935   0.649302                 \n",
      "   182     0.791554   0.78304    0.646512                 \n",
      "   183     0.786525   0.789652   0.650233                 \n",
      "   184     0.78769    0.786498   0.646512                 \n",
      "   185     0.791669   0.787131   0.648372                 \n",
      "   186     0.792194   0.784263   0.650233                 \n",
      "   187     0.7889     0.788775   0.644651                 \n",
      "   188     0.78433    0.782709   0.651163                 \n",
      "   189     0.785304   0.782757   0.649302                 \n",
      "   190     0.786076   0.781359   0.649302                 \n",
      "   191     0.787176   0.777855   0.655814                 \n",
      "   192     0.787749   0.778777   0.648372                 \n",
      "   193     0.786663   0.782566   0.652093                 \n",
      "   194     0.787083   0.777872   0.657674                  \n",
      "   195     0.785665   0.78183    0.648372                  \n",
      "   196     0.785105   0.779753   0.660465                 \n",
      "   197     0.778522   0.777928   0.651163                 \n",
      "   198     0.778926   0.775071   0.650233                  \n",
      "   199     0.778304   0.782947   0.653023                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91bade2b3e4077a4a5ba32c70be83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9891e0d97f38486a815ce5c68fedbc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.491115   1.024013   0.510698  \n",
      "    1      1.265206   0.970507   0.513488                \n",
      "    2      1.163285   0.950665   0.51814                 \n",
      "    3      1.098837   0.938725   0.545116                \n",
      "    4      1.070419   0.938848   0.532093                 \n",
      "    5      1.048349   0.938858   0.536744                \n",
      "    6      1.029549   0.934343   0.538605                 \n",
      "    7      1.014612   0.928456   0.546977                 \n",
      "    8      1.002113   0.913182   0.56093                 \n",
      "    9      0.995491   0.923464   0.546047                  \n",
      "    10     0.98633    0.93354    0.55814                   \n",
      "    11     0.980015   0.915812   0.55814                  \n",
      "    12     0.975119   0.903621   0.56186                   \n",
      "    13     0.969479   0.911689   0.563721                  \n",
      "    14     0.967753   0.911953   0.553488                  \n",
      "    15     0.963333   0.914922   0.565581                  \n",
      "    16     0.961956   0.913089   0.55907                   \n",
      "    17     0.965671   0.900822   0.572093                 \n",
      "    18     0.9587     0.908744   0.571163                 \n",
      "    19     0.953674   0.901178   0.564651                 \n",
      "    20     0.948764   0.889389   0.575814                 \n",
      "    21     0.947797   0.898325   0.571163                 \n",
      "    22     0.945161   0.892687   0.580465                 \n",
      "    23     0.937787   0.899648   0.563721                 \n",
      "    24     0.936443   0.894587   0.586047                 \n",
      "    25     0.932799   0.88477    0.582326                 \n",
      "    26     0.932724   0.876326   0.587907                  \n",
      "    27     0.931055   0.88529    0.587907                  \n",
      "    28     0.932294   0.889719   0.574884                  \n",
      "    29     0.929019   0.874208   0.602791                  \n",
      "    30     0.924016   0.872997   0.597209                  \n",
      "    31     0.930401   0.870628   0.607442                  \n",
      "    32     0.929168   0.892196   0.583256                  \n",
      "    33     0.925547   0.880541   0.591628                  \n",
      "    34     0.923881   0.879891   0.60186                   \n",
      "    35     0.919583   0.880072   0.583256                  \n",
      "    36     0.919711   0.870895   0.60186                  \n",
      "    37     0.915836   0.871848   0.602791                 \n",
      "    38     0.912701   0.867134   0.604651                 \n",
      "    39     0.918149   0.865677   0.604651                 \n",
      "    40     0.913028   0.872422   0.607442                 \n",
      "    41     0.90895    0.87226    0.608372                  \n",
      "    42     0.908099   0.865955   0.606512                 \n",
      "    43     0.901566   0.859475   0.619535                 \n",
      "    44     0.900719   0.862599   0.608372                 \n",
      "    45     0.897071   0.858536   0.615814                 \n",
      "    46     0.896211   0.855799   0.622326                 \n",
      "    47     0.897443   0.858777   0.612093                 \n",
      "    48     0.899595   0.858715   0.596279                 \n",
      "    49     0.897385   0.867314   0.59814                  \n",
      "    50     0.89211    0.859346   0.608372                 \n",
      "    51     0.888585   0.854241   0.605581                 \n",
      "    52     0.892164   0.847063   0.611163                 \n",
      "    53     0.89238    0.855549   0.612093                 \n",
      "    54     0.887788   0.85408    0.612093                 \n",
      "    55     0.887912   0.859286   0.612093                 \n",
      "    56     0.88241    0.84434    0.629767                 \n",
      "    57     0.882838   0.844593   0.620465                 \n",
      "    58     0.881183   0.847257   0.607442                 \n",
      "    59     0.881161   0.853108   0.611163                 \n",
      "    60     0.883357   0.840861   0.623256                 \n",
      "    61     0.87633    0.845773   0.616744                 \n",
      "    62     0.877699   0.837172   0.625116                 \n",
      "    63     0.871293   0.846302   0.613023                 \n",
      "    64     0.875782   0.839258   0.623256                 \n",
      "    65     0.87132    0.83756    0.625116                 \n",
      "    66     0.865091   0.833123   0.624186                 \n",
      "    67     0.866046   0.833077   0.632558                 \n",
      "    68     0.869104   0.835148   0.634419                 \n",
      "    69     0.873565   0.84079    0.619535                 \n",
      "    70     0.874749   0.836001   0.618605                 \n",
      "    71     0.870822   0.836466   0.625116                 \n",
      "    72     0.871294   0.83607    0.621395                 \n",
      "    73     0.863554   0.836659   0.618605                 \n",
      "    74     0.864411   0.835968   0.626047                 \n",
      "    75     0.870218   0.825992   0.637209                 \n",
      "    76     0.867775   0.842455   0.616744                \n",
      "    77     0.862695   0.83365    0.627907                 \n",
      "    78     0.859687   0.848964   0.626977                 \n",
      "    79     0.861234   0.831134   0.625116                 \n",
      "    80     0.856432   0.828189   0.625116                  \n",
      "    81     0.853423   0.82919    0.621395                  \n",
      "    82     0.857026   0.82615    0.636279                  \n",
      "    83     0.856018   0.82656    0.621395                  \n",
      "    84     0.862662   0.824042   0.629767                  \n",
      "    85     0.862168   0.826938   0.633488                  \n",
      "    86     0.86075    0.834724   0.625116                  \n",
      "    87     0.860487   0.824218   0.627907                  \n",
      "    88     0.856681   0.834701   0.626047                  \n",
      "    89     0.856375   0.827812   0.627907                  \n",
      "    90     0.857464   0.815089   0.634419                  \n",
      "    91     0.849565   0.819412   0.632558                 \n",
      "    92     0.848499   0.82366    0.629767                 \n",
      "    93     0.847762   0.816776   0.63907                  \n",
      "    94     0.847969   0.817163   0.629767                 \n",
      "    95     0.852522   0.821445   0.628837                 \n",
      "    96     0.845995   0.817421   0.633488                 \n",
      "    97     0.847991   0.822112   0.633488                 \n",
      "    98     0.848362   0.811758   0.629767                 \n",
      "    99     0.846273   0.819532   0.630698                 \n",
      "   100     0.846231   0.820605   0.624186                 \n",
      "   101     0.844804   0.820475   0.633488                 \n",
      "   102     0.840255   0.80324    0.64093                   \n",
      "   103     0.8368     0.821209   0.632558                  \n",
      "   104     0.834063   0.80448    0.636279                 \n",
      "   105     0.837897   0.805047   0.643721                 \n",
      "   106     0.8411     0.819478   0.634419                 \n",
      "   107     0.836387   0.811236   0.636279                 \n",
      "   108     0.842168   0.81221    0.634419                 \n",
      "   109     0.83554    0.812695   0.632558                 \n",
      "   110     0.83246    0.810597   0.632558                 \n",
      "   111     0.835513   0.806862   0.64                      \n",
      "   112     0.834164   0.809689   0.631628                 \n",
      "   113     0.834873   0.810028   0.634419                 \n",
      "   114     0.831371   0.802573   0.64                     \n",
      "   115     0.832575   0.810081   0.629767                 \n",
      "   116     0.832915   0.802951   0.636279                 \n",
      "   117     0.829409   0.799933   0.632558                 \n",
      "   118     0.831324   0.813536   0.632558                 \n",
      "   119     0.8329     0.811851   0.636279                  \n",
      "   120     0.828947   0.798948   0.63814                  \n",
      "   121     0.829195   0.806803   0.637209                 \n",
      "   122     0.828072   0.803162   0.63907                  \n",
      "   123     0.827794   0.80065    0.634419                 \n",
      "   124     0.826769   0.812609   0.628837                 \n",
      "   125     0.821651   0.808116   0.634419                 \n",
      "   126     0.820736   0.80414    0.63814                  \n",
      "   127     0.822242   0.803733   0.626977                 \n",
      "   128     0.822433   0.798225   0.63814                   \n",
      "   129     0.821791   0.801215   0.637209                  \n",
      "   130     0.821359   0.798108   0.63907                  \n",
      "   131     0.818983   0.797239   0.63907                   \n",
      "   132     0.821413   0.797988   0.64                     \n",
      "   133     0.823185   0.794028   0.63907                  \n",
      "   134     0.818955   0.796074   0.63814                   \n",
      "   135     0.820646   0.794218   0.646512                  \n",
      "   136     0.821667   0.794035   0.642791                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   137     0.823222   0.792093   0.64                      \n",
      "   138     0.823582   0.796077   0.636279                  \n",
      "   139     0.823469   0.794306   0.64186                   \n",
      "   140     0.817486   0.795449   0.653953                 \n",
      "   141     0.81274    0.795613   0.643721                 \n",
      "   142     0.808042   0.79144    0.634419                 \n",
      "   143     0.811174   0.796446   0.64093                  \n",
      "   144     0.809395   0.794654   0.643721                 \n",
      "   145     0.808781   0.784308   0.644651                  \n",
      "   146     0.805423   0.790751   0.648372                 \n",
      "   147     0.805025   0.789586   0.635349                 \n",
      "   148     0.809957   0.793449   0.63907                  \n",
      "   149     0.810362   0.78946    0.64186                 \n",
      "   150     0.808567   0.792277   0.63814                  \n",
      "   151     0.803658   0.786265   0.652093                 \n",
      "   152     0.803764   0.783497   0.650233                 \n",
      "   153     0.800268   0.790494   0.646512                 \n",
      "   154     0.808894   0.784826   0.644651                  \n",
      "   155     0.808714   0.788828   0.642791                  \n",
      "   156     0.805539   0.78983    0.643721                 \n",
      "   157     0.803931   0.796404   0.636279                 \n",
      "   158     0.801676   0.783202   0.649302                 \n",
      "   159     0.79814    0.790499   0.646512                 \n",
      "   160     0.801447   0.784611   0.649302                 \n",
      "   161     0.803365   0.790248   0.63814                  \n",
      "   162     0.802083   0.782364   0.648372                  \n",
      "   163     0.795885   0.786687   0.642791                  \n",
      "   164     0.79516    0.790688   0.64186                   \n",
      "   165     0.794315   0.789548   0.64                     \n",
      "   166     0.792927   0.782159   0.648372                 \n",
      "   167     0.789996   0.78225    0.651163                 \n",
      "   168     0.793476   0.784735   0.647442                 \n",
      "   169     0.795729   0.780902   0.652093                 \n",
      "   170     0.794667   0.782431   0.651163                  \n",
      "   171     0.800425   0.7872     0.63814                  \n",
      "   172     0.800947   0.778251   0.652093                 \n",
      "   173     0.802168   0.780049   0.648372                 \n",
      "   174     0.798188   0.786256   0.64186                   \n",
      "   175     0.796172   0.776674   0.651163                 \n",
      "   176     0.78804    0.779478   0.64093                  \n",
      "   177     0.792798   0.777273   0.645581                 \n",
      "   178     0.791378   0.786436   0.645581                  \n",
      "   179     0.788529   0.785302   0.647442                 \n",
      "   180     0.789089   0.779698   0.645581                 \n",
      "   181     0.785516   0.777476   0.649302                 \n",
      "   182     0.785524   0.783989   0.651163                 \n",
      "   183     0.786283   0.776246   0.659535                 \n",
      "   184     0.784448   0.777124   0.650233                  \n",
      "   185     0.782982   0.776205   0.657674                  \n",
      "   186     0.782154   0.77428    0.648372                 \n",
      "   187     0.78512    0.772726   0.655814                 \n",
      "   188     0.782913   0.771659   0.653953                 \n",
      "   189     0.783096   0.771521   0.654884                  \n",
      "   190     0.785933   0.774103   0.655814                  \n",
      "   191     0.784101   0.767845   0.660465                  \n",
      "   192     0.778065   0.778855   0.652093                 \n",
      "   193     0.774165   0.771617   0.658605                  \n",
      "   194     0.77314    0.7761     0.648372                  \n",
      "   195     0.780328   0.767709   0.654884                  \n",
      "   196     0.784014   0.770726   0.649302                  \n",
      "   197     0.786752   0.770354   0.646512                  \n",
      "   198     0.786393   0.777168   0.649302                  \n",
      "   199     0.783303   0.770934   0.647442                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309ed5d668114fe582a441a3e96641d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651d3472fbb146e58f4d14e77d87d231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.006022   0.876711   0.582326  \n",
      "    1      0.88284    0.796643   0.631628                  \n",
      "    2      0.813375   0.768548   0.649302                  \n",
      "    3      0.753734   0.737399   0.666977                  \n",
      "    4      0.715231   0.716099   0.673488                  \n",
      "    5      0.676938   0.702387   0.688372                  \n",
      "    6      0.643796   0.695684   0.690233                  \n",
      "    7      0.608759   0.67079    0.702326                  \n",
      "    8      0.579553   0.668894   0.714419                  \n",
      "    9      0.554862   0.695301   0.703256                  \n",
      "    10     0.533957   0.681545   0.712558                  \n",
      "    11     0.514223   0.684999   0.714419                  \n",
      "    12     0.495356   0.704271   0.707907                  \n",
      "    13     0.484206   0.682496   0.722791                  \n",
      "    14     0.465846   0.685365   0.725581                  \n",
      "    15     0.452164   0.69557    0.71814                   \n",
      "    16     0.432343   0.689574   0.730233                  \n",
      "    17     0.415029   0.677322   0.732093                  \n",
      "    18     0.406196   0.683547   0.728372                  \n",
      "    19     0.399109   0.706136   0.727442                  \n",
      "    20     0.386124   0.729134   0.715349                  \n",
      "    21     0.373575   0.694759   0.728372                  \n",
      "    22     0.370362   0.687673   0.740465                  \n",
      "    23     0.359785   0.682427   0.744186                  \n",
      "    24     0.351226   0.7401     0.733953                  \n",
      "    25     0.341474   0.675249   0.748837                  \n",
      "    26     0.330749   0.708553   0.733953                  \n",
      "    27     0.3285     0.728006   0.735814                  \n",
      "    28     0.32023    0.69769    0.743256                  \n",
      "    29     0.311377   0.699514   0.738605                  \n",
      "    30     0.30848    0.709782   0.747907                  \n",
      "    31     0.304561   0.718824   0.737674                  \n",
      "    32     0.301403   0.742343   0.735814                  \n",
      "    33     0.299464   0.734935   0.738605                  \n",
      "    34     0.287989   0.749264   0.736744                  \n",
      "    35     0.291132   0.714638   0.743256                  \n",
      "    36     0.287191   0.715393   0.742326                  \n",
      "    37     0.288624   0.728988   0.737674                  \n",
      "    38     0.277837   0.718981   0.736744                  \n",
      "    39     0.268468   0.737374   0.733953                  \n",
      "    40     0.266883   0.723695   0.749767                  \n",
      "    41     0.266972   0.715798   0.751628                  \n",
      "    42     0.263728   0.720149   0.748837                  \n",
      "    43     0.261598   0.74892    0.743256                  \n",
      "    44     0.257254   0.727438   0.741395                  \n",
      "    45     0.255883   0.719316   0.740465                  \n",
      "    46     0.253097   0.722611   0.746047                  \n",
      "    47     0.255207   0.779481   0.746047                  \n",
      "    48     0.254413   0.708278   0.752558                  \n",
      "    49     0.249528   0.720486   0.747907                  \n",
      "    50     0.245186   0.748874   0.747907                  \n",
      "    51     0.242933   0.755555   0.739535                  \n",
      "    52     0.240555   0.746469   0.746977                  \n",
      "    53     0.236444   0.734727   0.741395                  \n",
      "    54     0.227939   0.747239   0.746977                  \n",
      "    55     0.226777   0.757292   0.740465                  \n",
      "    56     0.221311   0.743201   0.739535                  \n",
      "    57     0.222746   0.787793   0.742326                  \n",
      "    58     0.225481   0.792453   0.740465                  \n",
      "    59     0.223218   0.729602   0.747907                  \n",
      "    60     0.219502   0.773008   0.75814                   \n",
      "    61     0.21681    0.767934   0.739535                  \n",
      "    62     0.219061   0.804932   0.738605                  \n",
      "    63     0.222458   0.771876   0.742326                  \n",
      "    64     0.217329   0.764768   0.755349                  \n",
      "    65     0.212679   0.768372   0.755349                  \n",
      "    66     0.216216   0.792607   0.744186                  \n",
      "    67     0.21458    0.811566   0.744186                  \n",
      "    68     0.21116    0.799076   0.748837                  \n",
      "    69     0.20962    0.769932   0.740465                  \n",
      "    70     0.206245   0.793827   0.750698                  \n",
      "    71     0.199566   0.786447   0.746977                  \n",
      "    72     0.197922   0.757254   0.746047                  \n",
      "    73     0.195205   0.768817   0.746977                  \n",
      "    74     0.195562   0.762685   0.756279                  \n",
      "    75     0.194818   0.788385   0.743256                  \n",
      "    76     0.19549    0.793221   0.756279                  \n",
      "    77     0.19728    0.821467   0.746047                  \n",
      "    78     0.199108   0.814093   0.752558                  \n",
      "    79     0.194936   0.780193   0.752558                  \n",
      "    80     0.188254   0.79385    0.753488                  \n",
      "    81     0.189035   0.81287    0.747907                  \n",
      "    82     0.189496   0.818463   0.747907                  \n",
      "    83     0.188642   0.806557   0.746047                  \n",
      "    84     0.187621   0.784406   0.753488                  \n",
      "    85     0.183615   0.787188   0.75907                   \n",
      "    86     0.18507    0.783525   0.747907                  \n",
      "    87     0.18305    0.829586   0.736744                  \n",
      "    88     0.186265   0.795069   0.749767                  \n",
      "    89     0.182972   0.759019   0.76                      \n",
      "    90     0.177315   0.8027     0.746047                  \n",
      "    91     0.176826   0.767423   0.750698                  \n",
      "    92     0.177061   0.81567    0.752558                  \n",
      "    93     0.17575    0.779948   0.753488                  \n",
      "    94     0.173671   0.810816   0.75814                   \n",
      "    95     0.174768   0.830713   0.746977                  \n",
      "    96     0.179753   0.801175   0.752558                  \n",
      "    97     0.180336   0.768974   0.762791                  \n",
      "    98     0.17777    0.797405   0.752558                  \n",
      "    99     0.173576   0.785232   0.76                      \n",
      "   100     0.169525   0.808198   0.749767                  \n",
      "   101     0.173558   0.804131   0.76186                   \n",
      "   102     0.171888   0.81633    0.750698                  \n",
      "   103     0.176723   0.805538   0.75907                   \n",
      "   104     0.174331   0.788838   0.76                      \n",
      "   105     0.176457   0.778623   0.766512                  \n",
      "   106     0.175357   0.821806   0.746047                  \n",
      "   107     0.171556   0.806322   0.755349                  \n",
      "   108     0.169304   0.815967   0.762791                  \n",
      "   109     0.166914   0.82255    0.754419                  \n",
      "   110     0.168787   0.849182   0.76186                   \n",
      "   111     0.168703   0.806774   0.754419                  \n",
      "   112     0.169123   0.847301   0.76093                   \n",
      "   113     0.170944   0.821652   0.752558                  \n",
      "   114     0.169829   0.825958   0.75814                   \n",
      "   115     0.167299   0.829042   0.757209                  \n",
      "   116     0.16255    0.80762    0.763721                  \n",
      "   117     0.162062   0.831705   0.756279                  \n",
      "   118     0.163358   0.809689   0.75814                   \n",
      "   119     0.159633   0.818683   0.762791                  \n",
      "   120     0.159777   0.859568   0.737674                  \n",
      "   121     0.159761   0.833099   0.757209                  \n",
      "   122     0.161737   0.855403   0.752558                  \n",
      "   123     0.160876   0.823408   0.754419                  \n",
      "   124     0.16249    0.820156   0.752558                  \n",
      "   125     0.158447   0.833489   0.76                      \n",
      "   126     0.156754   0.830424   0.76093                   \n",
      "   127     0.154132   0.83229    0.75814                   \n",
      "   128     0.155266   0.836434   0.755349                  \n",
      "   129     0.157247   0.836576   0.76093                   \n",
      "   130     0.157838   0.869687   0.750698                  \n",
      "   131     0.159398   0.844479   0.753488                  \n",
      "   132     0.160692   0.855117   0.76093                   \n",
      "   133     0.159953   0.857853   0.757209                  \n",
      "   134     0.162694   0.830689   0.763721                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.16008    0.849052   0.754419                  \n",
      "   136     0.158054   0.874804   0.754419                  \n",
      "   137     0.155279   0.828827   0.76093                   \n",
      "   138     0.158853   0.83837    0.754419                  \n",
      "   139     0.154028   0.852043   0.747907                  \n",
      "   140     0.154988   0.861919   0.742326                  \n",
      "   141     0.156717   0.899527   0.744186                  \n",
      "   142     0.156368   0.836459   0.754419                  \n",
      "   143     0.153855   0.832753   0.76                      \n",
      "   144     0.154586   0.820117   0.75814                   \n",
      "   145     0.151219   0.842086   0.754419                  \n",
      "   146     0.151607   0.859681   0.76093                   \n",
      "   147     0.154791   0.863075   0.750698                  \n",
      "   148     0.153302   0.840243   0.765581                  \n",
      "   149     0.155949   0.853644   0.747907                  \n",
      "   150     0.148867   0.822158   0.75907                   \n",
      "   151     0.147207   0.84183    0.75907                   \n",
      "   152     0.150329   0.877027   0.751628                  \n",
      "   153     0.151277   0.837312   0.752558                  \n",
      "   154     0.147939   0.869962   0.742326                  \n",
      "   155     0.145915   0.866263   0.749767                  \n",
      "   156     0.145803   0.855212   0.756279                  \n",
      "   157     0.145796   0.896501   0.748837                  \n",
      "   158     0.142441   0.843891   0.763721                  \n",
      "   159     0.144705   0.845819   0.750698                  \n",
      "   160     0.143819   0.821971   0.76093                   \n",
      "   161     0.139947   0.849517   0.764651                  \n",
      "   162     0.143457   0.828299   0.764651                  \n",
      "   163     0.144522   0.861152   0.76                      \n",
      "   164     0.143282   0.847315   0.757209                  \n",
      "   165     0.144364   0.862525   0.75814                   \n",
      "   166     0.145107   0.81175    0.753488                  \n",
      "   167     0.14448    0.819119   0.763721                  \n",
      "   168     0.14676    0.833941   0.754419                  \n",
      "   169     0.147284   0.846499   0.755349                  \n",
      "   170     0.143644   0.876297   0.751628                  \n",
      "   171     0.141344   0.858745   0.76093                   \n",
      "   172     0.137418   0.876386   0.748837                  \n",
      "   173     0.134651   0.859968   0.754419                  \n",
      "   174     0.136127   0.85851    0.754419                  \n",
      "   175     0.141145   0.8981     0.76093                   \n",
      "   176     0.143073   0.897158   0.746977                  \n",
      "   177     0.142846   0.87112    0.750698                  \n",
      "   178     0.142308   0.861355   0.757209                  \n",
      "   179     0.140728   0.883301   0.75907                   \n",
      "   180     0.139203   0.864426   0.771163                  \n",
      "   181     0.138089   0.905247   0.746977                  \n",
      "   182     0.139812   0.856702   0.75907                   \n",
      "   183     0.138926   0.879972   0.76186                   \n",
      "   184     0.136256   0.873963   0.75814                   \n",
      "   185     0.138413   0.874087   0.748837                  \n",
      "   186     0.139417   0.861853   0.755349                  \n",
      "   187     0.137224   0.832253   0.75814                   \n",
      "   188     0.135801   0.872798   0.756279                  \n",
      "   189     0.136525   0.879438   0.762791                  \n",
      "   190     0.139921   0.887418   0.753488                  \n",
      "   191     0.1378     0.860392   0.754419                  \n",
      "   192     0.137834   0.851275   0.75907                   \n",
      "   193     0.136413   0.871532   0.752558                  \n",
      "   194     0.134948   0.868979   0.75814                   \n",
      "   195     0.13628    0.877589   0.756279                  \n",
      "   196     0.133694   0.874245   0.757209                  \n",
      "   197     0.135702   0.866761   0.75814                   \n",
      "   198     0.133672   0.885512   0.756279                  \n",
      "   199     0.132423   0.880331   0.756279                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a93c7394b4143b296b97105a26b4b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39bd4e0c0f543f5bc0c0ce1e3470b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.995034   0.880999   0.597209  \n",
      "    1      0.870525   0.806414   0.636279                  \n",
      "    2      0.801556   0.770646   0.652093                  \n",
      "    3      0.746669   0.749455   0.661395                  \n",
      "    4      0.703771   0.744053   0.672558                  \n",
      "    5      0.662781   0.708417   0.693954                  \n",
      "    6      0.626158   0.725025   0.686512                  \n",
      "    7      0.59405    0.7034     0.695814                  \n",
      "    8      0.567825   0.712137   0.691163                  \n",
      "    9      0.544895   0.683891   0.707907                  \n",
      "    10     0.519328   0.730012   0.706977                  \n",
      "    11     0.500221   0.704677   0.703256                  \n",
      "    12     0.48125    0.70229    0.703256                  \n",
      "    13     0.465183   0.72544    0.705116                  \n",
      "    14     0.454831   0.72579    0.716279                  \n",
      "    15     0.439924   0.692184   0.707907                  \n",
      "    16     0.426555   0.71908    0.724651                  \n",
      "    17     0.415575   0.680359   0.746977                  \n",
      "    18     0.402792   0.680102   0.738605                  \n",
      "    19     0.396605   0.702444   0.714419                  \n",
      "    20     0.384972   0.719261   0.72186                   \n",
      "    21     0.373561   0.704378   0.738605                  \n",
      "    22     0.363869   0.669771   0.735814                  \n",
      "    23     0.356647   0.76585    0.711628                  \n",
      "    24     0.35299    0.719414   0.727442                  \n",
      "    25     0.343341   0.725752   0.739535                  \n",
      "    26     0.33595    0.714111   0.726512                  \n",
      "    27     0.327255   0.719629   0.733023                  \n",
      "    28     0.324848   0.705731   0.736744                  \n",
      "    29     0.318995   0.745562   0.729302                  \n",
      "    30     0.310866   0.718647   0.741395                  \n",
      "    31     0.305882   0.704387   0.738605                  \n",
      "    32     0.300716   0.77609    0.712558                  \n",
      "    33     0.307322   0.75421    0.711628                  \n",
      "    34     0.300678   0.750907   0.71814                   \n",
      "    35     0.301433   0.747742   0.730233                  \n",
      "    36     0.295938   0.785137   0.722791                  \n",
      "    37     0.297285   0.720657   0.743256                  \n",
      "    38     0.287038   0.731138   0.734884                  \n",
      "    39     0.280154   0.765117   0.735814                  \n",
      "    40     0.275266   0.686845   0.754419                  \n",
      "    41     0.271196   0.728846   0.739535                  \n",
      "    42     0.264015   0.72487    0.747907                  \n",
      "    43     0.260361   0.741141   0.746047                  \n",
      "    44     0.256266   0.716417   0.747907                  \n",
      "    45     0.251862   0.792119   0.737674                  \n",
      "    46     0.248272   0.767185   0.739535                  \n",
      "    47     0.245912   0.761112   0.738605                  \n",
      "    48     0.239582   0.765934   0.747907                  \n",
      "    49     0.232279   0.743425   0.744186                  \n",
      "    50     0.229284   0.73545    0.746977                  \n",
      "    51     0.230787   0.796342   0.741395                  \n",
      "    52     0.231368   0.761286   0.754419                  \n",
      "    53     0.233514   0.74132    0.746977                  \n",
      "    54     0.230162   0.740336   0.752558                  \n",
      "    55     0.22865    0.764428   0.748837                  \n",
      "    56     0.228933   0.781569   0.746047                  \n",
      "    57     0.227733   0.776256   0.753488                  \n",
      "    58     0.224765   0.790462   0.745116                  \n",
      "    59     0.224623   0.78318    0.753488                  \n",
      "    60     0.221757   0.770386   0.756279                  \n",
      "    61     0.217612   0.774827   0.76                      \n",
      "    62     0.220013   0.772847   0.749767                  \n",
      "    63     0.217437   0.786994   0.744186                  \n",
      "    64     0.211628   0.756863   0.746977                  \n",
      "    65     0.21144    0.786412   0.743256                  \n",
      "    66     0.207942   0.786788   0.747907                  \n",
      "    67     0.204211   0.810459   0.747907                  \n",
      "    68     0.205332   0.761443   0.75814                   \n",
      "    69     0.20486    0.794661   0.742326                  \n",
      "    70     0.205603   0.775425   0.747907                  \n",
      "    71     0.20692    0.824288   0.735814                  \n",
      "    72     0.207344   0.773289   0.753488                  \n",
      "    73     0.204033   0.812819   0.740465                  \n",
      "    74     0.203111   0.790751   0.747907                  \n",
      "    75     0.200602   0.788109   0.748837                  \n",
      "    76     0.199591   0.837138   0.733953                  \n",
      "    77     0.204082   0.822462   0.745116                  \n",
      "    78     0.204363   0.777373   0.749767                  \n",
      "    79     0.199012   0.785653   0.746047                  \n",
      "    80     0.195815   0.821881   0.751628                  \n",
      "    81     0.194193   0.817526   0.747907                  \n",
      "    82     0.192502   0.833687   0.738605                  \n",
      "    83     0.18941    0.803068   0.743256                  \n",
      "    84     0.186766   0.791615   0.743256                  \n",
      "    85     0.187606   0.793515   0.750698                  \n",
      "    86     0.186369   0.787686   0.743256                  \n",
      "    87     0.182712   0.770205   0.744186                  \n",
      "    88     0.182211   0.811988   0.742326                  \n",
      "    89     0.181303   0.824774   0.741395                  \n",
      "    90     0.184158   0.833508   0.749767                  \n",
      "    91     0.189454   0.781485   0.76093                   \n",
      "    92     0.186274   0.812675   0.745116                  \n",
      "    93     0.18316    0.766274   0.750698                  \n",
      "    94     0.181024   0.843183   0.748837                  \n",
      "    95     0.179798   0.815132   0.750698                  \n",
      "    96     0.176907   0.822787   0.742326                  \n",
      "    97     0.181546   0.809664   0.746977                  \n",
      "    98     0.179772   0.798183   0.746977                  \n",
      "    99     0.17918    0.796273   0.76186                   \n",
      "   100     0.179571   0.800927   0.754419                  \n",
      "   101     0.175865   0.829719   0.757209                  \n",
      "   102     0.177752   0.81959    0.744186                  \n",
      "   103     0.174567   0.827792   0.742326                  \n",
      "   104     0.17062    0.803208   0.752558                  \n",
      "   105     0.170162   0.809293   0.755349                  \n",
      "   106     0.169597   0.800207   0.752558                  \n",
      "   107     0.166287   0.807844   0.733023                  \n",
      "   108     0.169495   0.803839   0.747907                  \n",
      "   109     0.169921   0.820373   0.750698                  \n",
      "   110     0.170036   0.799948   0.751628                  \n",
      "   111     0.166485   0.803008   0.76186                   \n",
      "   112     0.169765   0.78269    0.76186                   \n",
      "   113     0.174788   0.800746   0.751628                  \n",
      "   114     0.17382    0.814476   0.750698                  \n",
      "   115     0.172535   0.83223    0.740465                  \n",
      "   116     0.171054   0.818354   0.741395                  \n",
      "   117     0.16913    0.842919   0.744186                  \n",
      "   118     0.164861   0.809476   0.748837                  \n",
      "   119     0.160726   0.802396   0.756279                  \n",
      "   120     0.161136   0.870495   0.744186                  \n",
      "   121     0.161254   0.827354   0.747907                  \n",
      "   122     0.160531   0.830618   0.748837                  \n",
      "   123     0.157867   0.84435    0.752558                  \n",
      "   124     0.157788   0.817996   0.748837                  \n",
      "   125     0.159778   0.834226   0.750698                  \n",
      "   126     0.163347   0.845485   0.76093                   \n",
      "   127     0.16528    0.833175   0.749767                  \n",
      "   128     0.163704   0.85026    0.746047                  \n",
      "   129     0.164471   0.816006   0.753488                  \n",
      "   130     0.166828   0.834176   0.746047                  \n",
      "   131     0.161845   0.7985     0.754419                  \n",
      "   132     0.159156   0.80426    0.754419                  \n",
      "   133     0.158676   0.876307   0.738605                  \n",
      "   134     0.156262   0.8155     0.755349                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.160052   0.849511   0.745116                  \n",
      "   136     0.161183   0.830924   0.747907                  \n",
      "   137     0.15899    0.856273   0.756279                  \n",
      "   138     0.15925    0.84099    0.754419                  \n",
      "   139     0.15667    0.829057   0.756279                  \n",
      "   140     0.153507   0.832885   0.756279                  \n",
      "   141     0.153649   0.850866   0.742326                  \n",
      "   142     0.149983   0.831184   0.753488                  \n",
      "   143     0.146016   0.848595   0.751628                  \n",
      "   144     0.145848   0.837137   0.751628                  \n",
      "   145     0.148702   0.831761   0.750698                  \n",
      "   146     0.147656   0.821588   0.754419                  \n",
      "   147     0.147665   0.876179   0.744186                  \n",
      "   148     0.150297   0.839648   0.741395                  \n",
      "   149     0.152008   0.844122   0.745116                  \n",
      "   150     0.148334   0.832618   0.747907                  \n",
      "   151     0.144518   0.844253   0.749767                  \n",
      "   152     0.146733   0.836785   0.755349                  \n",
      "   153     0.146094   0.84526    0.754419                  \n",
      "   154     0.150124   0.865053   0.746047                  \n",
      "   155     0.148201   0.855399   0.745116                  \n",
      "   156     0.146897   0.871161   0.755349                  \n",
      "   157     0.146049   0.877308   0.744186                  \n",
      "   158     0.147871   0.855383   0.746047                  \n",
      "   159     0.151058   0.856145   0.754419                  \n",
      "   160     0.148198   0.828177   0.752558                  \n",
      "   161     0.150983   0.864726   0.749767                  \n",
      "   162     0.149517   0.832694   0.749767                  \n",
      "   163     0.150343   0.864503   0.748837                  \n",
      "   164     0.150198   0.853717   0.752558                  \n",
      "   165     0.147371   0.842523   0.76                      \n",
      "   166     0.150721   0.825178   0.752558                  \n",
      "   167     0.148663   0.831496   0.746977                  \n",
      "   168     0.149109   0.856467   0.753488                  \n",
      "   169     0.147445   0.838832   0.757209                  \n",
      "   170     0.144539   0.855943   0.746047                  \n",
      "   171     0.144177   0.870955   0.743256                  \n",
      "   172     0.145331   0.849171   0.755349                  \n",
      "   173     0.144272   0.856497   0.747907                  \n",
      "   174     0.143729   0.828005   0.752558                  \n",
      "   175     0.141318   0.837844   0.748837                  \n",
      "   176     0.143768   0.840362   0.747907                  \n",
      "   177     0.143241   0.864314   0.755349                  \n",
      "   178     0.142776   0.853244   0.751628                  \n",
      "   179     0.143822   0.836155   0.755349                  \n",
      "   180     0.143055   0.84004    0.76093                   \n",
      "   181     0.140152   0.856309   0.75907                   \n",
      "   182     0.140649   0.827897   0.756279                  \n",
      "   183     0.141063   0.811127   0.76                      \n",
      "   184     0.139933   0.866845   0.747907                  \n",
      "   185     0.137283   0.854416   0.751628                  \n",
      "   186     0.138503   0.852444   0.757209                  \n",
      "   187     0.13958    0.864422   0.752558                  \n",
      "   188     0.137968   0.874178   0.741395                  \n",
      "   189     0.135799   0.860292   0.746047                  \n",
      "   190     0.135947   0.846456   0.749767                  \n",
      "   191     0.13468    0.860464   0.753488                  \n",
      "   192     0.135117   0.86877    0.749767                  \n",
      "   193     0.131506   0.880299   0.746977                  \n",
      "   194     0.13539    0.855507   0.746047                  \n",
      "   195     0.133686   0.848243   0.757209                  \n",
      "   196     0.13488    0.851325   0.750698                  \n",
      "   197     0.135604   0.858333   0.754419                  \n",
      "   198     0.133735   0.849611   0.75814                   \n",
      "   199     0.134533   0.85992    0.748837                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed12a9f3bb5f4fb48c8dcb78edb00782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed9e6acacf546199db39f6ac86f4c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.002352   0.922159   0.568372  \n",
      "    1      0.880987   0.835007   0.617674                  \n",
      "    2      0.811493   0.761075   0.659535                  \n",
      "    3      0.751411   0.738704   0.671628                  \n",
      "    4      0.703734   0.721668   0.685581                  \n",
      "    5      0.666691   0.710375   0.687442                  \n",
      "    6      0.640272   0.727133   0.684651                  \n",
      "    7      0.607853   0.713275   0.685581                  \n",
      "    8      0.579034   0.694413   0.708837                  \n",
      "    9      0.555893   0.691838   0.701395                  \n",
      "    10     0.531884   0.661518   0.717209                  \n",
      "    11     0.508757   0.676069   0.724651                  \n",
      "    12     0.490291   0.709585   0.717209                  \n",
      "    13     0.471972   0.663581   0.727442                  \n",
      "    14     0.453787   0.643054   0.728372                  \n",
      "    15     0.437861   0.657142   0.730233                  \n",
      "    16     0.425084   0.677687   0.729302                  \n",
      "    17     0.41546    0.685164   0.723721                  \n",
      "    18     0.407995   0.699073   0.727442                  \n",
      "    19     0.399318   0.674974   0.734884                  \n",
      "    20     0.386914   0.66597    0.732093                  \n",
      "    21     0.377081   0.700195   0.729302                  \n",
      "    22     0.371698   0.669784   0.746047                  \n",
      "    23     0.360152   0.697303   0.735814                  \n",
      "    24     0.355757   0.734255   0.72                      \n",
      "    25     0.35025    0.711411   0.731163                  \n",
      "    26     0.338038   0.663678   0.751628                  \n",
      "    27     0.324606   0.680033   0.751628                  \n",
      "    28     0.312812   0.678689   0.742326                  \n",
      "    29     0.307347   0.683131   0.744186                  \n",
      "    30     0.302072   0.708319   0.731163                  \n",
      "    31     0.299605   0.742327   0.727442                  \n",
      "    32     0.297069   0.731454   0.725581                  \n",
      "    33     0.301119   0.759344   0.746977                  \n",
      "    34     0.297774   0.708073   0.748837                  \n",
      "    35     0.295397   0.696281   0.747907                  \n",
      "    36     0.289322   0.701894   0.746047                  \n",
      "    37     0.290298   0.718091   0.740465                  \n",
      "    38     0.291314   0.714279   0.739535                  \n",
      "    39     0.285127   0.696403   0.746977                  \n",
      "    40     0.279856   0.713873   0.740465                  \n",
      "    41     0.275793   0.742252   0.747907                  \n",
      "    42     0.26433    0.721875   0.746047                  \n",
      "    43     0.263344   0.707533   0.75907                   \n",
      "    44     0.260479   0.737864   0.742326                  \n",
      "    45     0.259509   0.730388   0.742326                  \n",
      "    46     0.251749   0.73863    0.745116                  \n",
      "    47     0.244337   0.72       0.757209                  \n",
      "    48     0.235948   0.745534   0.742326                  \n",
      "    49     0.237167   0.763577   0.749767                  \n",
      "    50     0.23501    0.722174   0.753488                  \n",
      "    51     0.241655   0.746362   0.747907                  \n",
      "    52     0.235155   0.751435   0.756279                  \n",
      "    53     0.228885   0.779812   0.745116                  \n",
      "    54     0.230553   0.739617   0.756279                  \n",
      "    55     0.229087   0.741295   0.749767                  \n",
      "    56     0.223781   0.75815    0.752558                  \n",
      "    57     0.221014   0.769124   0.742326                  \n",
      "    58     0.22277    0.772889   0.749767                  \n",
      "    59     0.214841   0.762978   0.76                      \n",
      "    60     0.210249   0.740237   0.753488                  \n",
      "    61     0.208645   0.747578   0.75814                   \n",
      "    62     0.209221   0.802844   0.739535                  \n",
      "    63     0.209205   0.797611   0.748837                  \n",
      "    64     0.210901   0.752163   0.76093                   \n",
      "    65     0.208775   0.781883   0.744186                  \n",
      "    66     0.209665   0.785774   0.737674                  \n",
      "    67     0.211107   0.819476   0.741395                  \n",
      "    68     0.208906   0.804761   0.747907                  \n",
      "    69     0.209089   0.744805   0.752558                  \n",
      "    70     0.210389   0.773102   0.746047                  \n",
      "    71     0.206055   0.76087    0.76093                   \n",
      "    72     0.201954   0.793001   0.749767                  \n",
      "    73     0.194588   0.774356   0.753488                  \n",
      "    74     0.196397   0.769636   0.75814                   \n",
      "    75     0.199119   0.781826   0.745116                  \n",
      "    76     0.200196   0.770024   0.75814                   \n",
      "    77     0.198296   0.787321   0.744186                  \n",
      "    78     0.190202   0.792549   0.756279                  \n",
      "    79     0.186991   0.771658   0.76                      \n",
      "    80     0.187749   0.797165   0.756279                  \n",
      "    81     0.185655   0.813065   0.756279                  \n",
      "    82     0.187511   0.780784   0.751628                  \n",
      "    83     0.189622   0.817519   0.745116                  \n",
      "    84     0.18757    0.788448   0.75907                   \n",
      "    85     0.190525   0.83699    0.750698                  \n",
      "    86     0.188624   0.771611   0.76186                   \n",
      "    87     0.186677   0.814175   0.756279                  \n",
      "    88     0.180432   0.77411    0.763721                  \n",
      "    89     0.177368   0.829227   0.746047                  \n",
      "    90     0.178952   0.799435   0.754419                  \n",
      "    91     0.17434    0.821985   0.745116                  \n",
      "    92     0.17954    0.803982   0.752558                  \n",
      "    93     0.178699   0.80501    0.757209                  \n",
      "    94     0.180872   0.796991   0.747907                  \n",
      "    95     0.182336   0.803108   0.751628                  \n",
      "    96     0.184944   0.800445   0.751628                  \n",
      "    97     0.182931   0.80285    0.752558                  \n",
      "    98     0.181286   0.851756   0.737674                  \n",
      "    99     0.180551   0.806399   0.744186                  \n",
      "   100     0.179012   0.800558   0.750698                  \n",
      "   101     0.176747   0.821246   0.752558                  \n",
      "   102     0.173446   0.797997   0.753488                  \n",
      "   103     0.166759   0.866569   0.739535                  \n",
      "   104     0.168769   0.804796   0.750698                  \n",
      "   105     0.16816    0.825852   0.754419                  \n",
      "   106     0.167147   0.823006   0.750698                  \n",
      "   107     0.171147   0.848285   0.744186                  \n",
      "   108     0.172011   0.800339   0.757209                  \n",
      "   109     0.17166    0.819806   0.755349                  \n",
      "   110     0.169376   0.808798   0.76093                   \n",
      "   111     0.169965   0.813767   0.757209                  \n",
      "   112     0.166844   0.839429   0.749767                  \n",
      "   113     0.168199   0.826997   0.752558                  \n",
      "   114     0.166475   0.841545   0.753488                  \n",
      "   115     0.163738   0.798692   0.757209                  \n",
      "   116     0.165607   0.838019   0.746977                  \n",
      "   117     0.16345    0.818637   0.747907                  \n",
      "   118     0.163402   0.839652   0.753488                  \n",
      "   119     0.161642   0.817356   0.75907                   \n",
      "   120     0.165765   0.828705   0.754419                  \n",
      "   121     0.16855    0.863374   0.752558                  \n",
      "   122     0.168816   0.841277   0.744186                  \n",
      "   123     0.173099   0.832693   0.754419                  \n",
      "   124     0.169584   0.819482   0.762791                  \n",
      "   125     0.168235   0.863421   0.733953                  \n",
      "   126     0.168526   0.883036   0.748837                  \n",
      "   127     0.165242   0.853116   0.749767                  \n",
      "   128     0.160998   0.863777   0.747907                  \n",
      "   129     0.160828   0.844772   0.751628                  \n",
      "   130     0.159356   0.884594   0.748837                  \n",
      "   131     0.16183    0.876573   0.746047                  \n",
      "   132     0.163637   0.887924   0.749767                  \n",
      "   133     0.157461   0.851498   0.75814                   \n",
      "   134     0.155841   0.86558    0.749767                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.157207   0.865389   0.753488                  \n",
      "   136     0.155115   0.859303   0.749767                  \n",
      "   137     0.151704   0.837538   0.762791                  \n",
      "   138     0.149006   0.883495   0.756279                  \n",
      "   139     0.155721   0.897681   0.76093                   \n",
      "   140     0.166783   0.863919   0.745116                  \n",
      "   141     0.167231   0.865945   0.756279                  \n",
      "   142     0.162754   0.841073   0.76                      \n",
      "   143     0.156177   0.835807   0.762791                  \n",
      "   144     0.151611   0.864949   0.752558                  \n",
      "   145     0.150978   0.844476   0.757209                  \n",
      "   146     0.156541   0.823247   0.766512                  \n",
      "   147     0.153111   0.82997    0.763721                  \n",
      "   148     0.147358   0.887842   0.748837                  \n",
      "   149     0.152401   0.875172   0.752558                  \n",
      "   150     0.152323   0.855993   0.753488                  \n",
      "   151     0.147601   0.8575     0.749767                  \n",
      "   152     0.148517   0.869214   0.751628                  \n",
      "   153     0.147198   0.885649   0.753488                  \n",
      "   154     0.146822   0.852246   0.755349                  \n",
      "   155     0.146479   0.870257   0.76093                   \n",
      "   156     0.14662    0.852156   0.75907                   \n",
      "   157     0.147659   0.861743   0.757209                  \n",
      "   158     0.146323   0.85089    0.76186                   \n",
      "   159     0.145144   0.84797    0.755349                  \n",
      "   160     0.146398   0.875505   0.75907                   \n",
      "   161     0.148505   0.874534   0.757209                  \n",
      "   162     0.146378   0.861159   0.76093                   \n",
      "   163     0.144166   0.866971   0.749767                  \n",
      "   164     0.148443   0.89103    0.75814                   \n",
      "   165     0.146134   0.846843   0.76093                   \n",
      "   166     0.14299    0.841696   0.75907                   \n",
      "   167     0.140376   0.886093   0.75907                   \n",
      "   168     0.143374   0.875325   0.757209                  \n",
      "   169     0.142562   0.891382   0.744186                  \n",
      "   170     0.139917   0.867018   0.75814                   \n",
      "   171     0.140725   0.899875   0.751628                  \n",
      "   172     0.142742   0.856207   0.762791                  \n",
      "   173     0.144272   0.822355   0.768372                  \n",
      "   174     0.143909   0.848569   0.762791                  \n",
      "   175     0.143007   0.841276   0.748837                  \n",
      "   176     0.142423   0.879684   0.753488                  \n",
      "   177     0.142075   0.866502   0.76186                   \n",
      "   178     0.141126   0.835306   0.766512                  \n",
      "   179     0.140085   0.853611   0.765581                  \n",
      "   180     0.139417   0.873732   0.757209                  \n",
      "   181     0.137339   0.866114   0.762791                  \n",
      "   182     0.137028   0.924808   0.741395                  \n",
      "   183     0.135794   0.887977   0.764651                  \n",
      "   184     0.134301   0.907169   0.76186                   \n",
      "   185     0.135789   0.909419   0.764651                  \n",
      "   186     0.137491   0.902983   0.756279                  \n",
      "   187     0.135314   0.898976   0.755349                  \n",
      "   188     0.135887   0.863323   0.753488                  \n",
      "   189     0.135641   0.870495   0.75814                   \n",
      "   190     0.133349   0.855094   0.764651                  \n",
      "   191     0.134064   0.873735   0.75907                   \n",
      "   192     0.135664   0.876487   0.76186                   \n",
      "   193     0.134962   0.853885   0.76186                   \n",
      "   194     0.137115   0.871914   0.766512                  \n",
      "   195     0.138313   0.87404    0.76                      \n",
      "   196     0.142013   0.880287   0.76                      \n",
      "   197     0.139994   0.855586   0.764651                  \n",
      "   198     0.140803   0.861291   0.76186                   \n",
      "   199     0.138942   0.869455   0.75907                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd6656735464a2a8b636445ead28f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f107b1b0e97d45b8a90fd8bfedea2a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.052141   0.905419   0.55814   \n",
      "    1      0.939536   0.823448   0.624186                  \n",
      "    2      0.878846   0.791342   0.636279                  \n",
      "    3      0.837853   0.776496   0.650233                  \n",
      "    4      0.800931   0.742073   0.675349                  \n",
      "    5      0.767604   0.718878   0.692093                  \n",
      "    6      0.742871   0.72506    0.670698                  \n",
      "    7      0.721993   0.720596   0.67907                   \n",
      "    8      0.703178   0.701626   0.699535                  \n",
      "    9      0.686728   0.686438   0.700465                  \n",
      "    10     0.67123    0.685882   0.706977                  \n",
      "    11     0.662024   0.70484    0.688372                  \n",
      "    12     0.641138   0.692864   0.695814                  \n",
      "    13     0.628677   0.665256   0.703256                  \n",
      "    14     0.622837   0.668681   0.707907                  \n",
      "    15     0.608733   0.67056    0.708837                  \n",
      "    16     0.594472   0.670747   0.72                      \n",
      "    17     0.589052   0.655828   0.71907                   \n",
      "    18     0.586413   0.66163    0.725581                  \n",
      "    19     0.58213    0.668247   0.71814                   \n",
      "    20     0.571911   0.662322   0.71907                   \n",
      "    21     0.562129   0.644704   0.729302                  \n",
      "    22     0.553412   0.666805   0.716279                  \n",
      "    23     0.546841   0.641405   0.736744                  \n",
      "    24     0.537127   0.659308   0.726512                  \n",
      "    25     0.524217   0.647497   0.731163                  \n",
      "    26     0.519555   0.669941   0.714419                  \n",
      "    27     0.518437   0.643642   0.732093                  \n",
      "    28     0.514788   0.638553   0.740465                  \n",
      "    29     0.503408   0.640303   0.727442                  \n",
      "    30     0.491549   0.664215   0.723721                  \n",
      "    31     0.490998   0.637273   0.737674                  \n",
      "    32     0.483854   0.636672   0.735814                  \n",
      "    33     0.477751   0.623712   0.737674                  \n",
      "    34     0.479354   0.629233   0.730233                  \n",
      "    35     0.476836   0.624264   0.740465                  \n",
      "    36     0.46886    0.611915   0.732093                  \n",
      "    37     0.466668   0.620811   0.746047                  \n",
      "    38     0.462728   0.631805   0.744186                  \n",
      "    39     0.457835   0.613558   0.748837                  \n",
      "    40     0.457721   0.630733   0.739535                  \n",
      "    41     0.451931   0.649254   0.740465                  \n",
      "    42     0.44931    0.622676   0.746047                  \n",
      "    43     0.446248   0.633053   0.736744                  \n",
      "    44     0.442758   0.629354   0.76                      \n",
      "    45     0.434625   0.631002   0.751628                  \n",
      "    46     0.433163   0.636909   0.738605                  \n",
      "    47     0.431097   0.629117   0.743256                  \n",
      "    48     0.42297    0.643177   0.733953                  \n",
      "    49     0.417263   0.632984   0.745116                  \n",
      "    50     0.422694   0.659854   0.745116                  \n",
      "    51     0.425405   0.636714   0.741395                  \n",
      "    52     0.41669    0.630551   0.754419                  \n",
      "    53     0.412921   0.611143   0.747907                  \n",
      "    54     0.4099     0.618075   0.747907                  \n",
      "    55     0.403256   0.638688   0.740465                  \n",
      "    56     0.404446   0.63954    0.746977                  \n",
      "    57     0.408884   0.619284   0.757209                  \n",
      "    58     0.403643   0.633826   0.748837                  \n",
      "    59     0.401803   0.621354   0.748837                  \n",
      "    60     0.399008   0.615654   0.753488                  \n",
      "    61     0.390389   0.636979   0.749767                  \n",
      "    62     0.383453   0.623889   0.746047                  \n",
      "    63     0.379848   0.640507   0.742326                  \n",
      "    64     0.379438   0.663156   0.746977                  \n",
      "    65     0.376605   0.665873   0.745116                  \n",
      "    66     0.369154   0.660523   0.748837                  \n",
      "    67     0.374351   0.626074   0.753488                  \n",
      "    68     0.3678     0.636902   0.76093                   \n",
      "    69     0.364581   0.645904   0.753488                  \n",
      "    70     0.365337   0.64419    0.750698                  \n",
      "    71     0.36679    0.641633   0.75814                   \n",
      "    72     0.366256   0.654619   0.749767                  \n",
      "    73     0.363008   0.638272   0.757209                  \n",
      "    74     0.354205   0.644805   0.756279                  \n",
      "    75     0.349638   0.626917   0.763721                  \n",
      "    76     0.36084    0.655124   0.749767                  \n",
      "    77     0.346556   0.65976    0.753488                  \n",
      "    78     0.348557   0.676918   0.752558                  \n",
      "    79     0.346165   0.65232    0.755349                  \n",
      "    80     0.346841   0.66387    0.745116                  \n",
      "    81     0.341838   0.661769   0.750698                  \n",
      "    82     0.341139   0.661558   0.750698                  \n",
      "    83     0.339158   0.650693   0.75907                   \n",
      "    84     0.331104   0.630425   0.76186                   \n",
      "    85     0.330463   0.65816    0.752558                  \n",
      "    86     0.326767   0.665884   0.754419                  \n",
      "    87     0.325055   0.68609    0.749767                  \n",
      "    88     0.324712   0.676747   0.753488                  \n",
      "    89     0.327404   0.661257   0.746047                  \n",
      "    90     0.328692   0.649026   0.75907                   \n",
      "    91     0.325124   0.664419   0.752558                  \n",
      "    92     0.325788   0.637053   0.76093                   \n",
      "    93     0.326194   0.667333   0.757209                  \n",
      "    94     0.324037   0.650459   0.770233                  \n",
      "    95     0.324795   0.658485   0.751628                  \n",
      "    96     0.323934   0.672413   0.750698                  \n",
      "    97     0.316091   0.644866   0.76                      \n",
      "    98     0.315352   0.670811   0.75907                   \n",
      "    99     0.310601   0.642031   0.76093                   \n",
      "   100     0.310055   0.644713   0.766512                  \n",
      "   101     0.306379   0.658134   0.75907                   \n",
      "   102     0.306748   0.634592   0.777674                  \n",
      "   103     0.304077   0.655068   0.765581                  \n",
      "   104     0.301205   0.64336    0.764651                  \n",
      "   105     0.294774   0.646092   0.763721                  \n",
      "   106     0.290975   0.642751   0.767442                  \n",
      "   107     0.287537   0.634707   0.756279                  \n",
      "   108     0.291258   0.662479   0.75907                   \n",
      "   109     0.295448   0.668534   0.76093                   \n",
      "   110     0.294436   0.644053   0.75907                   \n",
      "   111     0.288847   0.662225   0.76                      \n",
      "   112     0.293312   0.669067   0.753488                  \n",
      "   113     0.294489   0.665378   0.75907                   \n",
      "   114     0.293804   0.683131   0.75814                   \n",
      "   115     0.289757   0.6734     0.752558                  \n",
      "   116     0.285405   0.683737   0.75814                   \n",
      "   117     0.28661    0.667415   0.75814                   \n",
      "   118     0.281772   0.673677   0.755349                  \n",
      "   119     0.28134    0.669603   0.764651                  \n",
      "   120     0.277786   0.675467   0.75814                   \n",
      "   121     0.277961   0.666434   0.75907                   \n",
      "   122     0.278299   0.68701    0.762791                  \n",
      "   123     0.275441   0.665162   0.762791                  \n",
      "   124     0.270127   0.665614   0.75907                   \n",
      "   125     0.272666   0.668137   0.754419                  \n",
      "   126     0.268833   0.687137   0.757209                  \n",
      "   127     0.271074   0.680965   0.766512                  \n",
      "   128     0.274119   0.674467   0.762791                  \n",
      "   129     0.271839   0.665666   0.772093                  \n",
      "   130     0.266311   0.6652     0.763721                  \n",
      "   131     0.268462   0.69297    0.76186                   \n",
      "   132     0.268209   0.681239   0.770233                  \n",
      "   133     0.274032   0.655843   0.770233                  \n",
      "   134     0.276149   0.650563   0.768372                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.269137   0.670929   0.763721                  \n",
      "   136     0.274195   0.689028   0.754419                  \n",
      "   137     0.271813   0.657527   0.766512                  \n",
      "   138     0.276761   0.683908   0.774884                  \n",
      "   139     0.275644   0.679454   0.762791                  \n",
      "   140     0.271526   0.67461    0.76093                   \n",
      "   141     0.267588   0.674      0.771163                  \n",
      "   142     0.272384   0.666591   0.772093                  \n",
      "   143     0.266079   0.673956   0.771163                  \n",
      "   144     0.264123   0.680242   0.76186                   \n",
      "   145     0.259313   0.679101   0.763721                  \n",
      "   146     0.257295   0.673976   0.766512                  \n",
      "   147     0.255443   0.670356   0.766512                  \n",
      "   148     0.255974   0.682248   0.76                      \n",
      "   149     0.2524     0.677935   0.764651                  \n",
      "   150     0.256546   0.670374   0.756279                  \n",
      "   151     0.256963   0.678356   0.76093                   \n",
      "   152     0.254409   0.711281   0.75814                   \n",
      "   153     0.251554   0.675148   0.754419                  \n",
      "   154     0.247275   0.665526   0.770233                  \n",
      "   155     0.24516    0.670518   0.757209                  \n",
      "   156     0.24587    0.668261   0.772093                  \n",
      "   157     0.246503   0.703499   0.75907                   \n",
      "   158     0.246743   0.707966   0.76093                   \n",
      "   159     0.253051   0.680444   0.768372                  \n",
      "   160     0.245931   0.70627    0.767442                  \n",
      "   161     0.245656   0.690492   0.763721                  \n",
      "   162     0.243007   0.689971   0.768372                  \n",
      "   163     0.245453   0.687021   0.772093                  \n",
      "   164     0.246974   0.689537   0.774884                  \n",
      "   165     0.244182   0.68395    0.76186                   \n",
      "   166     0.234606   0.683794   0.765581                  \n",
      "   167     0.231262   0.702201   0.763721                  \n",
      "   168     0.235695   0.697177   0.767442                  \n",
      "   169     0.243308   0.706867   0.764651                  \n",
      "   170     0.238664   0.704606   0.76186                   \n",
      "   171     0.23748    0.722459   0.75907                   \n",
      "   172     0.235285   0.713997   0.766512                  \n",
      "   173     0.232836   0.714456   0.76                      \n",
      "   174     0.23275    0.715611   0.766512                  \n",
      "   175     0.233739   0.710252   0.76093                   \n",
      "   176     0.233671   0.720135   0.762791                  \n",
      "   177     0.233294   0.719874   0.763721                  \n",
      "   178     0.236614   0.699159   0.762791                  \n",
      "   179     0.231907   0.697384   0.766512                  \n",
      "   180     0.235748   0.709894   0.772093                  \n",
      "   181     0.234549   0.696134   0.76186                   \n",
      "   182     0.232873   0.704153   0.764651                  \n",
      "   183     0.231599   0.713056   0.76                      \n",
      "   184     0.228069   0.721129   0.76                      \n",
      "   185     0.22739    0.727171   0.76186                   \n",
      "   186     0.223899   0.717361   0.762791                  \n",
      "   187     0.226917   0.711878   0.755349                  \n",
      "   188     0.222457   0.70897    0.764651                  \n",
      "   189     0.217296   0.716065   0.763721                  \n",
      "   190     0.219969   0.718613   0.769302                  \n",
      "   191     0.221502   0.695788   0.773023                  \n",
      "   192     0.219196   0.722166   0.770233                  \n",
      "   193     0.221724   0.711257   0.767442                  \n",
      "   194     0.222013   0.706156   0.763721                  \n",
      "   195     0.22174    0.717666   0.772093                  \n",
      "   196     0.221865   0.732778   0.771163                  \n",
      "   197     0.221479   0.726873   0.774884                  \n",
      "   198     0.218866   0.729865   0.768372                  \n",
      "   199     0.218153   0.705818   0.764651                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d263819e8d4354a9a6d2e68536e3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a8e8aa43be45a893ec019bf8b50351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.008562   0.870129   0.587907  \n",
      "    1      0.929562   0.821487   0.618605                  \n",
      "    2      0.872745   0.787564   0.646512                  \n",
      "    3      0.830689   0.759103   0.649302                  \n",
      "    4      0.798146   0.742391   0.655814                  \n",
      "    5      0.77152    0.72743    0.663256                  \n",
      "    6      0.746814   0.710966   0.672558                  \n",
      "    7      0.72542    0.700821   0.68093                   \n",
      "    8      0.708719   0.703887   0.68                      \n",
      "    9      0.690447   0.708672   0.68186                   \n",
      "    10     0.674758   0.694586   0.691163                  \n",
      "    11     0.663026   0.689157   0.702326                  \n",
      "    12     0.653878   0.678029   0.693953                  \n",
      "    13     0.634934   0.671119   0.705116                  \n",
      "    14     0.623674   0.680898   0.717209                  \n",
      "    15     0.618412   0.662376   0.726512                  \n",
      "    16     0.606062   0.662377   0.708837                  \n",
      "    17     0.594909   0.673017   0.707907                  \n",
      "    18     0.583053   0.643137   0.712558                  \n",
      "    19     0.575212   0.656161   0.729302                  \n",
      "    20     0.560034   0.636472   0.724651                  \n",
      "    21     0.554345   0.648637   0.737674                  \n",
      "    22     0.554193   0.640135   0.732093                  \n",
      "    23     0.545288   0.669389   0.724651                  \n",
      "    24     0.537454   0.647295   0.733953                  \n",
      "    25     0.535684   0.648923   0.737674                  \n",
      "    26     0.532262   0.643811   0.733023                  \n",
      "    27     0.528636   0.637469   0.728372                  \n",
      "    28     0.521306   0.642254   0.728372                  \n",
      "    29     0.516285   0.639449   0.729302                  \n",
      "    30     0.508125   0.625066   0.732093                  \n",
      "    31     0.500286   0.650001   0.727442                  \n",
      "    32     0.496212   0.626081   0.741395                  \n",
      "    33     0.488685   0.648127   0.736744                  \n",
      "    34     0.489702   0.641392   0.731163                  \n",
      "    35     0.483416   0.645297   0.733953                  \n",
      "    36     0.473752   0.625689   0.740465                  \n",
      "    37     0.468221   0.64553    0.741395                  \n",
      "    38     0.46335    0.626264   0.737674                  \n",
      "    39     0.456494   0.636087   0.732093                  \n",
      "    40     0.456832   0.623142   0.747907                  \n",
      "    41     0.44805    0.627759   0.746977                  \n",
      "    42     0.447654   0.62865    0.751628                  \n",
      "    43     0.447981   0.632405   0.736744                  \n",
      "    44     0.44034    0.618691   0.735814                  \n",
      "    45     0.436435   0.613112   0.752558                  \n",
      "    46     0.431831   0.625225   0.742326                  \n",
      "    47     0.42989    0.626379   0.754419                  \n",
      "    48     0.426416   0.63268    0.741395                  \n",
      "    49     0.424296   0.64089    0.753488                  \n",
      "    50     0.420328   0.633582   0.746047                  \n",
      "    51     0.416089   0.623939   0.750698                  \n",
      "    52     0.412482   0.633294   0.746977                  \n",
      "    53     0.403954   0.639108   0.752558                  \n",
      "    54     0.407531   0.627516   0.76                      \n",
      "    55     0.407832   0.617235   0.745116                  \n",
      "    56     0.4053     0.628771   0.746047                  \n",
      "    57     0.401535   0.617669   0.750698                  \n",
      "    58     0.399042   0.61105    0.75814                   \n",
      "    59     0.398353   0.631585   0.749767                  \n",
      "    60     0.393267   0.586238   0.763721                  \n",
      "    61     0.387255   0.611157   0.756279                  \n",
      "    62     0.383731   0.624609   0.753488                  \n",
      "    63     0.37727    0.628553   0.748837                  \n",
      "    64     0.378638   0.649999   0.743256                  \n",
      "    65     0.375493   0.627266   0.746047                  \n",
      "    66     0.368882   0.637258   0.751628                  \n",
      "    67     0.366053   0.642224   0.754419                  \n",
      "    68     0.364764   0.646572   0.741395                  \n",
      "    69     0.363915   0.640537   0.735814                  \n",
      "    70     0.364807   0.667662   0.745116                  \n",
      "    71     0.360233   0.636675   0.749767                  \n",
      "    72     0.357174   0.639282   0.746047                  \n",
      "    73     0.353088   0.635713   0.755349                  \n",
      "    74     0.355927   0.639618   0.748837                  \n",
      "    75     0.352376   0.642203   0.764651                  \n",
      "    76     0.36148    0.661754   0.749767                  \n",
      "    77     0.362427   0.628873   0.756279                  \n",
      "    78     0.352039   0.630174   0.76                      \n",
      "    79     0.346165   0.662177   0.752558                  \n",
      "    80     0.340041   0.649193   0.745116                  \n",
      "    81     0.344982   0.662274   0.742326                  \n",
      "    82     0.344309   0.63715    0.757209                  \n",
      "    83     0.344657   0.643355   0.749767                  \n",
      "    84     0.345319   0.632651   0.751628                  \n",
      "    85     0.339858   0.675071   0.746977                  \n",
      "    86     0.338477   0.671496   0.752558                  \n",
      "    87     0.342468   0.657304   0.741395                  \n",
      "    88     0.34331    0.651442   0.755349                  \n",
      "    89     0.34112    0.653739   0.745116                  \n",
      "    90     0.343784   0.635559   0.757209                  \n",
      "    91     0.339978   0.649903   0.752558                  \n",
      "    92     0.331117   0.649763   0.756279                  \n",
      "    93     0.329031   0.649735   0.75814                   \n",
      "    94     0.326978   0.645119   0.743256                  \n",
      "    95     0.320594   0.674126   0.740465                  \n",
      "    96     0.325409   0.647581   0.755349                  \n",
      "    97     0.321793   0.637824   0.75814                   \n",
      "    98     0.320323   0.657702   0.744186                  \n",
      "    99     0.3106     0.654756   0.75907                   \n",
      "   100     0.309963   0.660636   0.750698                  \n",
      "   101     0.305589   0.66458    0.763721                  \n",
      "   102     0.303998   0.643664   0.757209                  \n",
      "   103     0.301311   0.642565   0.762791                  \n",
      "   104     0.305994   0.668253   0.764651                  \n",
      "   105     0.303628   0.656994   0.754419                  \n",
      "   106     0.304495   0.652759   0.764651                  \n",
      "   107     0.300902   0.679802   0.752558                  \n",
      "   108     0.298665   0.658644   0.753488                  \n",
      "   109     0.299197   0.64875    0.756279                  \n",
      "   110     0.302564   0.660209   0.752558                  \n",
      "   111     0.300434   0.663024   0.762791                  \n",
      "   112     0.295637   0.652389   0.762791                  \n",
      "   113     0.295762   0.646295   0.76                      \n",
      "   114     0.289182   0.645886   0.766512                  \n",
      "   115     0.289957   0.652012   0.766512                  \n",
      "   116     0.287963   0.661621   0.76                      \n",
      "   117     0.287062   0.670895   0.755349                  \n",
      "   118     0.289616   0.682177   0.756279                  \n",
      "   119     0.291328   0.669007   0.764651                  \n",
      "   120     0.289081   0.635562   0.766512                  \n",
      "   121     0.282659   0.64508    0.75907                   \n",
      "   122     0.281859   0.678777   0.750698                  \n",
      "   123     0.278872   0.654973   0.76093                   \n",
      "   124     0.277898   0.67041    0.762791                  \n",
      "   125     0.277031   0.66379    0.76093                   \n",
      "   126     0.285479   0.675066   0.756279                  \n",
      "   127     0.287169   0.66639    0.757209                  \n",
      "   128     0.283928   0.666193   0.753488                  \n",
      "   129     0.278984   0.661023   0.756279                  \n",
      "   130     0.275106   0.66529    0.76186                   \n",
      "   131     0.275767   0.670827   0.751628                  \n",
      "   132     0.271136   0.650013   0.762791                  \n",
      "   133     0.268064   0.663901   0.751628                  \n",
      "   134     0.266097   0.683533   0.764651                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.265088   0.675069   0.764651                  \n",
      "   136     0.264494   0.674884   0.766512                  \n",
      "   137     0.264393   0.679025   0.749767                  \n",
      "   138     0.265423   0.667418   0.764651                  \n",
      "   139     0.267523   0.679211   0.76                      \n",
      "   140     0.265586   0.676203   0.75814                   \n",
      "   141     0.266123   0.684121   0.76                      \n",
      "   142     0.267657   0.685203   0.749767                  \n",
      "   143     0.267093   0.680037   0.753488                  \n",
      "   144     0.264425   0.664907   0.75907                   \n",
      "   145     0.257858   0.673433   0.76093                   \n",
      "   146     0.260609   0.660223   0.75814                   \n",
      "   147     0.261109   0.654055   0.75814                   \n",
      "   148     0.255044   0.685342   0.765581                  \n",
      "   149     0.261614   0.679681   0.76                      \n",
      "   150     0.262448   0.652499   0.763721                  \n",
      "   151     0.256244   0.669377   0.770233                  \n",
      "   152     0.256497   0.674689   0.764651                  \n",
      "   153     0.253277   0.66843    0.765581                  \n",
      "   154     0.249568   0.677598   0.763721                  \n",
      "   155     0.248169   0.673065   0.766512                  \n",
      "   156     0.250948   0.679245   0.766512                  \n",
      "   157     0.251182   0.664029   0.766512                  \n",
      "   158     0.255218   0.699405   0.75814                   \n",
      "   159     0.252256   0.667975   0.774884                  \n",
      "   160     0.249131   0.673048   0.76093                   \n",
      "   161     0.242471   0.670897   0.768372                  \n",
      "   162     0.24303    0.685423   0.764651                  \n",
      "   163     0.24291    0.682009   0.757209                  \n",
      "   164     0.240376   0.668387   0.768372                  \n",
      "   165     0.244815   0.680278   0.762791                  \n",
      "   166     0.241897   0.699149   0.75814                   \n",
      "   167     0.235177   0.700932   0.776744                  \n",
      "   168     0.239856   0.690454   0.763721                  \n",
      "   169     0.238104   0.696625   0.762791                  \n",
      "   170     0.235364   0.717496   0.76186                   \n",
      "   171     0.240074   0.707707   0.764651                  \n",
      "   172     0.239821   0.706403   0.76186                   \n",
      "   173     0.243543   0.748945   0.75907                   \n",
      "   174     0.242564   0.723975   0.76093                   \n",
      "   175     0.241314   0.714723   0.756279                  \n",
      "   176     0.244077   0.708111   0.75907                   \n",
      "   177     0.240579   0.714743   0.757209                  \n",
      "   178     0.24033    0.705337   0.765581                  \n",
      "   179     0.240657   0.698521   0.763721                  \n",
      "   180     0.233889   0.700663   0.755349                  \n",
      "   181     0.234331   0.697089   0.764651                  \n",
      "   182     0.229153   0.699398   0.769302                  \n",
      "   183     0.230887   0.706731   0.762791                  \n",
      "   184     0.227013   0.709492   0.764651                  \n",
      "   185     0.228695   0.702751   0.76093                   \n",
      "   186     0.229316   0.692442   0.76186                   \n",
      "   187     0.226496   0.6987     0.769302                  \n",
      "   188     0.229526   0.685997   0.770233                  \n",
      "   189     0.23063    0.707395   0.75907                   \n",
      "   190     0.22732    0.696058   0.756279                  \n",
      "   191     0.226399   0.703406   0.764651                  \n",
      "   192     0.2201     0.712176   0.769302                  \n",
      "   193     0.224014   0.731698   0.771163                  \n",
      "   194     0.229712   0.696522   0.763721                  \n",
      "   195     0.228062   0.688918   0.765581                  \n",
      "   196     0.225927   0.693608   0.768372                  \n",
      "   197     0.221435   0.698934   0.765581                  \n",
      "   198     0.2227     0.699667   0.769302                  \n",
      "   199     0.217411   0.700596   0.76093                   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae96b00c05da4aa0bf589258295acd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c45282ffa646e1b6c942276025ede7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.0142     0.897321   0.571163  \n",
      "    1      0.925906   0.835021   0.60186                   \n",
      "    2      0.869764   0.795613   0.632558                  \n",
      "    3      0.825625   0.771456   0.646512                  \n",
      "    4      0.793785   0.74072    0.67814                   \n",
      "    5      0.770169   0.738188   0.665116                  \n",
      "    6      0.743196   0.729224   0.672558                  \n",
      "    7      0.722122   0.725566   0.673488                  \n",
      "    8      0.704392   0.714932   0.673488                  \n",
      "    9      0.684066   0.702159   0.691163                  \n",
      "    10     0.664486   0.690182   0.695814                  \n",
      "    11     0.651634   0.706751   0.691163                  \n",
      "    12     0.641415   0.683067   0.704186                  \n",
      "    13     0.626635   0.674984   0.715349                  \n",
      "    14     0.615085   0.674812   0.726512                  \n",
      "    15     0.60674    0.663223   0.72                      \n",
      "    16     0.59412    0.655094   0.727442                  \n",
      "    17     0.588165   0.680739   0.709767                  \n",
      "    18     0.581255   0.686206   0.713488                  \n",
      "    19     0.569032   0.663832   0.708837                  \n",
      "    20     0.558102   0.655916   0.729302                  \n",
      "    21     0.5502     0.646569   0.733023                  \n",
      "    22     0.555548   0.666459   0.724651                  \n",
      "    23     0.554819   0.656362   0.733953                  \n",
      "    24     0.539973   0.642681   0.738605                  \n",
      "    25     0.528395   0.670689   0.708837                  \n",
      "    26     0.526175   0.647173   0.746977                  \n",
      "    27     0.520062   0.64627    0.740465                  \n",
      "    28     0.513013   0.658902   0.72093                   \n",
      "    29     0.510301   0.629222   0.731163                  \n",
      "    30     0.499786   0.647072   0.734884                  \n",
      "    31     0.497225   0.64144    0.731163                  \n",
      "    32     0.489296   0.642752   0.734884                  \n",
      "    33     0.484222   0.637608   0.735814                  \n",
      "    34     0.485459   0.630778   0.734884                  \n",
      "    35     0.480644   0.63613    0.724651                  \n",
      "    36     0.471941   0.63938    0.745116                  \n",
      "    37     0.467514   0.644204   0.746047                  \n",
      "    38     0.455045   0.654372   0.742326                  \n",
      "    39     0.451165   0.657699   0.737674                  \n",
      "    40     0.450862   0.673271   0.735814                  \n",
      "    41     0.451202   0.651281   0.745116                  \n",
      "    42     0.445495   0.640927   0.744186                  \n",
      "    43     0.44008    0.634148   0.752558                  \n",
      "    44     0.431885   0.637273   0.747907                  \n",
      "    45     0.426542   0.647573   0.737674                  \n",
      "    46     0.426949   0.657367   0.739535                  \n",
      "    47     0.422158   0.657373   0.736744                  \n",
      "    48     0.413693   0.650423   0.735814                  \n",
      "    49     0.408785   0.657426   0.743256                  \n",
      "    50     0.409757   0.644206   0.753488                  \n",
      "    51     0.410226   0.646406   0.744186                  \n",
      "    52     0.409347   0.658269   0.750698                  \n",
      "    53     0.408838   0.631804   0.748837                  \n",
      "    54     0.405164   0.648708   0.748837                  \n",
      "    55     0.399905   0.650075   0.740465                  \n",
      "    56     0.395582   0.640047   0.756279                  \n",
      "    57     0.39667    0.643531   0.746977                  \n",
      "    58     0.393452   0.678472   0.734884                  \n",
      "    59     0.394001   0.668577   0.747907                  \n",
      "    60     0.386679   0.637136   0.755349                  \n",
      "    61     0.38864    0.685965   0.735814                  \n",
      "    62     0.381403   0.643281   0.750698                  \n",
      "    63     0.379252   0.64428    0.754419                  \n",
      "    64     0.371711   0.654931   0.75907                   \n",
      "    65     0.368097   0.673389   0.748837                  \n",
      "    66     0.369733   0.648404   0.76093                   \n",
      "    67     0.363944   0.666565   0.751628                  \n",
      "    68     0.363945   0.638902   0.765581                  \n",
      "    69     0.355601   0.654879   0.767442                  \n",
      "    70     0.354141   0.649572   0.76093                   \n",
      "    71     0.351535   0.676486   0.756279                  \n",
      "    72     0.350596   0.651431   0.766512                  \n",
      "    73     0.352338   0.641834   0.767442                  \n",
      "    74     0.353389   0.675048   0.749767                  \n",
      "    75     0.352797   0.658975   0.762791                  \n",
      "    76     0.35478    0.652882   0.753488                  \n",
      "    77     0.354513   0.652706   0.770233                  \n",
      "    78     0.353291   0.676991   0.75814                   \n",
      "    79     0.357179   0.683533   0.751628                  \n",
      "    80     0.35154    0.663752   0.762791                  \n",
      "    81     0.349727   0.669511   0.76                      \n",
      "    82     0.341717   0.652836   0.76                      \n",
      "    83     0.335959   0.674087   0.753488                  \n",
      "    84     0.33924    0.670919   0.752558                  \n",
      "    85     0.340899   0.662673   0.75814                   \n",
      "    86     0.338405   0.680773   0.746977                  \n",
      "    87     0.336404   0.683405   0.751628                  \n",
      "    88     0.331923   0.670846   0.756279                  \n",
      "    89     0.330198   0.683125   0.754419                  \n",
      "    90     0.332087   0.685484   0.742326                  \n",
      "    91     0.323401   0.677462   0.75907                   \n",
      "    92     0.323258   0.650044   0.75814                   \n",
      "    93     0.324166   0.664962   0.754419                  \n",
      "    94     0.323079   0.663899   0.757209                  \n",
      "    95     0.316191   0.690592   0.752558                  \n",
      "    96     0.315124   0.677993   0.76                      \n",
      "    97     0.319793   0.693061   0.743256                  \n",
      "    98     0.321936   0.676294   0.76186                   \n",
      "    99     0.325002   0.683682   0.75814                   \n",
      "   100     0.326339   0.666109   0.757209                  \n",
      "   101     0.324251   0.682698   0.757209                  \n",
      "   102     0.317738   0.682228   0.753488                  \n",
      "   103     0.316713   0.680524   0.763721                  \n",
      "   104     0.309753   0.685057   0.755349                  \n",
      "   105     0.305981   0.688487   0.753488                  \n",
      "   106     0.299853   0.689608   0.768372                  \n",
      "   107     0.300829   0.665257   0.763721                  \n",
      "   108     0.30228    0.671237   0.753488                  \n",
      "   109     0.299326   0.668957   0.769302                  \n",
      "   110     0.301644   0.66949    0.746047                  \n",
      "   111     0.299564   0.664851   0.752558                  \n",
      "   112     0.295928   0.677432   0.755349                  \n",
      "   113     0.299901   0.675281   0.76                      \n",
      "   114     0.300063   0.695153   0.76186                   \n",
      "   115     0.293022   0.671234   0.75907                   \n",
      "   116     0.293578   0.678737   0.768372                  \n",
      "   117     0.289814   0.699492   0.76093                   \n",
      "   118     0.290529   0.694225   0.76186                   \n",
      "   119     0.286815   0.682462   0.75907                   \n",
      "   120     0.286116   0.664151   0.76093                   \n",
      "   121     0.280956   0.681091   0.75907                   \n",
      "   122     0.278406   0.668909   0.754419                  \n",
      "   123     0.278089   0.688135   0.768372                  \n",
      "   124     0.280976   0.696373   0.75814                   \n",
      "   125     0.27675    0.698284   0.762791                  \n",
      "   126     0.280973   0.683751   0.765581                  \n",
      "   127     0.277416   0.692721   0.75907                   \n",
      "   128     0.272171   0.681643   0.76093                   \n",
      "   129     0.275019   0.709743   0.76                      \n",
      "   130     0.278609   0.697821   0.767442                  \n",
      "   131     0.279223   0.683708   0.755349                  \n",
      "   132     0.270744   0.692887   0.75814                   \n",
      "   133     0.270932   0.694949   0.765581                  \n",
      "   134     0.27456    0.720232   0.769302                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.278847   0.694187   0.765581                  \n",
      "   136     0.279992   0.707174   0.752558                  \n",
      "   137     0.278133   0.691895   0.763721                  \n",
      "   138     0.279019   0.707342   0.752558                  \n",
      "   139     0.272893   0.681978   0.772093                  \n",
      "   140     0.272813   0.689313   0.765581                  \n",
      "   141     0.272905   0.672844   0.774884                  \n",
      "   142     0.271759   0.698334   0.764651                  \n",
      "   143     0.268791   0.704422   0.762791                  \n",
      "   144     0.268562   0.697119   0.762791                  \n",
      "   145     0.263688   0.673475   0.773953                  \n",
      "   146     0.257017   0.676792   0.766512                  \n",
      "   147     0.2581     0.697712   0.763721                  \n",
      "   148     0.254457   0.694197   0.75814                   \n",
      "   149     0.255382   0.705731   0.76093                   \n",
      "   150     0.253301   0.703848   0.770233                  \n",
      "   151     0.255423   0.686628   0.766512                  \n",
      "   152     0.255836   0.696327   0.763721                  \n",
      "   153     0.256277   0.692419   0.76186                   \n",
      " 41%|      | 9/22 [00:16<00:22,  1.73s/it, loss=0.254]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2cddba575ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dropouts = [0, 0, 0.2, 0.2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mv_a1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfld_loop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3cls_preT_dotest_200ep.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# precomupte, test dropouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mv_a2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfld_loop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3cls_preF_dotest_200ep.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no precom, test dropouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mv_a3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfld_loop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3cls_preT_dotest_500ep.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# precomupte, test dropouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mv_a4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfld_loop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3cls_preF_dotest_500ep.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no precom, test dropouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f31f890f6218>\u001b[0m in \u001b[0;36mkfld_loop3\u001b[0;34m(k, epochs, name, precomp, bs, dropouts)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropout='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecomp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/models/resnext_101_64x4d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLambdaMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambdaBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLambdaReduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambdaBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/models/resnext_101_64x4d.py\u001b[0m in \u001b[0;36mforward_prepare\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 282\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropouts = [0, 0.2, 0.4,0.5,0.6,0.8]\n",
    "#dropouts = [0, 0, 0.2, 0.2]\n",
    "v_a1, learn1 = kfld_loop3(3, 200, '3cls_preT_dotest_200ep.model', True, 200, dropouts) # precomupte, test dropouts\n",
    "v_a2, learn2 = kfld_loop3(3, 200, '3cls_preF_dotest_200ep.model', False, 200, dropouts) # no precom, test dropouts\n",
    "v_a3, learn3 = kfld_loop3(3, 500, '3cls_preT_dotest_500ep.model', True, 200, dropouts) # precomupte, test dropouts\n",
    "v_a4, learn4 = kfld_loop3(3, 500, '3cls_preF_dotest_500ep.model', False, 200, dropouts) # no precom, test dropouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.precompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be0bf59f8c4465c9835aca794e501f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.675276   0.754112   0.755349  \n",
      "    1      0.664167   0.735226   0.752558                  \n",
      "    2      0.653358   0.709646   0.755349                  \n",
      "    3      0.644328   0.697632   0.750698                  \n",
      "    4      0.61649    0.680189   0.753488                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.68019]), 0.7534883670474207]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 5, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('224_3class_preF_b58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('224_3class_preF_b58')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'plot_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-19342e74db55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'plot_lr'"
     ]
    }
   ],
   "source": [
    "learn.fit.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CosAnneal' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f991d3c4ec26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CosAnneal' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cb147981ee4dc9a91a7ffac65394b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starting training on small images for a few epochs, then switching to bigger images, and continuing training is an amazingly effective way to avoid overfitting.\n",
    "\n",
    "# http://forums.fast.ai/t/planet-classification-challenge/7824/96\n",
    "# set_data doesnt change the model at all. It just gives it new data to train with.\n",
    "learn.set_data(get_data(224, 118)) \n",
    "learn.freeze()\n",
    "\n",
    "#Source:   \n",
    "#    def set_data(self, data, precompute=False):\n",
    "#        super().set_data(data)\n",
    "#        if precompute:\n",
    "#            self.unfreeze()\n",
    "#            self.save_fc1()\n",
    "#            self.freeze()\n",
    "#            self.precompute = True\n",
    "#        else:\n",
    "#            self.freeze()\n",
    "#File:      ~/fastai/courses/dl1/fastai/conv_learner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d-1',\n",
       "              OrderedDict([('input_shape', [-1, 3, 224, 224]),\n",
       "                           ('output_shape', [-1, 64, 112, 112]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9408)])),\n",
       "             ('BatchNorm2d-2',\n",
       "              OrderedDict([('input_shape', [-1, 64, 112, 112]),\n",
       "                           ('output_shape', [-1, 64, 112, 112]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 128)])),\n",
       "             ('ReLU-3',\n",
       "              OrderedDict([('input_shape', [-1, 64, 112, 112]),\n",
       "                           ('output_shape', [-1, 64, 112, 112]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('MaxPool2d-4',\n",
       "              OrderedDict([('input_shape', [-1, 64, 112, 112]),\n",
       "                           ('output_shape', [-1, 64, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-5',\n",
       "              OrderedDict([('input_shape', [-1, 64, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 16384)])),\n",
       "             ('BatchNorm2d-6',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-7',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-8',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-9',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-10',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-11',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-12',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('Conv2d-13',\n",
       "              OrderedDict([('input_shape', [-1, 64, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 16384)])),\n",
       "             ('BatchNorm2d-14',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-15',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-16',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-17',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-18',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-19',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-20',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-21',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-22',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-23',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-24',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-25',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-26',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-27',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-28',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-29',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-30',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-31',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-32',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-33',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 256, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-34',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 512, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 131072)])),\n",
       "             ('BatchNorm2d-35',\n",
       "              OrderedDict([('input_shape', [-1, 512, 56, 56]),\n",
       "                           ('output_shape', [-1, 512, 56, 56]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-36',\n",
       "              OrderedDict([('input_shape', [-1, 512, 56, 56]),\n",
       "                           ('output_shape', [-1, 512, 56, 56]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-37',\n",
       "              OrderedDict([('input_shape', [-1, 512, 56, 56]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-38',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-39',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-40',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-41',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('Conv2d-42',\n",
       "              OrderedDict([('input_shape', [-1, 256, 56, 56]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 131072)])),\n",
       "             ('BatchNorm2d-43',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-44',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-45',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-46',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-47',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-48',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-49',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-50',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-51',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-52',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-53',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-54',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-55',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-56',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-57',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-58',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-59',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-60',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-61',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-62',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-63',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-64',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-65',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-66',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-67',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-68',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-69',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-70',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-71',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 512, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-72',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 1024, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 524288)])),\n",
       "             ('BatchNorm2d-73',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 28, 28]),\n",
       "                           ('output_shape', [-1, 1024, 28, 28]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-74',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 28, 28]),\n",
       "                           ('output_shape', [-1, 1024, 28, 28]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-75',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 28, 28]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-76',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-77',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-78',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-79',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('Conv2d-80',\n",
       "              OrderedDict([('input_shape', [-1, 512, 28, 28]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 524288)])),\n",
       "             ('BatchNorm2d-81',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-82',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-83',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-84',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-85',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-86',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-87',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-88',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-89',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-90',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-91',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-92',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-93',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-94',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-95',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-96',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-97',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-98',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-99',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-100',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-101',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-102',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-103',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-104',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-105',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-106',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-107',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-108',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-109',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-110',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-111',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-112',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-113',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-114',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-115',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-116',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-117',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-118',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-119',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-120',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-121',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-122',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-123',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-124',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-125',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-126',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-127',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-128',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-129',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-130',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-131',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-132',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-133',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-134',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-135',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-136',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-137',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-138',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-139',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-140',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-141',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-142',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-143',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-144',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-145',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-146',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-147',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-148',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-149',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-150',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-151',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-152',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-153',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-154',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-155',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-156',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-157',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-158',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-159',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-160',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-161',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-162',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-163',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-164',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-165',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-166',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-167',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-168',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-169',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-170',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-171',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-172',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-173',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-174',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-175',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-176',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-177',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-178',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-179',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-180',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-181',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-182',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-183',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-184',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-185',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-186',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-187',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-188',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-189',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-190',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-191',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-192',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-193',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-194',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-195',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-196',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-197',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-198',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-199',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-200',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-201',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-202',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-203',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-204',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-205',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-206',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-207',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-208',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-209',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-210',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-211',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-212',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-213',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-214',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-215',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-216',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-217',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-218',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-219',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-220',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-221',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-222',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-223',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-224',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-225',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-226',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-227',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-228',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-229',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-230',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-231',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-232',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-233',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-234',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-235',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-236',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-237',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-238',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-239',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-240',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-241',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-242',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-243',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-244',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-245',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-246',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-247',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-248',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-249',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-250',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-251',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-252',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-253',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-254',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-255',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-256',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-257',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-258',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-259',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-260',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-261',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-262',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-263',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-264',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-265',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-266',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-267',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-268',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-269',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-270',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-271',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-272',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-273',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-274',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-275',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-276',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-277',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-278',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-279',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-280',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 1024, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-281',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 2048, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2097152)])),\n",
       "             ('BatchNorm2d-282',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 14, 14]),\n",
       "                           ('output_shape', [-1, 2048, 14, 14]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-283',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 14, 14]),\n",
       "                           ('output_shape', [-1, 2048, 14, 14]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-284',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 14, 14]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-285',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-286',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-287',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-288',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('Conv2d-289',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 14, 14]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2097152)])),\n",
       "             ('BatchNorm2d-290',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-291',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-292',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-293',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-294',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-295',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-296',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-297',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-298',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-299',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-300',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-301',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-302',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-303',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-304',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-305',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-306',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-307',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-308',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-309',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 7, 7]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveMaxPool2d-310',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveAvgPool2d-311',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 2048, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveConcatPool2d-312',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 7, 7]),\n",
       "                           ('output_shape', [-1, 4096, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Flatten-313',\n",
       "              OrderedDict([('input_shape', [-1, 4096, 1, 1]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-314',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8192)])),\n",
       "             ('Dropout-315',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-316',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2097664)])),\n",
       "             ('ReLU-317',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-318',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('Dropout-319',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-320',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 3]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1539)])),\n",
       "             ('LogSoftmax-321',\n",
       "              OrderedDict([('input_shape', [-1, 3]),\n",
       "                           ('output_shape', [-1, 3]),\n",
       "                           ('nb_params', 0)]))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe04486e4caa417e9cbd8c7d34354587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.554965   0.620283   0.749767  \n",
      "    1      0.556964   0.617304   0.752558                  \n",
      "    2      0.554449   0.615064   0.750698                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.61506]), 0.7506976790206377]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(5e-3, 3, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0b29e14e57478881fe7e01a516845a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.711794   62.244102  0.4       \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrf=learn.lr_find(start_lr=1e-9, end_lr=10)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss is much lower than training loss. This is a sign of underfitting. Cycle_len=1 may be too short. Let's set cycle_mult=2 to find better parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5fcd47fd4d45048afda0a8eb9d9104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.53871    0.612978   0.749767  \n",
      "    1      0.54187    0.614596   0.751628                  \n",
      "    2      0.535008   0.612283   0.750698                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.61228]), 0.7506976659353389]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you are under fitting, it means cycle_len=1 is too short (learning rate is getting reset before it had the chance to zoom in properly).\n",
    "learn.fit(1e-2, 3, cycle_len=1) # 1+2+4 = 7 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss and validation loss are getting closer and smaller. We are on right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5330396475770925, 1.3259945803139326)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds, y = learn.TTA() # (5, 2044, 120), (2044,)\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1075, array([1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.val_ds.y), data.val_ds.y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5d34cd091d4e818d90e3f49d51f004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.530726   0.613472   0.753488  \n",
      "    1      0.526358   0.609563   0.750698                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.60956]), 0.7506976724779884]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-3, 1, cycle_len=2) # 1+1 = 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[282  27  43]                               \n",
      " [ 44 184 120]\n",
      " [ 34  24 317]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAEmCAYAAAAjsVjMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XeYlOXVx/Hvb3dpSlNQmmClKUoViSCCXYy9odgL9ljji0oUjURjjL0k2FsUrBgsoFhRUEFQMQIigiBNQJAuLOf947kXB9zdmV1m9plZzue95tqZp54hr2fvPc9dZGY455zLnLy4A3DOucrOE61zzmWYJ1rnnMswT7TOOZdhnmidcy7DPNE651yGeaJ1WUVSDUn/lbRE0vObcJ0+kkakM7a4SNpH0uS443DlJ+9H68pD0snAFUArYCkwARhoZqM28bqnApcAe5vZ2k0ONMtJMqC5mU2NOxaXOd6idWUm6QrgLuBvQAOgGfAAcGQaLr89MGVzSLKpkFQQdwwuDczMX/5K+QXUAZYBx5dyTDWiRDw7vO4CqoV9PYBZwJXAfGAOcGbYdyPwK7Am3ONsYADwdMK1dwAMKAifzwCmEbWqvwf6JGwflXDe3sBnwJLwc++Efe8BfwU+CtcZAdQv4bsVxX91QvxHAb2AKcAi4NqE4zsDo4HF4dj7gKph3wfhuywP3/fEhOv/HzAXeKpoWzhn53CPDuFzY2AB0CPu/9/wV8kvb9G6svoDUB14uZRjrgO6AO2AtkTJpn/C/oZECbsJUTK9X9JWZnYDUSt5sJnVNLNHSgtE0pbAPcChZlaLKJlOKOa4rYHXwrH1gDuA1yTVSzjsZOBMYFugKnBVKbduSPRv0AS4HngIOAXoCOwDXC9pp3BsIXA5UJ/o325/4EIAM+sejmkbvu/ghOtvTdS675t4YzP7jigJPyNpC+Ax4HEze6+UeF3MPNG6sqoHLLDS/7TvA9xkZvPN7CeiluqpCfvXhP1rzOx1otZcy3LGsw5oI6mGmc0xs6+LOeYw4Fsze8rM1prZs8Ak4PCEYx4zsylmthIYQvRLoiRriOrRa4DniJLo3Wa2NNz/a2APADMbZ2Zjwn2nA/8G9k3hO91gZqtDPBsws4eAb4FPgEZEv9hcFvNE68pqIVA/Se2wMTAj4fOMsG39NTZK1CuAmmUNxMyWE/25fT4wR9JrklqlEE9RTE0SPs8tQzwLzawwvC9KhPMS9q8sOl9SC0nDJM2V9AtRi71+KdcG+MnMViU55iGgDXCvma1OcqyLmSdaV1ajgVVEdcmSzCb6s7dIs7CtPJYDWyR8bpi408yGm9mBRC27SUQJKFk8RTH9WM6YyuJBoriam1lt4FpASc4ptSuQpJpEde9HgAGhNOKymCdaVyZmtoSoLnm/pKMkbSGpiqRDJd0WDnsW6C9pG0n1w/FPl/OWE4DukppJqgNcU7RDUgNJR4Ra7WqiEkRhMdd4HWgh6WRJBZJOBHYFhpUzprKoBfwCLAut7Qs22j8P2Ol3Z5XubmCcmZ1DVHv+1yZH6TLKE60rMzO7g6gPbX/gJ2AmcDHwSjjkZmAs8CXwFfB52Faee70FDA7XGseGyTGPqPfCbKIn8fsSHjRtdI2FwB/DsQuJegz80cwWlCemMrqK6EHbUqLW9uCN9g8AnpC0WNIJyS4m6UjgEKJyCUT/O3SQ1CdtEbu08wELzjmXYd6idc65DPNE65xzGeaJ1jnnMswTrXPOZZhPWJFFVK2mqUa95AdWMu123ibuEGKzbjN8GD1zxgwWLlyQrC9xSvJrb2+29neD537HVv403MwOScc9y8MTbRZRjXpU67H5jaYcNeTcuEOIzfLVm98kZQfu2yVt17K1K6nWMmmvOFZNuD/ZaLyM8kTrnMtdEuTlxx1FUp5onXO5Tdn/qMkTrXMutykt5d6M8kTrnMthXjpwzrnMEl46cM65zMqNFm32/ypwzrnSSMlfpZ6u6pI+lfSFpK8l3Ri27yjpE0nfShosqWrYXi18nhr275AsRE+0zrkcpqh0kOxVutXAfmbWlmgJo0MkdQH+DtxpZs2Bn4nWtyP8/NnMdgHuDMeVyhOtcy53iah0kOxVCossCx+rhJcB+wEvhO1P8NuqIkeGz4T9+0ulN5s90TrncljKLdr6ksYmvDZYXVhSvqQJREvIvwV8ByxOWNtuFr+tMdeEaLJ7wv4lRIuWlsgfhjnnclteSv1oF5hZp5J2hsU220mqC7wMtC7usPCzuBuWOmmFJ1rnXO4qKh2kiZktlvQe0AWoK6kgtFq347cFRmcBTYFZYTXoOkRLKZXISwfOuRy26Q/DwiKidcP7GsABwDfAu8Bx4bDTgaHh/avhM2H/O5ZkTTBv0Trnctumt2gbES2QmU/U+BxiZsMk/Q94TtLNwHii5d0JP5+SNJWoJds72Q080TrnclcK/WSTMbMvgfbFbJ8GdC5m+yrg+LLcwxOtcy63+RBc55zLpNwYguuJ1jmX23yaROecyyCfvcs55zLNSwfOOZd53qJ1zrkM8sUZXbbYrv6WPHxpTxrUrcE6Mx4dMYn7h01kjx3rce/53ahWNZ+1hcZl/x7F2G9/onf3XbjimLYALF+1hj/9axRfTS91hGHWmzVzJueefTrz5s4lLy+PM88+l4suuZTT+vRmypTJACxZspg6deoy5rPxMUebfoWFhRy4bxcaNWrCM8+/wmUX9WXC+HGYGTvv0px7HnyEmjVrxh1m+fjDMJcN1hauo99jo5kwbSE1q1fh438ezcgJsxh4+l4MHPw5Iz6fycEdmzLw9L04uP8wps9bykHX/ZfFy3/loA5Nuf/C7nS/+pW4v8YmyS8o4G9/v5327TuwdOlSunXpxH4HHMiTzzy3/ph+V19JnTp1YowycwY9eC8tWrRi6dKlAPz1ltupVbs2AH+55s88OugB/nTF1XGGWG5JZijMCtlf3HCbbO7PK5kwbSEAy1atYdKsxTSutyVmRu0aVQCos0VV5ixaAcCYyfNYvPxXAD6dPI8m9baMJ/A0atSoEe3bdwCgVq1atGzVmtk//rh+v5nx0ovPc/wJJ8UVYsbM/nEWbw9/gz6nn7V+W1GSNTNWrVqZE63C4kigPCV9xc1btJuZZtvWpN1O9flsynz+/Mho/ntDL245swt5Ej37Df3d8Wcc0Irhn8+MIdLMmTF9Ol98MZ49O++1fttHoz5k220bsEvz5jFGlhn9+13J9TfdwrJlSzfY/qcLzuHtEW/SslVrbhx4W0zRbSp5i7YsJB0ladeEzzdJOqAC778s+VHFnneZpC0SPr9eNBNQttmyegHP/t+B/PmRj1m6cg19D9mVqx8dTfNz/sPVj47mwYu7b3B89zaNOP2AlvR/8pOYIk6/ZcuWcXLv47jt9jupHVp1AM8PfpbjT0g6N0jOGfHGa9Svvy1tQ2s+0T0PPsxXU2bQvEUrhr70fAzRpYekpK+4ZU2iJVomYn2iNbPrzeztGONJ1WXA+kRrZr3MbHGM8RSrIF88+38HMvj9qQwdMx2APj1b8Mro7wF48aNpdGq+7frj22y/NQ9evC/H3zKCRUtXxxFy2q1Zs4aTTzyOE3ufzJFHHbN++9q1axk69GWOO/7EGKPLjE8/+ZjhbwyjY5vm9D3zFEZ98C4XnHP6+v35+fkcdezxDBv6coxRbpq8vLykr7hlNAJJr0gaF1aW7Bu2LZM0MKw4OUZSA0l7A0cA/5A0QdLOkh6XdFw4Z7qkGyV9LukrSa3C9i0lPSrpM0njJR0ZtleX9Fg4dryknmH7GZKGSnpT0mRJNxQTc01JIxPudWTCvV4LcU+UdKKkPwGNgXclvZsQa/3w/jRJX4Zznsrkv3Uy/7p4XybPWsw9r361ftucRcvZp00jAHrs0Zipc5YA0LT+ljzX70DOvvNdps5eEku86WZmXHDeObRs1Yo/XXbFBvveGfk2LVu2osl228UUXeb0HzCQLyZ9z7iJ3zLosafp1r0nDzz0ONO+mwpE/y7D33iNXVq0jDnSclKKr5hlukZ7lpktCpPpfibpRWBLYIyZXSfpNuBcM7tZ0qvAMDN7AYp9krjAzDpIuhC4CjgHuI5o0t2zwp/rn0p6GzgfwMx2D0l5hKQW4TqdgTbAihDTa2Y2NuE+q4CjzeyXkDDHhNgOAWab2WEhvjpmtkTSFUBPM1uQGKyk3UJ8Xc1sgaSti/sHCr+AovWLahR7yCbbu3UD+vRswVfTFzLmzqgld8PTn3HRAx/wj3P2piAvj9VrCrn4gQ8BuObEjmxdqzp3nd8VgLWFRrercrfFAzD644949pmn2K3N7nTZM5oRb8BNAznk0F688PzgSlk2KImZccn5Z7Ns6S+YGbu22YN/3Hlf3GGVi1BWtFiTyXSi/ZOko8P7pkBz4FdgWNg2DjgwxWu9lHBO0d99BwFHSLoqfK4ONAO6AfcCmNkkSTOAokT7lpktBJD0Ujg2MdEK+Juk7sA6ooXYGgBfAbdL+jvRL4QPk8S7H/BCUQI2s2I7oprZIGAQQF7d7Uudpb28Pv5mHjWOGlTsvq5X/j6BXnj/B1x4/weZCCU2e3ftxvLV64rdN+jhxyo4mnh03Wdfuu6zLwCvvfV+zNGkTzbUYJPJWKKV1INoSYg/mNmKsA5PdWBNwrIPhWWIoahQmHiOgGPNbPJG9y7tX37jZLbx5z7ANkBHM1sjaTpQ3cymSOoI9AJukTTCzG4q5T4q5trOuTTLhUSbyTZ3HeDnkGRbES12VpqlQK0y3mM4cElRYpVUNEv6B0QJk1AyaAYUJeMDJW0dyhlHAR8VE/f8kGR7AtuH6zQGVpjZ08DtQNFj3JLiHgmcIKleOD8zdQHnNmc50o82k4n2TaBA0pfAX4ExSY5/DvhzeHi1c4r3+CtQBfhS0sTwGeABIF/SV8Bg4AwzK2oRjwKeAiYAL25UnwV4BugkaSxRsp4Utu9OVAOeQFR7vTlsHwS8UfQwrIiZfQ0MBN6X9AVwR4rfyTmXIpG8a1c2tHgzVjoIie3QYnbVTDjmBeCF8P4jErp3AWckHLdDwvuxQI/wfiVwXjH3XpV4/kbmm9nFxZxTM/xcAPyhmPOmE7WgNz7vXkI9uJhYnwCeKCEO51waZEMiTcZHhjnnclcoHWS7zSrRmtnjwOMxh+GcSyNv0TrnXIblQqLN/p6+zjlXApG8x0Gy0oKkppLelfRNGMV6adg+QNKPYbTqBEm9Es65RtLUMML04GRxeovWOZe7lJYW7VrgSjP7XFItYJykt8K+O83s9g1uGU1+1RvYjWgI/tuSWphZYUk38ETrnMtpmzoE18zmAHPC+6WSviEaEVqSI4HnQs+q7yVNJRraP7rEGDcpQueci1tqk8rUlzQ24dW32EtJOwDtgaK5QS8OE0M9KmmrsK0JkDhJ8yxKT8yeaJ1zuS3FAQsLzKxTwut3k39Iqgm8CFxmZr8ADwI7A+2IWrz/LDq0mDBKHW7vpQPnXM6S0jN7l6QqREn2GTN7CcDM5iXsf4jfJsOaRTRJVpHtgNmlXd9btM65nLapQ3DDXCmPAN+Y2R0J2xslHHY0MDG8fxXoLamapB2JZiX8tLR7eIvWOZfbNr0bbVfgVOCrMJcJwLXASZLaEZUFphOG+5vZ15KGAP8j6rFwUWk9DsATrXMulyktvQ5GUXy6fr2UcwYSTRqVEk+0zrmcJXJjpXRPtM65HCbyfFIZ55zLrFyY68ATrXMud8lLB845l1EC8vOzP9N6onXO5TQvHTjnXCZ56cA55zJLpGcIbqZ5onXO5TRv0TrnXCYJ70frnHOZFI0M80TrnHMZlQN51hOtcy63eenAlckeO9VnxFNnxh1GhTtvyJdxhxCbo3ffNu4QKtyy1WvTd7H0LM6YcZ5onXM5y2fvcs65jPPZu5xzLuO8dOCccxkk70frnHOZ5y1a55zLsBzIs55onXM5LNdLB5Jql3aimf2S/nCccy51QjlfOviaaD3zxG9R9NmAZhmMyznnUpIDebbkRGtmTSsyEOecK4/8TSwdSGoKPAk0BNYBg8zsbklbA4OBHYDpwAlm9rOiJvTdQC9gBXCGmX1e2j1SmjFXUm9J14b320nqWL6v5Jxz6aMwBDfZK4m1wJVm1hroAlwkaVegHzDSzJoDI8NngEOB5uHVF3gw2Q2SJlpJ9wE9gVPDphXAv5Kd55xzFSE/T0lfpTGzOUUtUjNbCnwDNAGOBJ4Ihz0BHBXeHwk8aZExQF1JjUq7Ryq9DvY2sw6SxodAFkmqmsJ5zjmXcSnWaOtLGpvweZCZDfr9tbQD0B74BGhgZnMgSsaSimYAagLMTDhtVtg2p6Sbp5Jo10jKI3oAhqR6RHUM55yLlYh6HqRggZl1KvVaUk3gReAyM/ullJJDcTustGunUqO9P9x8G0k3AqOAv6dwnnPOZZaSlw1SeVgmqQpRnnvGzF4Km+cVlQTCz/lh+ywgsbPAdsDs0q6fNNGa2ZNAf+B2YBFwvJk9lzRy55yrAFLyV+nnS8AjwDdmdkfCrleB08P704GhCdtPU6QLsKSoxFCSVEeG5QNriJrH2b+2r3NusyAgb9M70nYletj/laQJYdu1wK3AEElnAz8Ax4d9rxN17ZpK1Dkg6Wz9SROtpOuAk4GXib7XfyQ9Y2a3lO27OOdc+m3qEFwzG0XxdVeA/Ys53oCLynKPVFq0pwAdzWwFgKSBwDjAE61zLlaplAayQSqJdsZGxxUA0zITjnPOlU1+DmTa0iaVuZOoJrsC+FrS8PD5IKKeB845F7tcn1RmYvj5NfBawvYxmQvHOedSFz0MizuK5EqbVOaRigzEOefKTJVkcUZJOwMDgV2B6kXbzaxFBuNyGVZYWMhB+3ahYaMmPPP8K+u3X3PVZTz3zBN8P+fnGKNLn3O6NKX9drX4ZdVarhk2BYBmW1XnzM7bUSU/j0Iznvh0FtMWrlx/zo71ajDg4ObcN2oGn/2wJK7QN8n9N1zO2A/eps7W9bnrxXcBeOKOmxj7wVsUVKlKw+225+Ib72TL2nUAeOmRexn5yrPk5eVx1v/dTPu9e8QYfdnkQukglT6xjwOPEbXSDwWGAD5gIcc99OC9NG/RaoNtEz4fxy9LFscUUWZ8OG0Rt73z/QbberdvzMtfzaP/61N46Yu59O7QeP0+CXq3b8RXc5ZWdKhp1eOIE/nLA89ssK1tl+7c9cK73Pn8SBpvvxMvPXovADO/m8Ko4UO568V36f/Af3job9dQWFgYR9hlVlQ6SPaKWyqJdgszGw5gZt+ZWX+i2bxcjpr94yzeGv4GfU4/a/22wsJCbvxLP67/a+XqtTd5/nKWr167wTYDalTJB6BG1Xx+XrFm/b6DWtbnsx+W8MuqDc/JNbt17ELN2lttsK3d3j3IL4j+iG2xR0cWzosGM3323nC6HXwkVapWo0GTZjRsugNTJ46v8JjLK09K+opbKol2dRii9p2k8yUdDmyb7CSXvf7S70quv+kW8vJ++5//kX8/wMGH/pEGDUud7a1SeGbsj/Tu0Ii7jm7NSR0aM2RClHC2qlFAp6Z1GPntwpgjzLyRrzxL+277AbBw/hzqNfytVV+vQSMWzZ8bV2hlIlWeRHs5UBP4E9FQtXOBs0o9YzMi6T1JncL71yXVjTum0ox44zXq19+Wtu07rN82d85s/vvKi5xzfpkGu+Ss/VvU45mxs7ns5W94ZuyPnNMlmh/klE5NeG78HKzUeZhy3wsP3U1+fgHdex0TbSjuC2dBckpVXp6SvuKW9GGYmX0S3i7lt8m/N0uSCsysxL8pzaxXRcZTHp9+8jHD3xjGyLfeZNWqVSxb+gvd92pH1arV6NKuNQArV6xgr7at+eSLb2KONjO67bQ1T42NJlv69Icl6xPtjvVqcFG37QGoVS2ftk1qsW6dMW5W5VmH9N1XhzDuw7cZ8O/B6x8i1WvQmIVzf5t8auG8OWy9TYO4QiyzXPidUNqAhZcpZY5FMzsmIxFVEEmnAVcRfccviR7y9QeqAguBPmY2T9IAoDHRukELwgQTjxH1wvgGqJFwzelAJzNbIOkKfmv5P2xmd1XA10qq/4CB9B8wEICPPnyfB+65c4NeBwA7Ntqq0iZZgJ9XrqFVgy2ZNG85uzasydylqwG44pVJ64/p+4emjP/xl0qVZMd/9C6vPH4/Nz38EtVqbLF+e6d9D+Kuay/i8FP7suinecz54Xt2adM+xkhTJ7KjNJBMaS3a+yosigomaTfgOqBrSIpbEyXcLmZmks4BrgauDKd0BLqZ2cqQQFeY2R6S9gB+tyhbWFPtTGAvogejn0h638xy5wlDJXFht2a0blCTmtUKuPvo1rz05TweHTOLUzo1Jj9PrClcx6OfzIo7zLS7o98FfD12NEsXL+Lcgzpy4gVX8vKj97Hm19XcdP6JQPRA7Lz+f6fZLi3Z+8DDufSYHuTn53PuNX8jPz8/5m+QIm36pDIVobQBCyMrMpAKth/wgpktgPXL8+wODA4T/FYFEvsEvWpmRR0tuwP3hPO+lPRlMdfvBrxsZssBJL0E7AP8LtFK6ku0wBvbNa3YFdy77rMvXffZ93fbK0sfWoAHRv1Q7Pbr3/i21PMGjZ5Z6v5sd8Wtv18v8ICjTy7x+OPOvZTjzr00kyFlTC7M25oLMWaC+H1Z5F7gPjPbHTiPhMEZwPKNjk32uCTlX7FmNsjMOplZp3r166d6mnOOsJTNpq+Cm3Gba6IdCZwQ1j8jlA7qAD+G/aeXdCLwAdAnnNcG2KOEY46StIWkLYGjgQ/TFLtzLkFBXvJX3FJdYQFJ1cxsdSaDqShm9nWYV/d9SYVEf9IPAJ6X9CPRxDk7lnD6g8BjoWQwAfi0mOt/LunxhH0Pe33WufSL5qONv8WaTCpzHXQmWk+nDtBMUlvgHDO7JNPBZZKZPcFva7YXGVrMcQM2+rwS6F3CNXdIeH8HcEdxxznn0icHnoWlVDq4B/gjUZcnzOwLfAiucy4LCNKyCm6mpVI6yDOzGRs1z3NjxgnnXKWXBSXYpFJJtDND+cAk5QOXAFMyG5ZzziUnZUeLNZlUEu0FROWDZsA84O2wzTnnYpcDz8KSt7rNbL6Z9Taz+uHVu6ijv3POxS0d89FKelTSfEkTE7YNkPSjpAnh1Sth3zWSpkqaLOngZNdPpdfBQxTTQd/M+iYP3znnMqfoYVgaPE407cCTG22/08xu3+Ce0q5EPY92I5oH5W1JLcysxGdXqZQO3k54X52o831uj090zlUOaVpBwcw+kLRDiocfCTwXxhV8L2kq0BkYXdIJqUyTODjxs6SngLdSDMg55zJKqY94L4+Lw0x/Y4ErzexnoAkbrgY+K2wrUXl6RuwIbF+O85xzLq1EykNw60sam/BKpfT5ILAz0A6YA/wz4bYbK3X+k1RqtD8nXCQPWAT0SyFI55zLuBSH4C4ws05lua6ZzUu4x0PAsPBxFtA04dDtgNmUotREG9YKa8tvk62sM6vsC30453KFBPkZGrEgqZGZzQkfjwaKeiS8CvxH0h1ED8OaU8ycJ4lKTbRhEuyXzazjJsbsnHMZkY4VFiQ9C/QgKjHMAm4AekhqR/QX/XSi6VOLJqUaAvwPWAtcVFqPA0it18GnkjqY2e9WEnDOuTiJtPU6OKmYzY+UcvxAYGCq1y9tzbCihQi7AedK+o5oAmxF97EOJZ3rnHMVQ+TnwNCw0lq0nwIdgKMqKBbnnCuTaIWFuKNIrrREKwAz+66CYnHOubJJ04CFTCst0W4TVnwtVpjY2jnnYpPGIbgZVVqizQdqUoaFBp1zrqKlo9dBppWWaOeY2U0VFolzzpWRgPzsz7PJa7TOOZe1KsHijPtXWBTOOVdO2Z9mS0m0ZraoIgNxzrmyikoH2Z9qUxkZ5pxzWSsH8qwnWudcLlPO12idcy6reenAOecqQPanWU+0WUWIagUZmlwziz3Uu23cIcRmqz0vjjuECrf6+znJD0qR5C1a55zLOK/ROudchmV/mvVE65zLYf4wzDnnKkAO5FlPtM65XCaUA8UDT7TOuZzlpQPnnMs0eenAOecyLtcn/nbOuayWruXGM23zG4bknKtUlML/Jb2G9Kik+ZImJmzbWtJbkr4NP7cK2yXpHklTJX0pqUOy63uidc7ltDwp6SsFjwOHbLStHzDSzJoDI8NngEOB5uHVF3gwaYwpfhfnnMs6RaWDZK9kzOwDYOPFDo4EngjvnwCOStj+pEXGAHUlNSrt+p5onXM5LJXCQbmLuA3MbA5A+Llt2N4EmJlw3KywrUT+MMw5l7tSbLEC9SWNTfg8yMwGlf+uv2OlneCJ1jmXs6LSQUqZdoGZdSrj5edJamRmc0JpYH7YPgtomnDcdsDs0i7kpQPnXE6Tkr/K6VXg9PD+dGBowvbTQu+DLsCSohJDSbxF65zLaemY60DSs0APohLDLOAG4FZgiKSzgR+A48PhrwO9gKnACuDMZNf3ROucy2npGBhmZieVsGv/Yo414KKyXN8TrXMup+XACFxPtM653CXSUzrINE+0zrnc5bN3Oedc5nmidc65jMqNFRa8H+1mZtWqVey3Txe67tWBLh334G9/HbDB/j9fcSlNtqkTT3AZNHPmTA4+oCftdm9Nh7a7cd89d2+w/847bqdGFbFgwYKYIkyfalUL+PCpq/hkcD/GvXAd/c/vBcD5J3Zn4tAbWDn+PurV3XL98Zeftj9jnuvHmOf6Mfb5a1k29h62qr1FXOGXSbrmOsg0b9FuZqpVq8arb7xNzZo1WbNmDYfs350DDz6EPTt3Yfy4sSxZsjjuEDOioKCAW2/7J+07dGDp0qXsvVdH9j/gQFrvuiszZ87knbffommzZnGHmRarf13LIX3vYfnKXykoyOOdR69gxEf/Y/SEabz+wURGPHzpBsff+eRI7nxyJAC9urfhkj49+fmXFXGEXj5ZkEiT8RbtZkYSNWvWBGDNmjWsWbMWIQoLC/nLdf/HTTffGnOEmdGoUSPad4imDa1VqxatWrVm9uwfAbj6qssZeMttKBeKfSlavvJXAKoU5FNQkI+Z8cXkWfwwZ+MJqjZ0wiGdGPLmuIoIMW0yOKlM2nii3QwVFhbSba+ONN++ET33358/0jyoAAAS5UlEQVROnfdi0L/u59DDDqdho1Jne6sUZkyfzoQJ49mz814M+++rNG7chD3ato07rLTKyxNjnuvHDyNv5Z0xk/hs4oyk59SoXoUD927NKyMnVECE6eOlA5eV8vPzGfXJOBYvXswpvY/lo1EfMPSlFxg2/J24Q8u4ZcuWcdIJx/KPf95FQUEBf79lIMPeGBF3WGm3bp3Rpfet1KlZg8F3nMuuOzfif9+VOhyfw7rvzugJ03KvbJAFiTQZb9FuxurWrUu3ffblw/ffY9p339G+TUt2b7UzK1asoH2blnGHl3Zr1qzhpBOO5cST+nDU0ccw7bvvmDH9ezp3bEvLXXbgx1mz+EPnDsydOzfuUNNmybKVfDD2Ww7ae9ekxx5/cEeez7GyAXjpoFKSdFpYJ+gLSU9JejysH/SxpGmSjgvHNZL0gaQJkiZK2ifu2AEW/PQTixdHD7xWrlzJ+++OpF37DkyZ/iNfTfqOryZ9xxZbbMH4iZNjjjS9zIzzzz2blq1ac+nlVwDQZvfd+WH2fCZPnc7kqdNpst12jP70cxo2bBhztJum/lY1qVOzBgDVq1Vhv71aMnn6vFLPqV2zOt067sJ/3/uyIkJMG+91UAlJ2g24DuhqZgskbQ3cATQCugGtiKZQewE4GRhuZgMl5QPF9peR1Jdo3SGaNs38U++5c+dwwblnUbiuEFu3jqOOOY5Dev0x4/eN28cffcR/nnmKNm12Z6+O7QC48ea/ccihvWKOLP0a1q/NQzedSn5eHnl54sW3PueNDydy4Un7csXpB9CgXm0+G3Itb476mgtv+g8AR/Rsy8gxk1ix6teYoy+HLEikySiaiMalQtIlQEMzuy5h2+PAW2b2TPi81MxqSeoOPAo8DbxiZkmfMLTv0Mne++iTzASfxapVyY87hNhstefFcYdQ4VZPHsK6FfPTkh7btO1gL7w5KulxrRtvOa4cE3+njZcOykYUv2TF6o2OKVrsrTvwI/CUpNMyH55zm59cKB14oi2bkcAJkupBtO57SQdK2h6Yb2YPAY8ASdd+d86Vg1J4xcxrtGVgZl9LGgi8L6kQGF/K4T2AP0taAywDvEXrXJpJKa8ZFitPtGVkZk/w21rvxe2vmcpxzrn0yP4064nWOZfrciDTeqJ1zuUweenAOecyKUuedSXlidY5l9tyINN6onXO5TQvHTjnXIalI81Kmg4sBQqBtWbWKfSTHwzsAEwHTjCzn8tzfR+w4JzLXYoms0/2SlFPM2uXMFS3HzDSzJoTDVbqV94wPdE653KWiAYtJHuV05H81hf+CeCo8l7IE61zLqelOAK3vqSxCa++G13GgBGSxiXsa2BmcwDCz23LG6PXaJ1zOS3Fh2ELksze1dXMZkvaFnhL0qT0RBfxFq1zLrelYVIZM5sdfs4HXgY6A/MkNYJoIn9gfnlD9ETrnMtpm5pnJW0pqVbRe+AgYCLRJP6nh8NOB4aWN0YvHTjnclaaZu9qALwceicUAP8xszclfQYMkXQ28ANwfHlv4InWOZfbNjHPmtk04HfrzZvZQmD/Tbt6xBOtcy6nZcMKCsl4onXO5bDsWE48GU+0zrmcVTRgIdt5onXO5TRPtM45l2FeOnDOuUzatLkMKownWudczvIarXPOVQAvHTjnXIZ5P1rnnMs0T7TOOZdZuVA6kJnFHYMLJP0EzIjp9vWBBTHdO07+vSve9ma2TTouJOlNou+SzAIzOyQd9ywPT7QOAEljk0yMXCn593YVweejdc65DPNE65xzGeaJ1hUZFHcAMfHv7TLOa7TOOZdh3qJ1zrkM80TrnHMZ5onWOecyzBOtc5s5KRfmv8ptnmhd2hX9hyupWtyxlJWkvIT31TfaV+kSkqRjgY6J39uln//jurSSJDMzSQcC/5RUI+6YysLM1gFIOhe4V9JdkrpJqmqVrIuOpIuAAcCiou8dtle6Xyhx80Tr0iohyd4DvGxmK+OOqawknQBcCjxCNPHSEcDRsQaVZpJ2B84GDjSzaZIOlHS0pCaV7RdKNvDZu9wmk9TYzGaH93lAd+A6MxspqcDM1iYcq2z7D1lSD+AnM/s6tOZ2BR40szGSvgROBw6T9IKZFcYZaxr9ALwH3CVpIdAamANsgw9mSDtv0bp0uF5Sa1j/p/cWQC9JeUVJVlIXSQ2yLckG2wDLJG0V4vsWOEBSczNbYWYPAk2BnWKNMg0ktZPU3syWAEOBKcB9ZrYfMBHYPtYAKylPtG6Tmdn5wHJJg8Omp4BfgN4AkjoAdwLbxRNh8SS1D0nn+bBpiqS9gBHAZOBUSXtLOhyoASyKK9Z0kHQpMAS4R9KzZva+mV1vZt9IOgU4Fng63igrJ0+0rlwkbSlp6/C+jZn9AGwv6V9ELcIJwNGS3gUeA241s3HxRVysY4F/SGpnZjOAgUR12YZEvyyWA38F+gLnmdnC2CLdRJI6A12BPc1sH2AnSS+Hfa2AQ4EzzeybGMOstHyuA1cuoeV3NfAmcCWwD1GL723gG+CScOhuwC9mNj1b6rOhpFHUu2AQsC1wo5mND0/iLwb6mNnnkuoAhWa2LMaQN4mkQ4EzgHrA+WY2NWz/GFhiZodK2sLMVsQYZqXmLVpXLmb2CdEM/f8C/mpmP4UHRQcALYDBZlZoZl+a2fRwTuxJFjbownU2sCXQBHgilBHuJ+oxMUJSJzNbkuNJ9nygD/Bfov+99pHUFMDM9gaqhZ4GnmQzyHsduDJJ6CfbgKj1ugy4XNLnZvaNmRVKOggYFhLX+Hgj/k1iizp0b7oc6GpmSyTdDNwu6Uoze1DSauDnOOPdVJKOAC4CDjOzHyQtBk6MduldM/s+PARzGeaJ1qUsIckeCZwE9Dez5yXdADwfEmwLoJ2Z9Yo12I1slGQ7EHVvmkTU42CJmfWXNBR4QdIxZvZojOGmS2Pg2ZBkC8xsmKRC4CxgpaSZRGWRrPhLozLz0oFLWUiy3YlGE91iZlPDf8A3Av8GngPuBmbGGGaxEpJsH+BmorrscmBPSUULBT5HVF+eH0uQ6TeDqFTQMqEvcx6wEHjXzNZ6kq0Y/jDMlYmk84CWwENAD+AwYAlwLtAM+DWMNMqKB1+JJP2B6MHdADObKOkQ4AJgFpAP7A6cHHog5DxJtYkeWOYBHwN1gT8Bvc1sWpyxbW68RetKlTBBTNH49w+BBsDzgICHgbnAzmY2qeg/4GxIsgmx50kqAPYi6pB/gqRqZvYm8BeinhPfE3VvqhRJFsDMfgHuJyqTXEj0S/EcT7IVz1u0rkQJNdlDgfbAOjO7VVIVoK6Z/SSpPfAfolbSF7EGnGCjmmwDM5sX3p8KdAZGA0MShwdXZpKqApjZr3HHsjnyh2GuWEV9TSX1Am4FzgGGStqNqHW0KMwR8BBweTYlWdigJnsRcKSkL4CJZvZE+EWxF1BV0lOVaP6CEnmCjZeXDtwGJO0i6Q8hydYBzgROJers/j3RMNpngdpE3Z9ONrNhsQW8kYQSB5LOAE4mGtm1PXCFpKtDj4IpRJPHbBlHnG7z4i1at7GOwLOS9jWzD8PDr22IehrsY2a/SlpBNFz1UjNbE2OsG9ioXNAJWAr8kajDfm2iB0F/l7TOzG6XVCfUMZ3LKG/Rug2Y2WCiaQGHSepuZouAQmA20EDSHkQ12WezKcnCBuWCC4DrgC+IGhMHAKeY2ftE36OnpK3DDFbOZZy3aN3vmNlT4U/wYZKOMLP3JM0Abid6kHR2aO1mYxeuI4i6bB1uZjMkNSJqzbYID/VWAH3DLxDnKoQnWrdemOGpDTDWzJ6UtIzoAVhPM7tMUkegIMxzkBVduIrRGHguJNkqZjZH0mtEk9xsD1xgZgviDdFtbjzROmD9KgMPAp8CfSS9A9xG9CBprKT9zOy9+CJM2QyiXgYtzWxy2DaZaDTUYMvBpXVc7vNE65DUEvg/4CwzGx2S7iFEfWOfklQTqBpnjGXwEdG8q6eHaQDrEq3/dZInWRcXfxi2mQt9SjsTDas9CiC0XL8hSlbVzOwRMxuR2HUqW5UwGupsC3OwOhcHHxm2GUoY8VULWB26bJ1E9HR+jJk9FHoX/JOoVZuTKwv4aCiXLbx0sBkKSfYIovlYq0l6nWjmqnzgMkl/JCoV3JWrSRY8wbrs4Yl2MySpDXA90YOu5UQPvdYCdwAG9AQmmdlr4fis68blXC7xRLt5qk40Z+yXZrY2dPB/j2hRxVeJku3hkk4ys2c9yTq3aTzRbgYSarJVgHXAj8AqoJ2k/5nZbEkPAlXNbKmkN4lauB/GGLZzlYY/DNtMhJrsocAWRPMUnEjU0+BTohUFBhJ173o/HL9+pVjn3KbxRLsZCMvP/BM4BhgMTDOzUySdBuwMNAeeMLPhXo91Lv080W4GJF1CNMXhOqA/0QQr0yTlW7RqbXUzWxVvlM5VXl6jrYSKaZUuJuphUJffkuypQGdJlwFZNQuXc5WNjwyrhMKDrz9I6ilpV+A1olVf3wCWhEUKrwZeN7PCzWGFAefi5KWDSiRh+ZmuwAvAMKAp0RLg/wP+QdR1qz7RYIT/ek3WuczzRFsJSKpBNJS2KMkeBAw3s48lHQZcBdxmZm+ELl71w/SBnmSdqwBeOshxkrYhmmu1dth0XPhcI3x+m2jk1w2SzgmrIsyFrJ1P1rlKx1u0OS60UJsRDUBoYmafSipaCeEIM1ssqRqwP7CwaNJu51zF8USbo0KCrWZmyyRtAVxLlHDvNrNxYaRXS+A4M1vkZQLn4uOJNgdJKiBqoS4HdgD2IBrZdSXRirWPmNlYSY8BLYB9zWxtTOE6t9nzfrQ5KEwEs4aoF0FD4CozWyJpINGsXGeEwQhnSmrjSda5ePnDsBxTtMqBmb0DTAKmAaskNTKz1USJdhVwtqQ6ZjYxvmidc+Clg5ySMAvXzsA8oiG1ewHnEg0+eFpSfaIeCPlm9m2M4TrnAi8d5IiEJHsw8DDwLjAd+DtQDThVUlvgHGB/M/s8tmCdcxvwFm0OkbQn0QKKb4RNhxFNe3gNsAvQCZhhZiPjidA5VxxPtDki9IX9FphnZnuGbR2BY4mG1P7FzOYlHO/duZzLEv4wLIsVPfiStAtQC+gONJPUD8DMxgFDiWbnqp94ridZ57KHt2iznKTDgZuBGcBk4H3gcaK5C24Lx9Q2s19iC9I5Vyp/GJbFJHUh6q51YHgNAlYCZwAvhL6yt3iSdS67eYs2i0naDmgEbEXUqj0Z+Dcwm2i12sVm9lZ8ETrnUuE12ixmZrPM7DNgX+AZM5tKVDZoDYwxs7eK6rjOuezlpYPc8BVwXpjj4HDgEjObCf7Qy7lc4Ik2N7xONCjhCKKHYKNjjsc5VwZeo80hkgrChDLeR9a5HOI12txSCF4ucC7XeIvWOecyzFu0zjmXYZ5onXMuwzzROudchnmidRVKUqGkCZImSno+LCxZ3mv1kDQsvD+iaLKdEo6tK+nCctxjgKSrUt2+0TGPSzquDPfaQZKviFEJeaJ1FW2lmbUzszbAr8D5iTsVKfP/X5rZq2Z2aymH1AXKnGidSwdPtC5OHwK7hJbcN5IeAD4Hmko6SNJoSZ+Hlm9NAEmHSJokaRRwTNGFJJ0h6b7wvoGklyV9EV57A7cCO4fW9D/CcX+W9JmkLyXdmHCt6yRNlvQ20ZLtpZJ0brjOF5Je3KiVfoCkDyVNkfTHcHy+pH8k3Pu8Tf2HdNnNE62LRRhOfCjR8GKIEtqTZtaeaBn1/sABZtYBGAtcIak68BDRMOR9iFYALs49wPtm1hboAHwN9AO+C63pP0s6CGgOdAbaAR0ldQ+TqfcG2hMl8j1T+Dovmdme4X7fAGcn7NuBaK6Kw4B/he9wNrAkTOC+J3CupB1TuI/LUT4E11W0GpImhPcfAo8AjYmW4BkTtncBdgU+CnPmVAVGA62A74sWnZT0NNC3mHvsB5wGYGaFwBJJW210zEHhNT58rkmUeGsBL5vZinCPV1P4Tm0k3UxUnqgJDE/YN8TM1gHfSpoWvsNBwB4J9ds64d5TUriXy0GeaF1FW2lm7RI3hGS6PHET8JaZnbTRce2AdI2wEXCLmf17o3tcVo57PA4cZWZfSDoD6JGwb+NrWbj3JWaWmJCRtEMZ7+tyhJcOXDYaA3QNS/ggaQtJLYBJwI5huXWAk0o4fyRwQTg3X1JtYClRa7XIcOCshNpvE0nbAh8AR0uqIakWUZkimVrAHElVgD4b7TteUl6IeSeiVTKGAxeE45HUQtKWKdzH5Shv0bqsY2Y/hZbhs2FRSoD+ZjZFUl/gNUkLgFFAm2IucSkwSNLZRPNDXGBmoyV9FLpPvRHqtK2B0aFFvQw4xcw+lzQYmEC0fNCHKYT8F+CTcPxXbJjQi5YfagCcb2arJD1MVLv9PMwn/BPR6saukvK5DpxzLsO8dOCccxnmidY55zLME61zzmWYJ1rnnMswT7TOOZdhnmidcy7DPNE651yG/T8Bqgv9kT4CkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('224_3cls_unf_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('224_3cls_unf_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5259911894273128, 1.3275131736628687)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds, y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=np.array([5e-5,5e-4,5e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8237f0e4d514062836c61a8b5a551f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learn.set_data(get_data(299, 48)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1baed6466f214ecdbfceb6200c287485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94%|| 85/90 [01:06<00:04,  1.21it/s, loss=1.39] "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrf=learn.lr_find(start_lr=1e-9, end_lr=10)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787e256531e04a76b65ccda8025000dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.682825   0.67513    0.725581  \n",
      "    1      0.681779   0.669802   0.724651                  \n",
      "    2      0.665875   0.668171   0.723721                  \n",
      "    3      0.675664   0.668802   0.71814                   \n",
      "    4      0.674737   0.66029    0.731163                  \n",
      "    5      0.657659   0.658051   0.736744                  \n",
      "    6      0.63473    0.662198   0.72093                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.6622]), 0.7209302371601726]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 3, cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_3cls_unf_bs48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_3cls_unf_bs48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ecb8b5806a417dbfe25ade95d37e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1023), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27%|       | 51/190 [00:36<01:35,  1.45it/s, loss=1.38]"
     ]
    }
   ],
   "source": [
    "learn.fit(lr, 10, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is so similar to ImageNet dataset. Training convolution layers doesn't help much. We are not going to unfreeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission\n",
    "\n",
    "https://youtu.be/9C06ZPF8Uuc?t=1905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA(is_test=True) # use test dataset rather than validation dataset\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "#accuracy_np(probs, y), metrcs.log_loss(y, probs) # This does not make sense since test dataset has no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape # (n_images, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(probs)\n",
    "df.columns = data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, 'id', [o[5:-4] for o in data.test_ds.fnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM = f'{PATH}/subm/'\n",
    "os.makedirs(SUBM, exist_ok=True)\n",
    "df.to_csv(f'{SUBM}subm.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(f'{SUBM}subm.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = data.val_ds.fnames[0]\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(PATH + fn).resize((150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1.\n",
    "trn_tfms, val_tfms = tfms_from_model(arch, sz)\n",
    "ds = FilesIndexArrayDataset([fn], np.array([0]), val_tfms, PATH)\n",
    "dl = DataLoader(ds)\n",
    "preds = learn.predict_dl(dl)\n",
    "np.argmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2.\n",
    "trn_tfms, val_tfms = tfms_from_model(arch, sz)\n",
    "im = val_tfms(open_image(PATH + fn)) # open_image() returns numpy.ndarray\n",
    "preds = learn.predict_array(im[None])\n",
    "np.argmax(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastaiold",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0147ea39a83144aa99e7877a908ca502": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "02af08793c9a4c5ba0cbd7462a715f89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "03c7476e32ce48c2b38a7023c2dcc0a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "075434e4b0234c5fa61c8d63fec51679": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "08ba1e10c1a641f7876680a632f6821d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b0edcf9ef0754ca4bb7fe6e8790c464b",
       "max": 6,
       "style": "IPY_MODEL_c13bdace91114d639b408aa120c97f79",
       "value": 6
      }
     },
     "0b49f5119c6d4a25bb575a20e33fe219": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12cc98b1fc954667bd49c08972278f1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13cbbf3a554a40f09d40df39c2cacf47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "148c1abd5f1747a2a787c09cef3071bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_902c060a37b4491788462dce6be1798b",
       "style": "IPY_MODEL_88db019106ca4893816ea780e567a175",
       "value": "100% 3/3 [16:30&lt;00:00, 330.17s/it]"
      }
     },
     "16993380d0fa42b888a52dbf1bb42f20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c74dedf09f57490a9f1ac9af54e14bf3",
       "max": 6,
       "style": "IPY_MODEL_557d35c901f54575b5963e3b7ce932a3",
       "value": 6
      }
     },
     "19c5557812814cb28bc7e0cc80c65c5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c241c0e5b5844508c1c50e2a7c09b1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c509f99160e41d5af6e565b1541e571": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9352e3fd2af54e09ad67afb4ac446b27",
       "style": "IPY_MODEL_62e7ecfbbea7427385deb25ed2f4232f",
       "value": "100% 6/6 [00:00&lt;00:00, 380.26it/s]"
      }
     },
     "1e2f1c40607144eb9cb7b9e257d06f35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1fc90792a2b74c38aaf73659c3756831": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "201ae86e437e4b5390782e9df272326d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c8871c57d7bd4b61a99633668fb44621",
        "IPY_MODEL_38eb37d7293d4273a6ae06b5a451617d"
       ],
       "layout": "IPY_MODEL_1c241c0e5b5844508c1c50e2a7c09b1b"
      }
     },
     "20bfce64bf354e2398353b6a2a31d0e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21e75994f5e749748dd5693819e51d93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "22952b3bfa674aff898e452b30bd319e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "26e2b0e7abb24e3fad1b5c0c166aa44d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_363595af2f5f4ad3a3f5a9e46069d58e",
       "max": 5,
       "style": "IPY_MODEL_02af08793c9a4c5ba0cbd7462a715f89",
       "value": 5
      }
     },
     "288366a4c5ba435286c34d6d5ad69201": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33f7abed940a4b24b01110da8ec67ddc",
        "IPY_MODEL_93f4380114fe48b88db18d0c97736fd4"
       ],
       "layout": "IPY_MODEL_5b17418f64954c14843a914942f8ce3d"
      }
     },
     "292be092bc964dd7a48bc62c906f5814": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2d1dfcb285154a7bac2db184a87f3c4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ff92f760a2b46a99576ab77daccac1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33ce7c9b732e4715ae12b71669addfdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_20bfce64bf354e2398353b6a2a31d0e5",
       "max": 6,
       "style": "IPY_MODEL_830aa543858f4a54a14aad440bcbde34",
       "value": 6
      }
     },
     "33f7abed940a4b24b01110da8ec67ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7ba0d94e2ef544fe9126eacee6588468",
       "max": 6,
       "style": "IPY_MODEL_8c8cb52ff5e54c55a297af7571a105ea",
       "value": 6
      }
     },
     "363595af2f5f4ad3a3f5a9e46069d58e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37f00a662a3c4b5f9f29db6903cfc5ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "37faed3c96f343e895d8f29ab07137c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "38eb37d7293d4273a6ae06b5a451617d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_42c23fec7e814a9aaf39ffa59c0ffe20",
       "style": "IPY_MODEL_7163788d004d4c15b0b0137a654079a0",
       "value": "100% 7/7 [35:35&lt;00:00, 305.08s/it]"
      }
     },
     "39cf7d6b5e98460e9a621f1d74e32fad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb554ddf38bb4597b282655ee74ca321",
       "style": "IPY_MODEL_c45beabad9d34af5b39ce603b5308ef9",
       "value": "100% 5/5 [27:26&lt;00:00, 329.38s/it]"
      }
     },
     "39d7df82d70b40df9f1171dad9ce2b4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3a1160614abc4724b73cc9bc2e0d0d46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_19c5557812814cb28bc7e0cc80c65c5f",
       "style": "IPY_MODEL_87707de469bc4c4bb2ba2a2ed7322de1",
       "value": "100% 5/5 [00:28&lt;00:00,  5.70s/it]"
      }
     },
     "3b2e29458fdc49d0864a7fc9a1faafb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ed035ff92d947ad9059d4bc03f2576c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_16993380d0fa42b888a52dbf1bb42f20",
        "IPY_MODEL_a52bd3eee7aa4e9093ee525fc42ff232"
       ],
       "layout": "IPY_MODEL_66e271ee790148eb89ae51b9b22216aa"
      }
     },
     "40060908ceee46fb859195274eca3da8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4b3362ffeac948e490ce91f1e0786f2e",
        "IPY_MODEL_c09f764cc1c74e3aa75fea864a8d21f3"
       ],
       "layout": "IPY_MODEL_e6926f2efb5340528a00c5045376c380"
      }
     },
     "405077ae66ac44afbd35d90a6afd6cab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41a9d48c29ef4752bcd2ddc7076dac3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "42c23fec7e814a9aaf39ffa59c0ffe20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "43f1cb9394c5422798a91c1b6b0a5907": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45a9f25bf3964af68dfce8bbb2091917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4654ebf91a484cb8b153920fb89bb522": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b3362ffeac948e490ce91f1e0786f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_67e1b7b183484f03b2f63c22c44e8713",
       "max": 6,
       "style": "IPY_MODEL_12cc98b1fc954667bd49c08972278f1d",
       "value": 6
      }
     },
     "4c5e108b0b9145cb9987a673dda89bb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d7cbd1c9b0b45359d4d31ad4e50e441": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "54f04d9c381446649b742e1242de6953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_f6539e378c294a6c955da278da4ade4e",
       "max": 2,
       "style": "IPY_MODEL_b1cddc992f864976b0f4132e0c4819f4",
       "value": 2
      }
     },
     "557d35c901f54575b5963e3b7ce932a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5791e4167111408ca3f1be2e16334803": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b0ec0306b6440c1a068951bf0d4a50c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5b17418f64954c14843a914942f8ce3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b494db622384a7fabd7ed86d7f0575c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_26e2b0e7abb24e3fad1b5c0c166aa44d",
        "IPY_MODEL_c05cfeeac9a04c68a7387dc65bf48227"
       ],
       "layout": "IPY_MODEL_4654ebf91a484cb8b153920fb89bb522"
      }
     },
     "5c13867f02614d49bc038af563f5a5a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7234b227a46b404598576e19a230a94e",
       "max": 6,
       "style": "IPY_MODEL_7ae2ea4a6e144242ad504c2032fbb9b9",
       "value": 6
      }
     },
     "5d3aa8de854a491485458bcaf88b9de4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c6a3a9d502b94ffba6cb6708c9762bd0",
        "IPY_MODEL_8d9389e09e1d44d88ffffdb83ac6f407"
       ],
       "layout": "IPY_MODEL_03c7476e32ce48c2b38a7023c2dcc0a1"
      }
     },
     "5f4033806f5144f28e0999110bf34612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_e59a2233ae454d81ada1b8494763e7bc",
       "max": 2,
       "style": "IPY_MODEL_292be092bc964dd7a48bc62c906f5814",
       "value": 2
      }
     },
     "61defccfe9dc4330931ce420b9976dcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4d7cbd1c9b0b45359d4d31ad4e50e441",
       "style": "IPY_MODEL_45a9f25bf3964af68dfce8bbb2091917",
       "value": "100% 2/2 [10:08&lt;00:00, 304.26s/it]"
      }
     },
     "62e7ecfbbea7427385deb25ed2f4232f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "62e8fbf1622b4a36a186e7f319552990": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_41a9d48c29ef4752bcd2ddc7076dac3c",
       "max": 2,
       "style": "IPY_MODEL_5b0ec0306b6440c1a068951bf0d4a50c",
       "value": 2
      }
     },
     "65d9210f3d3345019a1fcf4ad82f1ae9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "66179fdeb4534eb281f2690ded33950e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "66e271ee790148eb89ae51b9b22216aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67e1b7b183484f03b2f63c22c44e8713": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7163788d004d4c15b0b0137a654079a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7234b227a46b404598576e19a230a94e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "77132b06e4ad40fdabdc0dd93922af21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "784cd320488247aa87d575558f81d06b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7998921f656343ffadf133220692f859": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_0b49f5119c6d4a25bb575a20e33fe219",
       "max": 5,
       "style": "IPY_MODEL_cfeaf84bdbe34c7898e1ea8fc61b539c",
       "value": 5
      }
     },
     "7ad08fc0082a41bc882cedf6004206dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33ce7c9b732e4715ae12b71669addfdf",
        "IPY_MODEL_bc6e6b33b84344f094704454a0cf6f6b"
       ],
       "layout": "IPY_MODEL_2ff92f760a2b46a99576ab77daccac1d"
      }
     },
     "7adf432cd98f46df927bf9da9d94b182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7ae2ea4a6e144242ad504c2032fbb9b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7b215de0d6b64c3897c22bd817bbcb8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_1e2f1c40607144eb9cb7b9e257d06f35",
       "max": 3,
       "style": "IPY_MODEL_f38f8ae81eca43f9b1828369ebffd849",
       "value": 3
      }
     },
     "7ba0d94e2ef544fe9126eacee6588468": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8228413e70844312b5aa3128e3bea3ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_890f789420e4416697cafbf732f38b95",
       "style": "IPY_MODEL_b52cba9c0daf49aab716b75c91137766",
       "value": "100% 2/2 [00:06&lt;00:00,  3.12s/it]"
      }
     },
     "830aa543858f4a54a14aad440bcbde34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "868f105f38aa47b2b0223680dc202859": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87707de469bc4c4bb2ba2a2ed7322de1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "879f0f26c06041698336e79b9b92b0c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c19f1e5cf61444b5926c89b67c374dc8",
        "IPY_MODEL_148c1abd5f1747a2a787c09cef3071bc"
       ],
       "layout": "IPY_MODEL_b4a9636246544cd7a3ebcff56595bea2"
      }
     },
     "88db019106ca4893816ea780e567a175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "890f789420e4416697cafbf732f38b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c8cb52ff5e54c55a297af7571a105ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8d9389e09e1d44d88ffffdb83ac6f407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1fc90792a2b74c38aaf73659c3756831",
       "style": "IPY_MODEL_66179fdeb4534eb281f2690ded33950e",
       "value": "100% 2/2 [00:06&lt;00:00,  3.17s/it]"
      }
     },
     "902c060a37b4491788462dce6be1798b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "91b150ff417c44d1bc68e66d1b3c5925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9352e3fd2af54e09ad67afb4ac446b27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93f4380114fe48b88db18d0c97736fd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b7e6303bb38542c9a8282079c33beca7",
       "style": "IPY_MODEL_21e75994f5e749748dd5693819e51d93",
       "value": "100% 6/6 [00:00&lt;00:00, 343.35it/s]"
      }
     },
     "956dbbddf33a476db031c3d1b8ad5b4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7998921f656343ffadf133220692f859",
        "IPY_MODEL_39cf7d6b5e98460e9a621f1d74e32fad"
       ],
       "layout": "IPY_MODEL_65d9210f3d3345019a1fcf4ad82f1ae9"
      }
     },
     "a19c0f9c5cb34e9e8cdfe5c356084b80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a52bd3eee7aa4e9093ee525fc42ff232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_405077ae66ac44afbd35d90a6afd6cab",
       "style": "IPY_MODEL_fa7fbd7aa5ab41a5aaabfca1db46058e",
       "value": "100% 6/6 [00:00&lt;00:00, 377.47it/s]"
      }
     },
     "a6ba13af33984f1b96ed120d3798b035": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6cf535c8f30471e947503cae6bdb611": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_e77a75b0e9b446318ef399e225003e65",
       "max": 7,
       "style": "IPY_MODEL_c8469a077e5449b69821e6b27b8e74e5",
       "value": 7
      }
     },
     "a7afeac8de60479e8c92248312fde8fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aad1581c410d48feb48e6b943c14bdb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b0edcf9ef0754ca4bb7fe6e8790c464b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b1258a718296423d9f4d9c0535aae63b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3b2e29458fdc49d0864a7fc9a1faafb6",
       "style": "IPY_MODEL_37f00a662a3c4b5f9f29db6903cfc5ba",
       "value": "100% 3/3 [15:16&lt;00:00, 305.39s/it]"
      }
     },
     "b1cddc992f864976b0f4132e0c4819f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b3ae58ce0bce4cd1b678a043b8ce4415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_54f04d9c381446649b742e1242de6953",
        "IPY_MODEL_e10c6be126a045d9809b90c0880e7362"
       ],
       "layout": "IPY_MODEL_77132b06e4ad40fdabdc0dd93922af21"
      }
     },
     "b4a9636246544cd7a3ebcff56595bea2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b4c9fe6ac6f649dc9ed8a077a5eaeb20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b52cba9c0daf49aab716b75c91137766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b7e6303bb38542c9a8282079c33beca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba26964d72134b3aa6095e3102f1531b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc6e6b33b84344f094704454a0cf6f6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e6337586212341de8a4de0ce6bd25649",
       "style": "IPY_MODEL_7adf432cd98f46df927bf9da9d94b182",
       "value": "100% 6/6 [00:00&lt;00:00, 270.73it/s]"
      }
     },
     "bd8469f87f2d4e789610110acd07571d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed2e720acb7a467c85bd7db465281523",
        "IPY_MODEL_3a1160614abc4724b73cc9bc2e0d0d46"
       ],
       "layout": "IPY_MODEL_37faed3c96f343e895d8f29ab07137c0"
      }
     },
     "c01a6ed0806a4aaaaf580bf56e78398f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c05cfeeac9a04c68a7387dc65bf48227": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7afeac8de60479e8c92248312fde8fc",
       "style": "IPY_MODEL_22952b3bfa674aff898e452b30bd319e",
       "value": "100% 5/5 [00:16&lt;00:00,  3.28s/it]"
      }
     },
     "c09f764cc1c74e3aa75fea864a8d21f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c80d11f161704ab3bda86e7d1dfaff03",
       "style": "IPY_MODEL_e1df5ccb47594688afaee7c51d9cfeed",
       "value": "100% 6/6 [00:00&lt;00:00, 377.26it/s]"
      }
     },
     "c13bdace91114d639b408aa120c97f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c19f1e5cf61444b5926c89b67c374dc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_43f1cb9394c5422798a91c1b6b0a5907",
       "max": 3,
       "style": "IPY_MODEL_aad1581c410d48feb48e6b943c14bdb8",
       "value": 3
      }
     },
     "c28408357da544cd8c8bbd5c7469e541": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5c13867f02614d49bc038af563f5a5a2",
        "IPY_MODEL_1c509f99160e41d5af6e565b1541e571"
       ],
       "layout": "IPY_MODEL_e41e9dc77ce44f3fad0aee4db429cd7d"
      }
     },
     "c30905e882964682bbe231e445b57833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08ba1e10c1a641f7876680a632f6821d",
        "IPY_MODEL_e6f6231f671e4871b9e496cce50bf2b9"
       ],
       "layout": "IPY_MODEL_db256ef0046f46d085353ffba880c8fa"
      }
     },
     "c438be876cfc49e8920f2704c6cddbdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_de9c288444e74ea38dbac2ef1b1078e4",
        "IPY_MODEL_fe3c562acb4e432280f6432df0710577"
       ],
       "layout": "IPY_MODEL_ba26964d72134b3aa6095e3102f1531b"
      }
     },
     "c45beabad9d34af5b39ce603b5308ef9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c522ca216e5a49e4a7624f94b7215c52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_62e8fbf1622b4a36a186e7f319552990",
        "IPY_MODEL_8228413e70844312b5aa3128e3bea3ad"
       ],
       "layout": "IPY_MODEL_cb6af7a441964132bb58e75ef7b4bd19"
      }
     },
     "c6a3a9d502b94ffba6cb6708c9762bd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_868f105f38aa47b2b0223680dc202859",
       "max": 2,
       "style": "IPY_MODEL_f4c78df23c774bb290250936916d962e",
       "value": 2
      }
     },
     "c74dedf09f57490a9f1ac9af54e14bf3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c80d11f161704ab3bda86e7d1dfaff03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8469a077e5449b69821e6b27b8e74e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c8871c57d7bd4b61a99633668fb44621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_2d1dfcb285154a7bac2db184a87f3c4e",
       "max": 7,
       "style": "IPY_MODEL_ce4922cb72aa4e5c8b5713e47e49029e",
       "value": 7
      }
     },
     "c8ef187b9a1249018de7378cee824894": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b215de0d6b64c3897c22bd817bbcb8d",
        "IPY_MODEL_b1258a718296423d9f4d9c0535aae63b"
       ],
       "layout": "IPY_MODEL_784cd320488247aa87d575558f81d06b"
      }
     },
     "cac9427c464f4db9b71c36da9245c066": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb554ddf38bb4597b282655ee74ca321": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cb6af7a441964132bb58e75ef7b4bd19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce4922cb72aa4e5c8b5713e47e49029e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cfeaf84bdbe34c7898e1ea8fc61b539c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d2b4db8ba2cb4c859ba3e0041c243193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_13cbbf3a554a40f09d40df39c2cacf47",
       "style": "IPY_MODEL_91b150ff417c44d1bc68e66d1b3c5925",
       "value": "100% 7/7 [38:27&lt;00:00, 329.65s/it]"
      }
     },
     "db256ef0046f46d085353ffba880c8fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "de9c288444e74ea38dbac2ef1b1078e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_0147ea39a83144aa99e7877a908ca502",
       "max": 5,
       "style": "IPY_MODEL_075434e4b0234c5fa61c8d63fec51679",
       "value": 5
      }
     },
     "def3b0d0275c454e95c5c2952fc9d017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5f4033806f5144f28e0999110bf34612",
        "IPY_MODEL_61defccfe9dc4330931ce420b9976dcd"
       ],
       "layout": "IPY_MODEL_e22e48eeda32478cbb6bf53babd9e700"
      }
     },
     "e10c6be126a045d9809b90c0880e7362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5791e4167111408ca3f1be2e16334803",
       "style": "IPY_MODEL_c01a6ed0806a4aaaaf580bf56e78398f",
       "value": "100% 2/2 [10:59&lt;00:00, 329.53s/it]"
      }
     },
     "e1df5ccb47594688afaee7c51d9cfeed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e22e48eeda32478cbb6bf53babd9e700": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e41e9dc77ce44f3fad0aee4db429cd7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e59a2233ae454d81ada1b8494763e7bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6337586212341de8a4de0ce6bd25649": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6926f2efb5340528a00c5045376c380": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6f6231f671e4871b9e496cce50bf2b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4c5e108b0b9145cb9987a673dda89bb6",
       "style": "IPY_MODEL_a19c0f9c5cb34e9e8cdfe5c356084b80",
       "value": "100% 6/6 [00:00&lt;00:00, 42.32it/s]"
      }
     },
     "e77a75b0e9b446318ef399e225003e65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ed2e720acb7a467c85bd7db465281523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_f9852feeb51543e4a0d21c5cd6326380",
       "max": 5,
       "style": "IPY_MODEL_cac9427c464f4db9b71c36da9245c066",
       "value": 5
      }
     },
     "f2cf6963cc204b18921e63e1933ad18b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a6cf535c8f30471e947503cae6bdb611",
        "IPY_MODEL_d2b4db8ba2cb4c859ba3e0041c243193"
       ],
       "layout": "IPY_MODEL_b4c9fe6ac6f649dc9ed8a077a5eaeb20"
      }
     },
     "f38f8ae81eca43f9b1828369ebffd849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f4c78df23c774bb290250936916d962e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f6539e378c294a6c955da278da4ade4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f9852feeb51543e4a0d21c5cd6326380": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa7fbd7aa5ab41a5aaabfca1db46058e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fe3c562acb4e432280f6432df0710577": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a6ba13af33984f1b96ed120d3798b035",
       "style": "IPY_MODEL_39d7df82d70b40df9f1171dad9ce2b4c",
       "value": "100% 5/5 [25:26&lt;00:00, 305.38s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
