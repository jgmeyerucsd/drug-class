{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precompute=T then F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "PATH = \"data/drugs2/pics/\"\n",
    "sz = 224\n",
    "arch = resnext101_64\n",
    "bs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz, bs, val_idxs, label_csv): # sz: image size, bs: batch size\n",
    "    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'train', label_csv,\n",
    "                                       val_idxs=val_idxs, suffix='.png', tfms=tfms, bs=bs)\n",
    "    \n",
    "    # http://forums.fast.ai/t/how-to-train-on-the-full-dataset-using-imageclassifierdata-from-csv/7761/13\n",
    "    # http://forums.fast.ai/t/how-to-train-on-the-full-dataset-using-imageclassifierdata-from-csv/7761/37\n",
    "    return data if sz > 300 else data.resize(340, 'tmp') # Reading the jpgs and resizing is slow for big images, so resizing them all to 340 first saves time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute = F, testing dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfld_loop3(k, epochs, name, precomp, bs, dropouts, label_csv, n):\n",
    "    validation_accuracy = []\n",
    "    for do in dropouts:\n",
    "        for reps in range(k):\n",
    "            val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1)) # random 20% data for validation set\n",
    "            data = get_data(sz, bs, val_idxs, label_csv)\n",
    "            print('dropout='+str(do))\n",
    "            learn = ConvLearner.pretrained(arch, data, precompute=precomp, ps = do)\n",
    "            val_loss, val_acc = learn.fit(1e-2, epochs)\n",
    "            validation_accuracy.append(val_acc)\n",
    "            learn.save(str(reps)+'_DO_'+str(do)+name)\n",
    "    return validation_accuracy, learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-class, testing dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f2f4e7e6ae4e43b06ecb6575430d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b77033e7064bb690dc6e8521fa138a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.407791   1.223355   0.505817  \n",
      "    1      1.207939   1.148773   0.538504                 \n",
      "    2      1.116      1.109407   0.555125                 \n",
      "    3      1.047501   1.102384   0.568975                 \n",
      "    4      0.983114   1.054575   0.576177                 \n",
      "    5      0.928545   1.040219   0.574515                  \n",
      "    6      0.885943   1.030758   0.603324                  \n",
      "    7      0.851791   1.023538   0.601662                  \n",
      "    8      0.813639   1.016968   0.601108                  \n",
      "    9      0.783929   1.02619    0.612742                  \n",
      "    10     0.755137   1.019078   0.60277                   \n",
      "    11     0.73928    1.001652   0.607756                  \n",
      "    12     0.708923   1.001565   0.598892                  \n",
      "    13     0.686529   1.009206   0.607202                  \n",
      "    14     0.660161   1.015776   0.614958                  \n",
      "    15     0.647457   0.998438   0.614958                  \n",
      "    16     0.630543   1.013641   0.623823                  \n",
      "    17     0.613479   1.004779   0.618283                  \n",
      "    18     0.592486   1.0105     0.617175                  \n",
      "    19     0.57793    1.023558   0.624377                  \n",
      "    20     0.556709   1.02702    0.619391                  \n",
      "    21     0.543644   1.034761   0.628255                  \n",
      "    22     0.53727    1.062432   0.600554                  \n",
      "    23     0.540249   1.026698   0.610526                  \n",
      "    24     0.523233   1.023384   0.623823                  \n",
      "    25     0.514721   1.025157   0.618283                  \n",
      "    26     0.509249   1.043912   0.614958                  \n",
      "    27     0.498727   1.061949   0.609972                  \n",
      "    28     0.48728    1.06024    0.623823                  \n",
      "    29     0.481046   1.056667   0.623269                  \n",
      "    30     0.474253   1.038415   0.631025                  \n",
      "    31     0.467163   1.065808   0.627701                  \n",
      "    32     0.460666   1.065672   0.623269                  \n",
      "    33     0.451277   1.06405    0.631025                  \n",
      "    34     0.442589   1.065174   0.619945                  \n",
      "    35     0.444372   1.079517   0.634349                  \n",
      "    36     0.43414    1.105441   0.620499                  \n",
      "    37     0.438999   1.093048   0.632133                  \n",
      "    38     0.429752   1.11442    0.618283                  \n",
      "    39     0.418011   1.101149   0.626039                  \n",
      "    40     0.408454   1.127435   0.625485                  \n",
      "    41     0.398872   1.131787   0.616066                  \n",
      "    42     0.397878   1.110157   0.622715                  \n",
      "    43     0.393965   1.113783   0.624377                  \n",
      "    44     0.393712   1.129768   0.628809                  \n",
      "    45     0.373045   1.125706   0.622715                  \n",
      "    46     0.360858   1.117824   0.633241                  \n",
      "    47     0.36778    1.156765   0.622161                  \n",
      "    48     0.368777   1.130315   0.631025                  \n",
      "    49     0.363438   1.149002   0.634903                  \n",
      "    50     0.353378   1.156325   0.629363                  \n",
      "    51     0.341795   1.145329   0.622161                  \n",
      "    52     0.349821   1.129412   0.636011                  \n",
      "    53     0.365364   1.16558    0.630471                  \n",
      "    54     0.367234   1.193157   0.622161                  \n",
      "    55     0.354605   1.154195   0.623269                  \n",
      "    56     0.35126    1.147304   0.631025                  \n",
      "    57     0.34976    1.189164   0.627147                 \n",
      "    58     0.344563   1.173814   0.626593                  \n",
      "    59     0.33562    1.171271   0.623823                  \n",
      "    60     0.341784   1.210294   0.624377                  \n",
      "    61     0.351613   1.1894     0.627147                  \n",
      "    62     0.344325   1.196398   0.621053                  \n",
      "    63     0.329016   1.18964    0.623269                  \n",
      "    64     0.324462   1.215602   0.624931                  \n",
      "    65     0.332047   1.230055   0.626039                  \n",
      "    66     0.331974   1.212541   0.624931                  \n",
      "    67     0.332902   1.194418   0.628809                  \n",
      "    68     0.331113   1.207272   0.624377                 \n",
      "    69     0.315186   1.221957   0.618837                  \n",
      "    70     0.315099   1.224807   0.628255                  \n",
      "    71     0.310387   1.212953   0.628809                  \n",
      "    72     0.318561   1.193924   0.628255                  \n",
      "    73     0.315659   1.20162    0.624931                  \n",
      "    74     0.32504    1.237167   0.627147                  \n",
      "    75     0.334664   1.232219   0.632687                  \n",
      "    76     0.330125   1.20714    0.631579                  \n",
      "    77     0.326265   1.213215   0.627147                  \n",
      "    78     0.317125   1.209139   0.629363                  \n",
      "    79     0.308756   1.231689   0.631579                  \n",
      "    80     0.308341   1.233286   0.617175                  \n",
      "    81     0.30695    1.217158   0.627701                  \n",
      "    82     0.323911   1.229547   0.627701                  \n",
      "    83     0.322017   1.208619   0.629363                  \n",
      "    84     0.304607   1.230982   0.622161                  \n",
      "    85     0.294576   1.2445     0.631579                  \n",
      "    86     0.292542   1.258811   0.624377                  \n",
      "    87     0.29072    1.257374   0.626039                  \n",
      "    88     0.289955   1.258404   0.621607                  \n",
      "    89     0.286132   1.243067   0.628255                  \n",
      "    90     0.281239   1.237958   0.623269                  \n",
      "    91     0.282207   1.239519   0.627701                  \n",
      "    92     0.271093   1.242071   0.624931                  \n",
      "    93     0.272102   1.24438    0.629917                  \n",
      "    94     0.267356   1.254612   0.620499                  \n",
      "    95     0.269468   1.24843    0.626039                  \n",
      "    96     0.271394   1.262949   0.623823                  \n",
      "    97     0.264493   1.253564   0.628255                  \n",
      "    98     0.260985   1.254606   0.624931                  \n",
      "    99     0.264396   1.278144   0.630471                  \n",
      "   100     0.273955   1.28791    0.625485                  \n",
      "   101     0.266476   1.285277   0.621607                  \n",
      "   102     0.260186   1.265641   0.621607                  \n",
      "   103     0.260503   1.273095   0.624377                  \n",
      "   104     0.258829   1.290451   0.631579                  \n",
      "   105     0.265365   1.301055   0.627147                  \n",
      "   106     0.259741   1.283871   0.627701                  \n",
      "   107     0.255781   1.287722   0.624931                  \n",
      "   108     0.26299    1.301288   0.621607                  \n",
      "   109     0.257588   1.29806    0.632687                  \n",
      "   110     0.268236   1.289853   0.631025                  \n",
      "   111     0.281776   1.298778   0.629917                  \n",
      "   112     0.278101   1.280428   0.627147                  \n",
      "   113     0.275173   1.27716    0.632133                  \n",
      "   114     0.269358   1.292605   0.632133                  \n",
      "   115     0.27367    1.263007   0.623823                  \n",
      "   116     0.266267   1.298683   0.631025                  \n",
      "   117     0.265616   1.265964   0.624931                  \n",
      "   118     0.255978   1.290419   0.632687                  \n",
      "   119     0.25861    1.312368   0.624931                  \n",
      "   120     0.264362   1.327458   0.626593                  \n",
      "   121     0.263023   1.320142   0.624931                  \n",
      "   122     0.259043   1.297996   0.626039                  \n",
      "   123     0.252487   1.335279   0.622715                  \n",
      "   124     0.25145    1.326072   0.631025                  \n",
      "   125     0.251753   1.325772   0.625485                  \n",
      "   126     0.251312   1.309912   0.629917                  \n",
      "   127     0.245858   1.313904   0.631025                  \n",
      "   128     0.250431   1.330599   0.627701                  \n",
      "   129     0.253086   1.343164   0.623269                  \n",
      "   130     0.24991    1.352464   0.624377                  \n",
      "   131     0.247271   1.332502   0.628809                  \n",
      "   132     0.250483   1.353277   0.626593                  \n",
      "   133     0.251113   1.340727   0.628809                  \n",
      "   134     0.255419   1.347163   0.623823                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.24663    1.361012   0.628809                  \n",
      "   136     0.250727   1.346276   0.627147                  \n",
      "   137     0.250185   1.3266     0.625485                  \n",
      "   138     0.247914   1.321586   0.625485                  \n",
      "   139     0.248594   1.334683   0.621053                  \n",
      "   140     0.245947   1.340573   0.617175                  \n",
      "   141     0.247088   1.357107   0.615512                  \n",
      "   142     0.256996   1.334824   0.626039                  \n",
      "   143     0.249253   1.350659   0.624931                  \n",
      "   144     0.243047   1.383204   0.623269                  \n",
      "   145     0.237656   1.363766   0.624377                  \n",
      "   146     0.238937   1.338641   0.624931                  \n",
      "   147     0.245729   1.34806    0.620499                  \n",
      "   148     0.245261   1.348041   0.623269                  \n",
      "   149     0.250651   1.351554   0.618837                  \n",
      "   150     0.241373   1.343461   0.623823                  \n",
      "   151     0.238708   1.350392   0.618283                  \n",
      "   152     0.239981   1.361644   0.626039                  \n",
      "   153     0.239683   1.371305   0.624377                  \n",
      "   154     0.233761   1.349947   0.623823                  \n",
      "   155     0.227834   1.343914   0.622715                  \n",
      "   156     0.235381   1.349939   0.621607                  \n",
      "   157     0.235624   1.340884   0.631025                  \n",
      "   158     0.229879   1.332423   0.628255                  \n",
      "   159     0.234111   1.344498   0.632687                  \n",
      "   160     0.247371   1.342343   0.634903                  \n",
      "   161     0.244356   1.323381   0.624377                  \n",
      "   162     0.23094    1.32913    0.629917                  \n",
      "   163     0.227585   1.328087   0.631579                  \n",
      "   164     0.221679   1.341986   0.626593                  \n",
      "   165     0.222947   1.366146   0.627147                  \n",
      "   166     0.229009   1.384      0.627147                  \n",
      "   167     0.229532   1.383678   0.626593                  \n",
      "   168     0.233621   1.372084   0.629917                  \n",
      "   169     0.231327   1.351377   0.628809                  \n",
      "   170     0.231861   1.355289   0.633795                  \n",
      "   171     0.237565   1.3725     0.624931                  \n",
      "   172     0.239088   1.347619   0.626593                  \n",
      "   173     0.239089   1.352693   0.624931                  \n",
      "   174     0.238978   1.358981   0.622161                  \n",
      "   175     0.231558   1.366471   0.622161                  \n",
      "   176     0.230858   1.341431   0.628255                  \n",
      "   177     0.226194   1.364621   0.633241                  \n",
      "   178     0.221751   1.3716     0.632687                 \n",
      "   179     0.22343    1.39275    0.631579                  \n",
      "   180     0.220511   1.365274   0.628255                  \n",
      "   181     0.222346   1.379817   0.629363                  \n",
      "   182     0.216933   1.401028   0.631025                  \n",
      "   183     0.21461    1.39627    0.627147                  \n",
      "   184     0.210958   1.391266   0.633241                  \n",
      "   185     0.208316   1.387296   0.634903                  \n",
      "   186     0.215027   1.401902   0.628255                  \n",
      "   187     0.222565   1.36695    0.627147                  \n",
      "   188     0.223534   1.381898   0.625485                  \n",
      "   189     0.221943   1.385713   0.629363                  \n",
      "   190     0.217706   1.376344   0.633241                  \n",
      "   191     0.220126   1.389893   0.627147                  \n",
      "   192     0.214783   1.390749   0.626039                  \n",
      "   193     0.216017   1.363432   0.628255                  \n",
      "   194     0.219059   1.369898   0.631579                  \n",
      "   195     0.21837    1.39516    0.629363                  \n",
      "   196     0.210201   1.390941   0.627147                  \n",
      "   197     0.212265   1.394061   0.626593                  \n",
      "   198     0.214218   1.400627   0.622161                  \n",
      "   199     0.209921   1.388107   0.626039                  \n",
      "   200     0.223617   1.379494   0.632687                  \n",
      "   201     0.214866   1.363392   0.627147                  \n",
      "   202     0.215056   1.369625   0.631579                  \n",
      "   203     0.215341   1.378672   0.629917                  \n",
      "   204     0.213224   1.365062   0.623269                  \n",
      "   205     0.213804   1.372205   0.631579                  \n",
      "   206     0.212402   1.364232   0.623269                  \n",
      "   207     0.225819   1.360246   0.629917                  \n",
      "   208     0.222209   1.343176   0.630471                  \n",
      "   209     0.21671    1.365831   0.626593                  \n",
      "   210     0.213564   1.387869   0.627147                  \n",
      "   211     0.208461   1.404104   0.624931                  \n",
      "   212     0.208051   1.377023   0.629917                  \n",
      "   213     0.210818   1.409803   0.621053                  \n",
      "   214     0.213697   1.404837   0.623269                  \n",
      "   215     0.214374   1.389171   0.624377                  \n",
      "   216     0.211689   1.399184   0.626593                  \n",
      "   217     0.209819   1.384863   0.630471                  \n",
      "   218     0.20803    1.407576   0.628255                  \n",
      "   219     0.207714   1.417983   0.628255                  \n",
      "   220     0.207506   1.416582   0.629917                  \n",
      "   221     0.208546   1.404098   0.628809                  \n",
      "   222     0.204367   1.42905    0.631025                  \n",
      "   223     0.204993   1.427653   0.633795                  \n",
      "   224     0.202505   1.42244    0.631579                  \n",
      "   225     0.203526   1.420986   0.627701                  \n",
      "   226     0.20659    1.400556   0.627147                  \n",
      "   227     0.201338   1.386614   0.626039                  \n",
      "   228     0.203417   1.398455   0.627701                  \n",
      "   229     0.201792   1.416108   0.624931                  \n",
      "   230     0.205244   1.400461   0.630471                  \n",
      "   231     0.203354   1.424843   0.631579                  \n",
      "   232     0.208112   1.40896    0.629363                  \n",
      "   233     0.210339   1.408836   0.629363                  \n",
      "   234     0.207186   1.410291   0.629917                  \n",
      "   235     0.209896   1.418625   0.624931                  \n",
      "   236     0.214589   1.420275   0.631579                  \n",
      "   237     0.207378   1.408789   0.629363                  \n",
      "   238     0.207459   1.419128   0.636565                  \n",
      "   239     0.211912   1.422633   0.628809                  \n",
      "   240     0.209555   1.4043     0.636011                  \n",
      "   241     0.209047   1.385763   0.632687                  \n",
      "   242     0.206276   1.407313   0.632133                  \n",
      "   243     0.204057   1.417368   0.627147                  \n",
      "   244     0.204336   1.423832   0.636011                  \n",
      "   245     0.209943   1.415757   0.626039                  \n",
      "   246     0.214489   1.420343   0.626593                  \n",
      "   247     0.207691   1.398513   0.633795                  \n",
      "   248     0.20777    1.445075   0.629363                  \n",
      "   249     0.206589   1.425854   0.628809                  \n",
      "   250     0.204507   1.434428   0.628809                  \n",
      "   251     0.205666   1.413582   0.628809                  \n",
      "   252     0.208496   1.42588    0.632133                  \n",
      "   253     0.207126   1.427412   0.627701                  \n",
      "   254     0.204739   1.417286   0.626593                  \n",
      "   255     0.20424    1.443097   0.627147                  \n",
      "   256     0.207734   1.425152   0.630471                  \n",
      "   257     0.205396   1.427989   0.631025                  \n",
      "   258     0.20767    1.439982   0.618283                  \n",
      "   259     0.20726    1.429035   0.624377                  \n",
      "   260     0.211187   1.439927   0.623823                  \n",
      "   261     0.212206   1.441915   0.628255                  \n",
      "   262     0.217815   1.432185   0.627147                  \n",
      "   263     0.218603   1.429537   0.629917                  \n",
      "   264     0.216476   1.421616   0.631579                  \n",
      "   265     0.211429   1.411472   0.630471                  \n",
      "   266     0.202596   1.413431   0.634903                  \n",
      "   267     0.20403    1.429648   0.635457                  \n",
      "   268     0.207897   1.423245   0.628255                  \n",
      "   269     0.206498   1.427162   0.629363                  \n",
      "   270     0.208388   1.439991   0.631025                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   271     0.204671   1.442157   0.629363                  \n",
      "   272     0.200225   1.444499   0.630471                  \n",
      "   273     0.198683   1.456312   0.633241                  \n",
      "   274     0.196912   1.446197   0.633795                  \n",
      "   275     0.199911   1.446832   0.635457                  \n",
      "   276     0.197713   1.436845   0.630471                  \n",
      "   277     0.202727   1.443118   0.637119                  \n",
      "   278     0.205802   1.431847   0.636565                  \n",
      "   279     0.207309   1.455286   0.631579                  \n",
      "   280     0.204471   1.436892   0.629917                  \n",
      "   281     0.200017   1.41081    0.632133                  \n",
      "   282     0.197642   1.436425   0.629363                  \n",
      "   283     0.198436   1.426939   0.629917                  \n",
      "   284     0.195619   1.448906   0.626593                  \n",
      "   285     0.204291   1.428471   0.628809                  \n",
      "   286     0.20108    1.430119   0.629363                  \n",
      "   287     0.20273    1.432481   0.632687                  \n",
      "   288     0.210707   1.420751   0.633795                 \n",
      "   289     0.206135   1.40015    0.634349                  \n",
      "   290     0.197593   1.404064   0.636565                  \n",
      "   291     0.202736   1.416891   0.631579                  \n",
      "   292     0.206846   1.407664   0.634903                  \n",
      "   293     0.203415   1.405142   0.635457                  \n",
      "   294     0.208172   1.42603    0.630471                  \n",
      "   295     0.206298   1.419186   0.635457                  \n",
      "   296     0.204549   1.426739   0.622161                  \n",
      "   297     0.200199   1.436724   0.626593                  \n",
      "   298     0.195599   1.425625   0.629917                  \n",
      "   299     0.204549   1.415078   0.632133                  \n",
      "   300     0.201382   1.406067   0.629363                  \n",
      "   301     0.197058   1.423628   0.634349                  \n",
      "   302     0.196464   1.416538   0.631579                  \n",
      "   303     0.196047   1.43635    0.624931                  \n",
      "   304     0.203668   1.432095   0.629917                  \n",
      "   305     0.203514   1.417551   0.635457                  \n",
      "   306     0.199296   1.426729   0.629917                  \n",
      "   307     0.209218   1.432455   0.633241                  \n",
      "   308     0.21001    1.41922    0.627701                  \n",
      "   309     0.205425   1.419727   0.630471                  \n",
      "   310     0.202565   1.410667   0.626039                  \n",
      "   311     0.197167   1.421517   0.633241                  \n",
      "   312     0.199743   1.428714   0.632133                  \n",
      "   313     0.198141   1.426779   0.630471                  \n",
      "   314     0.196085   1.43104    0.628809                  \n",
      "   315     0.201481   1.427532   0.624377                  \n",
      "   316     0.198133   1.425068   0.630471                  \n",
      "   317     0.190927   1.424313   0.631025                  \n",
      "   318     0.192281   1.437885   0.635457                  \n",
      "   319     0.189112   1.444744   0.629363                  \n",
      "   320     0.190405   1.435174   0.632687                  \n",
      "   321     0.195241   1.441342   0.638781                  \n",
      "   322     0.195107   1.435851   0.637119                  \n",
      "   323     0.193662   1.438207   0.633241                  \n",
      "   324     0.197738   1.456172   0.634349                  \n",
      "   325     0.205226   1.457475   0.627147                  \n",
      "   326     0.196841   1.430247   0.634349                  \n",
      "   327     0.192007   1.435286   0.629917                  \n",
      "   328     0.190357   1.473874   0.632133                  \n",
      "   329     0.192866   1.448167   0.631579                  \n",
      "   330     0.188862   1.462647   0.628809                  \n",
      "   331     0.192804   1.452857   0.633241                  \n",
      "   332     0.18728    1.428722   0.633795                  \n",
      "   333     0.195489   1.456891   0.633241                  \n",
      "   334     0.198296   1.436633   0.634349                  \n",
      "   335     0.199033   1.428757   0.640997                  \n",
      "   336     0.196478   1.433054   0.632133                  \n",
      "   337     0.192636   1.429709   0.631025                  \n",
      "   338     0.189323   1.446869   0.637119                  \n",
      "   339     0.193345   1.442574   0.633241                  \n",
      "   340     0.19503    1.451164   0.630471                 \n",
      "   341     0.194462   1.446826   0.634903                  \n",
      "   342     0.191153   1.457762   0.634903                  \n",
      "   343     0.19112    1.442531   0.631025                  \n",
      "   344     0.189188   1.457069   0.633795                  \n",
      "   345     0.192786   1.450189   0.632687                  \n",
      "   346     0.19085    1.454724   0.629917                  \n",
      "   347     0.190275   1.465873   0.631025                  \n",
      "   348     0.185022   1.456677   0.629363                  \n",
      "   349     0.184619   1.452515   0.631025                  \n",
      "   350     0.181912   1.468948   0.630471                  \n",
      "   351     0.179228   1.477316   0.629363                  \n",
      "   352     0.185253   1.479092   0.631025                  \n",
      "   353     0.185409   1.480098   0.631025                  \n",
      "   354     0.187217   1.487165   0.631579                  \n",
      "   355     0.181499   1.469019   0.628255                  \n",
      "   356     0.183505   1.470509   0.629917                  \n",
      "   357     0.180633   1.469745   0.628255                  \n",
      "   358     0.178805   1.476606   0.625485                  \n",
      "   359     0.187268   1.466729   0.629363                  \n",
      "   360     0.198588   1.485371   0.633795                  \n",
      "   361     0.198806   1.482871   0.631579                  \n",
      "   362     0.194117   1.485811   0.628809                  \n",
      "   363     0.19168    1.480799   0.626593                  \n",
      "   364     0.192002   1.466795   0.629917                  \n",
      "   365     0.188648   1.479429   0.629363                  \n",
      "   366     0.184645   1.471647   0.629363                  \n",
      "   367     0.183017   1.468033   0.637119                  \n",
      "   368     0.183542   1.480655   0.632687                  \n",
      "   369     0.18367    1.476684   0.624931                  \n",
      "   370     0.184081   1.480682   0.626039                  \n",
      "   371     0.180962   1.465343   0.627147                  \n",
      "   372     0.180436   1.476373   0.627701                  \n",
      "   373     0.183472   1.467217   0.627701                  \n",
      "   374     0.182822   1.487922   0.621053                  \n",
      "   375     0.185053   1.459058   0.631025                  \n",
      "   376     0.182611   1.458934   0.626039                  \n",
      "   377     0.197876   1.493631   0.625485                  \n",
      "   378     0.187763   1.473812   0.630471                  \n",
      "   379     0.18594    1.476917   0.627147                  \n",
      "   380     0.192286   1.46977    0.632687                  \n",
      "   381     0.190146   1.439002   0.629363                  \n",
      "   382     0.198069   1.474323   0.633241                  \n",
      "   383     0.193159   1.482628   0.625485                  \n",
      "   384     0.192497   1.480459   0.628809                  \n",
      "   385     0.186997   1.492021   0.621053                  \n",
      "   386     0.193371   1.482792   0.629363                  \n",
      "   387     0.193232   1.454586   0.628255                  \n",
      "   388     0.191803   1.454548   0.627147                  \n",
      "   389     0.189733   1.470855   0.627701                  \n",
      "   390     0.183298   1.467396   0.629363                  \n",
      "   391     0.181938   1.475165   0.628255                  \n",
      "   392     0.186623   1.464027   0.629917                  \n",
      "   393     0.184041   1.455689   0.629363                  \n",
      "   394     0.185796   1.458459   0.627147                  \n",
      "   395     0.190897   1.449951   0.628255                  \n",
      "   396     0.196142   1.44409    0.626039                 \n",
      "   397     0.194587   1.423133   0.629363                  \n",
      "   398     0.19432    1.444115   0.631579                  \n",
      "   399     0.191512   1.433293   0.631579                  \n",
      "   400     0.189696   1.43747    0.626039                  \n",
      "   401     0.189829   1.456834   0.627701                  \n",
      "   402     0.190511   1.435634   0.632133                  \n",
      "   403     0.184759   1.441466   0.634349                  \n",
      "   404     0.18102    1.443442   0.633795                  \n",
      "   405     0.178653   1.454941   0.631025                  \n",
      "   406     0.178677   1.469305   0.628809                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   407     0.186652   1.476609   0.624377                  \n",
      "   408     0.191644   1.459994   0.627701                  \n",
      "   409     0.198321   1.477981   0.626039                  \n",
      "   410     0.195368   1.455788   0.628809                  \n",
      "   411     0.19585    1.450111   0.629917                  \n",
      "   412     0.20204    1.439534   0.632133                  \n",
      "   413     0.193437   1.428961   0.632133                  \n",
      "   414     0.189791   1.443999   0.634349                  \n",
      "   415     0.186531   1.448776   0.630471                  \n",
      "   416     0.181281   1.445668   0.628809                  \n",
      "   417     0.18637    1.454877   0.627147                  \n",
      "   418     0.19278    1.439291   0.631579                  \n",
      "   419     0.187961   1.455486   0.629363                  \n",
      "   420     0.188818   1.44979    0.623823                  \n",
      "   421     0.189784   1.482472   0.627701                  \n",
      "   422     0.184415   1.487451   0.629917                  \n",
      "   423     0.188136   1.471533   0.632133                  \n",
      "   424     0.188545   1.460301   0.633795                  \n",
      "   425     0.185436   1.461204   0.637119                  \n",
      "   426     0.180275   1.476304   0.631025                 \n",
      "   427     0.184209   1.474448   0.638781                  \n",
      "   428     0.18106    1.474363   0.631025                  \n",
      "   429     0.17979    1.479914   0.629363                  \n",
      "   430     0.183865   1.498885   0.627701                  \n",
      "   431     0.178896   1.486939   0.632687                  \n",
      "   432     0.175551   1.497237   0.631579                  \n",
      "   433     0.182578   1.503272   0.632133                  \n",
      "   434     0.179346   1.495899   0.632687                  \n",
      "   435     0.183883   1.481314   0.632687                  \n",
      "   436     0.184842   1.492011   0.626039                  \n",
      "   437     0.190235   1.50067    0.631025                  \n",
      "   438     0.189184   1.473198   0.631025                  \n",
      "   439     0.189354   1.460986   0.629917                  \n",
      "   440     0.181548   1.457623   0.631579                  \n",
      "   441     0.178761   1.481869   0.635457                  \n",
      "   442     0.179477   1.474404   0.633795                  \n",
      "   443     0.177528   1.480639   0.633241                  \n",
      "   444     0.181948   1.483357   0.633795                  \n",
      "   445     0.183142   1.493707   0.632133                  \n",
      "   446     0.177764   1.495954   0.631579                  \n",
      "   447     0.183105   1.494053   0.633241                  \n",
      "   448     0.191714   1.515807   0.632687                  \n",
      "   449     0.183948   1.461592   0.632687                  \n",
      "   450     0.183402   1.467503   0.632133                  \n",
      "   451     0.182831   1.492343   0.626039                  \n",
      "   452     0.185392   1.494838   0.626039                 \n",
      "   453     0.184463   1.478483   0.628809                  \n",
      "   454     0.185477   1.501872   0.628255                  \n",
      "   455     0.180301   1.497819   0.621053                  \n",
      "   456     0.180396   1.495514   0.624931                  \n",
      "   457     0.177902   1.489644   0.627147                  \n",
      "   458     0.174465   1.494419   0.626039                  \n",
      "   459     0.17772    1.50651    0.628255                  \n",
      "   460     0.175539   1.506423   0.627701                  \n",
      "   461     0.179234   1.50787    0.622715                  \n",
      "   462     0.177752   1.478873   0.624377                  \n",
      "   463     0.175664   1.50181    0.629363                  \n",
      "   464     0.176148   1.500897   0.627701                  \n",
      "   465     0.175658   1.492101   0.625485                  \n",
      "   466     0.180996   1.510422   0.626039                  \n",
      "   467     0.184676   1.497956   0.626039                  \n",
      "   468     0.179367   1.4988     0.630471                  \n",
      "   469     0.179726   1.499821   0.630471                  \n",
      "   470     0.182872   1.506276   0.625485                  \n",
      "   471     0.18861    1.49192    0.628255                  \n",
      "   472     0.190427   1.483996   0.633795                  \n",
      "   473     0.187479   1.48227    0.629917                  \n",
      "   474     0.179021   1.480591   0.632133                  \n",
      "   475     0.176874   1.493005   0.637119                  \n",
      "   476     0.188353   1.523257   0.622715                  \n",
      "   477     0.185236   1.516726   0.628255                  \n",
      "   478     0.181494   1.508249   0.633241                  \n",
      "   479     0.174622   1.531706   0.634349                  \n",
      "   480     0.174854   1.502948   0.631579                  \n",
      "   481     0.171371   1.506394   0.631025                  \n",
      "   482     0.171969   1.506548   0.631025                  \n",
      "   483     0.1727     1.51328    0.627147                  \n",
      "   484     0.177782   1.509996   0.627701                  \n",
      "   485     0.177994   1.51899    0.628809                  \n",
      "   486     0.179381   1.518399   0.629917                  \n",
      "   487     0.175359   1.502735   0.622715                  \n",
      "   488     0.172977   1.514839   0.624931                  \n",
      "   489     0.174981   1.537713   0.629363                  \n",
      "   490     0.17981    1.518649   0.633795                  \n",
      "   491     0.175976   1.528315   0.630471                  \n",
      "   492     0.171259   1.531615   0.623269                  \n",
      "   493     0.176632   1.535477   0.629917                 \n",
      "   494     0.175426   1.526412   0.627701                  \n",
      "   495     0.174367   1.540137   0.628255                  \n",
      "   496     0.17164    1.510652   0.625485                  \n",
      "   497     0.170126   1.504774   0.627147                  \n",
      "   498     0.17624    1.538557   0.624377                  \n",
      "   499     0.172664   1.518965   0.629363                  \n",
      "   500     0.17146    1.506265   0.628255                  \n",
      "   501     0.174035   1.511234   0.629363                  \n",
      "   502     0.176242   1.523247   0.629917                  \n",
      "   503     0.177224   1.511922   0.627701                  \n",
      "   504     0.174983   1.516407   0.626593                  \n",
      "   505     0.172383   1.536344   0.628255                  \n",
      "   506     0.1718     1.528586   0.624931                  \n",
      "   507     0.177178   1.546061   0.628255                  \n",
      "   508     0.180289   1.543681   0.630471                  \n",
      "   509     0.178639   1.540541   0.622715                  \n",
      "   510     0.17637    1.539222   0.631579                  \n",
      "   511     0.178063   1.535957   0.627147                  \n",
      "   512     0.175603   1.538311   0.628809                  \n",
      "   513     0.17808    1.526773   0.624931                  \n",
      "   514     0.180572   1.540428   0.625485                  \n",
      "   515     0.183601   1.54939    0.628255                  \n",
      "   516     0.177086   1.559949   0.622161                  \n",
      "   517     0.175828   1.548198   0.628255                  \n",
      "   518     0.196966   1.534172   0.621053                  \n",
      "   519     0.193119   1.517481   0.628255                  \n",
      "   520     0.184761   1.509132   0.628255                  \n",
      "   521     0.183183   1.536487   0.628809                  \n",
      "   522     0.179835   1.510868   0.632687                  \n",
      "   523     0.179548   1.495482   0.632133                  \n",
      "   524     0.177631   1.51905    0.631025                  \n",
      "   525     0.173442   1.506273   0.627147                  \n",
      "   526     0.170695   1.510712   0.632687                  \n",
      "   527     0.169096   1.495149   0.633241                  \n",
      "   528     0.171745   1.514092   0.627701                  \n",
      "   529     0.177868   1.535964   0.631579                  \n",
      "   530     0.180822   1.542633   0.623823                  \n",
      "   531     0.179048   1.525863   0.632133                  \n",
      "   532     0.177336   1.526798   0.630471                  \n",
      "   533     0.177028   1.550191   0.626039                  \n",
      "   534     0.177765   1.531539   0.632133                  \n",
      "   535     0.190572   1.535351   0.634349                  \n",
      "   536     0.182928   1.54323    0.630471                  \n",
      "   537     0.177161   1.516448   0.632133                  \n",
      "   538     0.175387   1.541306   0.629363                  \n",
      "   539     0.176026   1.556507   0.628255                  \n",
      "   540     0.179033   1.55341    0.627701                  \n",
      "   541     0.179178   1.549392   0.631025                  \n",
      "   542     0.175907   1.550593   0.631579                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   543     0.172103   1.533415   0.629917                  \n",
      "   544     0.169136   1.532414   0.636565                  \n",
      "   545     0.176332   1.546879   0.636011                  \n",
      "   546     0.176954   1.544896   0.627701                  \n",
      "   547     0.187281   1.515236   0.628255                  \n",
      "   548     0.184526   1.526987   0.623269                  \n",
      "   549     0.187002   1.538341   0.628255                  \n",
      "   550     0.181099   1.507115   0.629917                  \n",
      "   551     0.181107   1.533753   0.631025                  \n",
      "   552     0.184806   1.52238    0.627147                  \n",
      "   553     0.180706   1.529159   0.625485                  \n",
      "   554     0.180871   1.524727   0.632133                  \n",
      "   555     0.17752    1.543963   0.631025                  \n",
      "   556     0.177613   1.521046   0.626593                  \n",
      "   557     0.178582   1.520682   0.626593                  \n",
      "   558     0.177148   1.524042   0.628809                  \n",
      "   559     0.175034   1.523258   0.628255                  \n",
      "   560     0.179628   1.52978    0.629363                  \n",
      "   561     0.180322   1.510125   0.633795                  \n",
      "   562     0.177202   1.524144   0.633795                  \n",
      "   563     0.180234   1.529522   0.636011                  \n",
      "   564     0.175933   1.535072   0.627147                  \n",
      "   565     0.173885   1.529906   0.631579                  \n",
      "   566     0.167055   1.544987   0.636011                  \n",
      "   567     0.174209   1.543107   0.633241                  \n",
      "   568     0.174243   1.531088   0.635457                  \n",
      "   569     0.175259   1.549043   0.635457                  \n",
      "   570     0.175272   1.527896   0.629917                  \n",
      "   571     0.179253   1.507191   0.634903                  \n",
      "   572     0.177405   1.504643   0.637119                  \n",
      "   573     0.176013   1.523708   0.632133                  \n",
      "   574     0.17822    1.543793   0.633241                  \n",
      "   575     0.181621   1.529483   0.636011                  \n",
      "   576     0.18005    1.519878   0.636565                  \n",
      "   577     0.182564   1.517366   0.627701                  \n",
      "   578     0.178209   1.53725    0.623269                  \n",
      "   579     0.175245   1.522767   0.630471                  \n",
      "   580     0.171142   1.543193   0.626039                  \n",
      "   581     0.169855   1.527404   0.629363                  \n",
      "   582     0.16845    1.542143   0.632687                  \n",
      "   583     0.172365   1.558535   0.630471                  \n",
      "   584     0.172225   1.533206   0.628255                  \n",
      "   585     0.169649   1.54867    0.628255                  \n",
      "   586     0.170458   1.541199   0.629917                  \n",
      "   587     0.170382   1.535937   0.628255                  \n",
      "   588     0.173206   1.532085   0.627701                  \n",
      "   589     0.181923   1.516462   0.630471                  \n",
      "   590     0.178259   1.518498   0.639889                  \n",
      "   591     0.179578   1.540217   0.617175                  \n",
      "   592     0.179574   1.503587   0.631025                  \n",
      "   593     0.172495   1.519055   0.628255                  \n",
      "   594     0.174984   1.515541   0.632133                  \n",
      "   595     0.180386   1.515866   0.631579                  \n",
      "   596     0.176138   1.512507   0.626593                  \n",
      "   597     0.174607   1.504069   0.626039                  \n",
      "   598     0.174978   1.51763    0.623269                  \n",
      "   599     0.170647   1.510718   0.626593                  \n",
      "   600     0.172343   1.520232   0.638227                  \n",
      "   601     0.169468   1.520523   0.629917                  \n",
      "   602     0.175303   1.535997   0.626593                  \n",
      "   603     0.176206   1.539451   0.633241                  \n",
      "   604     0.173936   1.520324   0.633795                  \n",
      "   605     0.169681   1.507915   0.636011                  \n",
      "   606     0.172018   1.521526   0.636011                  \n",
      "   607     0.182452   1.513438   0.633241                  \n",
      "   608     0.178642   1.52296    0.636565                  \n",
      "   609     0.172188   1.507507   0.636565                  \n",
      "   610     0.17537    1.524508   0.633795                  \n",
      "   611     0.178577   1.515998   0.632687                  \n",
      "   612     0.176797   1.528737   0.627147                  \n",
      "   613     0.180131   1.509869   0.635457                  \n",
      "   614     0.176896   1.522447   0.633241                  \n",
      "   615     0.173259   1.535997   0.631025                  \n",
      "   616     0.171304   1.542346   0.634349                  \n",
      "   617     0.175115   1.512247   0.638781                  \n",
      "   618     0.179031   1.490737   0.639889                  \n",
      "   619     0.176676   1.496754   0.636011                  \n",
      "   620     0.172326   1.514155   0.633795                  \n",
      "   621     0.181869   1.50898    0.631579                  \n",
      "   622     0.177553   1.499059   0.632133                  \n",
      "   623     0.175088   1.5045     0.634903                  \n",
      "   624     0.1721     1.509135   0.631579                  \n",
      "   625     0.171616   1.488151   0.628809                  \n",
      "   626     0.173412   1.526823   0.630471                  \n",
      "   627     0.179614   1.523603   0.631025                  \n",
      "   628     0.178323   1.534966   0.635457                  \n",
      "   629     0.18209    1.516679   0.629917                  \n",
      "   630     0.17915    1.52093    0.628809                  \n",
      "   631     0.175306   1.522223   0.628255                  \n",
      "   632     0.176183   1.531407   0.631579                  \n",
      "   633     0.181338   1.538828   0.634349                  \n",
      "   634     0.1753     1.525708   0.629917                  \n",
      "   635     0.178838   1.544351   0.623823                  \n",
      "   636     0.176879   1.525929   0.629363                  \n",
      "   637     0.179904   1.527422   0.628809                  \n",
      "   638     0.182437   1.530083   0.632133                  \n",
      "   639     0.180073   1.531124   0.636011                  \n",
      "   640     0.174161   1.544916   0.632133                  \n",
      "   641     0.176703   1.547393   0.627701                  \n",
      "   642     0.179187   1.558398   0.624931                  \n",
      "   643     0.180701   1.553953   0.627701                  \n",
      "   644     0.176448   1.540665   0.631579                  \n",
      "   645     0.179333   1.540678   0.631025                  \n",
      "   646     0.177219   1.53051    0.634903                  \n",
      "   647     0.173256   1.540946   0.624377                  \n",
      "   648     0.17412    1.540704   0.626039                  \n",
      "   649     0.17126    1.523646   0.625485                  \n",
      "   650     0.178267   1.543552   0.632687                  \n",
      "   651     0.183797   1.552391   0.625485                  \n",
      "   652     0.182229   1.535444   0.634349                  \n",
      "   653     0.181543   1.516488   0.637673                  \n",
      "   654     0.175715   1.528171   0.632687                  \n",
      "   655     0.170986   1.532945   0.636565                  \n",
      "   656     0.171712   1.534393   0.632133                  \n",
      "   657     0.165795   1.553902   0.632133                  \n",
      "   658     0.169903   1.552957   0.628255                  \n",
      "   659     0.171705   1.546523   0.627701                  \n",
      "   660     0.171761   1.558895   0.634903                  \n",
      "   661     0.175984   1.546395   0.638227                  \n",
      "   662     0.173631   1.541746   0.637119                  \n",
      "   663     0.185749   1.535194   0.628255                  \n",
      "   664     0.178397   1.529286   0.631579                  \n",
      "   665     0.174877   1.51058    0.632687                  \n",
      "   666     0.169954   1.53235    0.636011                  \n",
      "   667     0.171144   1.527387   0.632687                  \n",
      "   668     0.172048   1.542614   0.632133                  \n",
      "   669     0.177767   1.545047   0.629917                  \n",
      "   670     0.174988   1.53666    0.631025                  \n",
      "   671     0.169949   1.532812   0.627701                  \n",
      "   672     0.170827   1.540019   0.633241                  \n",
      "   673     0.168588   1.539308   0.632133                  \n",
      "   674     0.168105   1.548884   0.628809                  \n",
      "   675     0.169776   1.57405    0.631025                  \n",
      "   676     0.172027   1.560551   0.631579                  \n",
      "   677     0.171296   1.559369   0.633241                  \n",
      "   678     0.177946   1.563458   0.629917                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   679     0.171401   1.569273   0.628255                  \n",
      "   680     0.172965   1.589803   0.633241                  \n",
      "   681     0.181621   1.589101   0.630471                  \n",
      "   682     0.175584   1.563127   0.630471                  \n",
      "   683     0.175082   1.569259   0.631579                  \n",
      "   684     0.171239   1.567521   0.630471                  \n",
      "   685     0.175409   1.572492   0.624931                  \n",
      "   686     0.174811   1.546174   0.632133                  \n",
      "   687     0.172872   1.562157   0.631579                  \n",
      "   688     0.174294   1.570578   0.634349                  \n",
      "   689     0.173646   1.556275   0.633241                  \n",
      "   690     0.170962   1.556435   0.634903                  \n",
      "   691     0.167544   1.552072   0.631579                  \n",
      "   692     0.167431   1.554529   0.627147                  \n",
      "   693     0.16976    1.577305   0.626039                  \n",
      "   694     0.173632   1.562938   0.627147                  \n",
      "   695     0.174178   1.563484   0.627147                  \n",
      "   696     0.17427    1.574246   0.629363                  \n",
      "   697     0.171469   1.562752   0.633241                  \n",
      "   698     0.166637   1.565924   0.634349                  \n",
      "   699     0.166737   1.580243   0.632687                  \n",
      "   700     0.163921   1.55737    0.637673                  \n",
      "   701     0.166834   1.555357   0.632687                  \n",
      "   702     0.172113   1.563112   0.633795                  \n",
      "   703     0.169904   1.557626   0.634903                  \n",
      "   704     0.169369   1.577735   0.630471                  \n",
      "   705     0.165803   1.568644   0.631579                  \n",
      "   706     0.173556   1.578708   0.629917                  \n",
      "   707     0.172611   1.557099   0.632687                  \n",
      "   708     0.171875   1.577593   0.636011                  \n",
      "   709     0.172603   1.558758   0.634349                  \n",
      "   710     0.168258   1.550254   0.632687                 \n",
      "   711     0.165678   1.544041   0.630471                  \n",
      "   712     0.1711     1.560528   0.627147                  \n",
      "   713     0.171498   1.571455   0.629917                  \n",
      "   714     0.171175   1.568896   0.622715                  \n",
      "   715     0.172249   1.559308   0.628255                  \n",
      "   716     0.170992   1.584743   0.626039                  \n",
      "   717     0.167042   1.569088   0.632133                  \n",
      "   718     0.168704   1.579408   0.624931                  \n",
      "   719     0.170265   1.574552   0.626039                  \n",
      "   720     0.170992   1.56021    0.628809                  \n",
      "   721     0.170352   1.561067   0.631025                  \n",
      "   722     0.166998   1.564145   0.631025                  \n",
      "   723     0.169429   1.575761   0.629363                  \n",
      "   724     0.170925   1.585811   0.626593                  \n",
      "   725     0.17098    1.5823     0.633795                  \n",
      "   726     0.169281   1.583553   0.629917                  \n",
      "   727     0.165183   1.586417   0.632687                  \n",
      "   728     0.164574   1.591986   0.629363                  \n",
      "   729     0.16516    1.587954   0.631579                  \n",
      "   730     0.170865   1.56785    0.631579                  \n",
      "   731     0.171252   1.582607   0.633795                 \n",
      "   732     0.166715   1.575692   0.632133                  \n",
      "   733     0.169248   1.588155   0.631025                  \n",
      "   734     0.165992   1.575978   0.630471                  \n",
      "   735     0.166691   1.582551   0.630471                  \n",
      "   736     0.168152   1.558776   0.629917                  \n",
      "   737     0.169773   1.573724   0.629363                  \n",
      "   738     0.171782   1.56409    0.630471                  \n",
      "   739     0.167613   1.577537   0.627701                  \n",
      "   740     0.173505   1.577231   0.629917                  \n",
      "   741     0.171524   1.56574    0.628255                  \n",
      "   742     0.172109   1.550701   0.629363                  \n",
      "   743     0.178658   1.561801   0.631025                  \n",
      "   744     0.1768     1.556028   0.627147                  \n",
      "   745     0.174033   1.538499   0.631579                  \n",
      "   746     0.172286   1.549156   0.626593                  \n",
      "   747     0.17427    1.559772   0.623823                  \n",
      "   748     0.172772   1.572318   0.631025                  \n",
      "   749     0.169027   1.577396   0.627701                  \n",
      "   750     0.167655   1.551054   0.629917                  \n",
      "   751     0.162948   1.58288    0.628809                  \n",
      "   752     0.164025   1.579591   0.628809                 \n",
      "   753     0.164119   1.599656   0.622715                  \n",
      "   754     0.165296   1.578915   0.624377                  \n",
      "   755     0.164083   1.556991   0.626039                  \n",
      "   756     0.179958   1.59559    0.627147                  \n",
      "   757     0.177131   1.567421   0.627147                  \n",
      "   758     0.179629   1.580026   0.625485                  \n",
      "   759     0.185598   1.556429   0.629363                  \n",
      "   760     0.18043    1.565215   0.626593                  \n",
      "   761     0.171278   1.562324   0.625485                  \n",
      "   762     0.164602   1.557894   0.627701                  \n",
      "   763     0.168891   1.580868   0.629363                  \n",
      "   764     0.176286   1.593616   0.624931                  \n",
      "   765     0.168585   1.599478   0.629363                  \n",
      "   766     0.166025   1.59009    0.628809                  \n",
      "   767     0.162863   1.586918   0.628809                  \n",
      "   768     0.164222   1.581231   0.629363                  \n",
      "   769     0.160345   1.576136   0.626593                  \n",
      "   770     0.161849   1.606337   0.634349                  \n",
      "   771     0.165036   1.600268   0.630471                  \n",
      "   772     0.165067   1.601702   0.633795                  \n",
      "   773     0.168799   1.607766   0.632687                  \n",
      "   774     0.183247   1.579974   0.627147                  \n",
      "   775     0.173473   1.583892   0.623823                  \n",
      "   776     0.170881   1.580453   0.632687                  \n",
      "   777     0.168548   1.593336   0.630471                  \n",
      "   778     0.16474    1.586728   0.635457                  \n",
      "   779     0.166181   1.597529   0.629917                  \n",
      "   780     0.166929   1.575167   0.637119                  \n",
      "   781     0.164103   1.603673   0.634903                  \n",
      "   782     0.172187   1.605368   0.629917                  \n",
      "   783     0.1787     1.579203   0.634903                  \n",
      "   784     0.171376   1.589332   0.639889                  \n",
      "   785     0.168207   1.584563   0.635457                  \n",
      "   786     0.167392   1.57965    0.631579                  \n",
      "   787     0.167433   1.587069   0.637673                  \n",
      "   788     0.164678   1.58287    0.636565                  \n",
      "   789     0.168084   1.619257   0.631579                 \n",
      "   790     0.170357   1.572501   0.630471                  \n",
      "   791     0.172262   1.563768   0.631579                  \n",
      "   792     0.167889   1.56917    0.634903                  \n",
      "   793     0.169203   1.578351   0.636565                  \n",
      "   794     0.163129   1.584682   0.632133                  \n",
      "   795     0.16115    1.587443   0.633241                  \n",
      "   796     0.161259   1.581601   0.629363                  \n",
      "   797     0.163802   1.594436   0.629363                  \n",
      "   798     0.165689   1.592507   0.634349                  \n",
      "   799     0.171119   1.579792   0.632687                  \n",
      "   800     0.168344   1.566552   0.633241                 \n",
      "   801     0.174518   1.584733   0.633241                  \n",
      "   802     0.172415   1.590506   0.628809                  \n",
      "   803     0.169327   1.587824   0.627147                  \n",
      "   804     0.170446   1.589748   0.631025                  \n",
      "   805     0.174531   1.578089   0.630471                  \n",
      "   806     0.171081   1.565353   0.631579                  \n",
      "   807     0.171433   1.581795   0.629917                  \n",
      "   808     0.17787    1.591394   0.629363                  \n",
      "   809     0.177582   1.570552   0.634903                  \n",
      "   810     0.173551   1.569392   0.636565                  \n",
      "   811     0.173118   1.560272   0.633795                  \n",
      "   812     0.167596   1.575988   0.632133                  \n",
      "   813     0.175475   1.57376    0.632687                  \n",
      "   814     0.169626   1.552528   0.637673                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   815     0.16807    1.553762   0.628809                  \n",
      "   816     0.167237   1.550717   0.632687                  \n",
      "   817     0.172972   1.564334   0.629363                  \n",
      "   818     0.177976   1.582243   0.626039                  \n",
      "   819     0.174252   1.567202   0.629917                  \n",
      "   820     0.170524   1.580621   0.631579                  \n",
      "   821     0.171508   1.593114   0.631025                  \n",
      "   822     0.171307   1.577918   0.627701                  \n",
      "   823     0.166217   1.575684   0.628255                  \n",
      "   824     0.163593   1.58926    0.633241                  \n",
      "   825     0.160815   1.595052   0.633241                  \n",
      "   826     0.1614     1.604824   0.631025                  \n",
      "   827     0.161169   1.605148   0.626039                  \n",
      "   828     0.168707   1.601373   0.630471                  \n",
      "   829     0.174535   1.591766   0.632687                  \n",
      "   830     0.16696    1.615135   0.629917                  \n",
      "   831     0.171231   1.619288   0.628255                  \n",
      "   832     0.173769   1.605071   0.632133                  \n",
      "   833     0.169167   1.622183   0.627701                  \n",
      "   834     0.165784   1.618894   0.633795                  \n",
      "   835     0.163838   1.636082   0.633795                  \n",
      "   836     0.163463   1.629474   0.634349                  \n",
      "   837     0.159897   1.614677   0.634903                  \n",
      "   838     0.156336   1.641607   0.634349                  \n",
      "   839     0.162783   1.632578   0.629363                  \n",
      "   840     0.162432   1.651093   0.630471                  \n",
      "   841     0.160912   1.620141   0.632687                  \n",
      "   842     0.16588    1.635621   0.629363                  \n",
      "   843     0.169963   1.61106    0.634349                  \n",
      "   844     0.177032   1.617353   0.633241                  \n",
      "   845     0.174013   1.627118   0.629363                  \n",
      "   846     0.168326   1.61969    0.629917                  \n",
      "   847     0.165291   1.620629   0.631025                  \n",
      "   848     0.164638   1.63709    0.632133                  \n",
      "   849     0.165502   1.635934   0.628809                  \n",
      "   850     0.163472   1.634349   0.631025                  \n",
      "   851     0.165736   1.650565   0.628809                  \n",
      "   852     0.163778   1.621295   0.632687                  \n",
      "   853     0.156608   1.652514   0.631025                  \n",
      "   854     0.163826   1.641503   0.638227                  \n",
      "   855     0.165583   1.646147   0.634903                  \n",
      "   856     0.166088   1.607605   0.636011                  \n",
      "   857     0.16362    1.639694   0.634903                  \n",
      "   858     0.160627   1.625132   0.631025                  \n",
      "   859     0.161874   1.635312   0.632133                  \n",
      "   860     0.161506   1.643452   0.630471                  \n",
      "   861     0.165384   1.643062   0.634903                  \n",
      "   862     0.162951   1.642902   0.631025                  \n",
      "   863     0.168006   1.630877   0.629363                  \n",
      "   864     0.170751   1.62934    0.631579                  \n",
      "   865     0.171492   1.613984   0.633241                  \n",
      "   866     0.163301   1.635867   0.638227                  \n",
      "   867     0.159247   1.622347   0.631579                  \n",
      "   868     0.157702   1.618881   0.634903                  \n",
      "   869     0.160656   1.623647   0.634349                  \n",
      "   870     0.159404   1.608829   0.629917                  \n",
      "   871     0.159891   1.602058   0.636011                  \n",
      "   872     0.159667   1.608917   0.632133                  \n",
      "   873     0.158801   1.613789   0.637673                  \n",
      "   874     0.162624   1.619277   0.633795                  \n",
      "   875     0.165361   1.616283   0.631025                  \n",
      "   876     0.165262   1.599126   0.633241                  \n",
      "   877     0.157745   1.611      0.632133                  \n",
      "   878     0.159081   1.630796   0.630471                  \n",
      "   879     0.162406   1.62896    0.626039                  \n",
      "   880     0.166573   1.638337   0.629917                  \n",
      "   881     0.171276   1.650813   0.629917                  \n",
      "   882     0.167988   1.591451   0.632133                  \n",
      "   883     0.169127   1.624804   0.628809                  \n",
      "   884     0.16779    1.626931   0.630471                  \n",
      "   885     0.165926   1.609189   0.628809                  \n",
      "   886     0.165218   1.60762    0.620499                  \n",
      "   887     0.164151   1.616348   0.624931                  \n",
      "   888     0.163368   1.611549   0.628255                  \n",
      "   889     0.164293   1.637451   0.621053                  \n",
      "   890     0.161403   1.606737   0.626039                  \n",
      "   891     0.160276   1.64569    0.624931                  \n",
      "   892     0.157597   1.624324   0.622715                  \n",
      "   893     0.159475   1.633946   0.633241                  \n",
      "   894     0.160368   1.645481   0.628809                  \n",
      "   895     0.158712   1.649853   0.628255                  \n",
      "   896     0.157927   1.645376   0.625485                  \n",
      "   897     0.162372   1.644543   0.626593                  \n",
      "   898     0.163706   1.64222    0.630471                  \n",
      "   899     0.167281   1.623363   0.629363                  \n",
      "   900     0.166426   1.636445   0.627147                  \n",
      "   901     0.162      1.629898   0.625485                  \n",
      "   902     0.165452   1.618016   0.631579                  \n",
      "   903     0.161757   1.622805   0.633795                  \n",
      "   904     0.161303   1.612979   0.631025                  \n",
      "   905     0.162349   1.625604   0.634349                 \n",
      "   906     0.157359   1.618251   0.627147                  \n",
      "   907     0.158945   1.628461   0.629917                  \n",
      "   908     0.159282   1.616499   0.627701                  \n",
      "   909     0.157737   1.64461    0.626039                  \n",
      "   910     0.157202   1.62777    0.626593                  \n",
      "   911     0.161701   1.62738    0.623823                  \n",
      "   912     0.160026   1.661302   0.626039                  \n",
      "   913     0.162224   1.652529   0.630471                  \n",
      "   914     0.162726   1.689733   0.629363                  \n",
      "   915     0.162749   1.62394    0.628255                  \n",
      "   916     0.163975   1.645942   0.622715                  \n",
      "   917     0.160599   1.64312    0.623823                  \n",
      "   918     0.163138   1.626679   0.623823                  \n",
      "   919     0.172571   1.596159   0.629917                  \n",
      "   920     0.168133   1.616692   0.630471                  \n",
      "   921     0.174379   1.627151   0.630471                  \n",
      "   922     0.170829   1.624155   0.628255                  \n",
      "   923     0.172172   1.610751   0.631025                  \n",
      "   924     0.168781   1.615999   0.632687                  \n",
      "   925     0.170969   1.615823   0.627147                  \n",
      "   926     0.169027   1.614642   0.626039                  \n",
      "   927     0.168961   1.602778   0.631579                  \n",
      "   928     0.165444   1.598802   0.630471                  \n",
      "   929     0.167347   1.600465   0.627147                  \n",
      "   930     0.170524   1.606725   0.624931                  \n",
      "   931     0.165324   1.610091   0.625485                  \n",
      "   932     0.164675   1.612212   0.625485                  \n",
      "   933     0.159071   1.605428   0.626593                  \n",
      "   934     0.156376   1.625248   0.630471                  \n",
      "   935     0.161798   1.625438   0.623269                  \n",
      "   936     0.160867   1.629898   0.624377                  \n",
      "   937     0.156299   1.613533   0.631025                  \n",
      "   938     0.159597   1.618422   0.633241                  \n",
      "   939     0.156743   1.620498   0.631025                  \n",
      "   940     0.155049   1.646158   0.631579                  \n",
      "   941     0.1533     1.655236   0.631025                  \n",
      "   942     0.163658   1.655716   0.632133                  \n",
      "   943     0.160829   1.63991    0.631025                  \n",
      "   944     0.159661   1.656098   0.632133                  \n",
      "   945     0.15975    1.641212   0.628809                  \n",
      "   946     0.158394   1.605776   0.628255                  \n",
      "   947     0.160016   1.621636   0.626593                  \n",
      "   948     0.160729   1.635618   0.628255                  \n",
      "   949     0.165769   1.650537   0.631025                  \n",
      "   950     0.162322   1.619336   0.626039                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   951     0.164137   1.635093   0.626039                  \n",
      "   952     0.158369   1.635247   0.623823                  \n",
      "   953     0.159229   1.658631   0.622161                  \n",
      "   954     0.158048   1.649389   0.628255                  \n",
      "   955     0.161305   1.661994   0.622715                  \n",
      "   956     0.161181   1.662558   0.623269                  \n",
      "   957     0.172034   1.633831   0.620499                  \n",
      "   958     0.164835   1.634691   0.626039                  \n",
      "   959     0.163382   1.628109   0.624377                  \n",
      "   960     0.168486   1.650287   0.624931                  \n",
      "   961     0.172281   1.624589   0.624931                  \n",
      "   962     0.177826   1.644527   0.616621                  \n",
      "   963     0.176694   1.605251   0.624931                  \n",
      "   964     0.182403   1.627471   0.626593                  \n",
      "   965     0.18197    1.618335   0.626039                  \n",
      "   966     0.178373   1.623744   0.623823                  \n",
      "   967     0.172894   1.601902   0.622161                  \n",
      "   968     0.167917   1.618722   0.627701                  \n",
      "   969     0.160788   1.621087   0.626039                  \n",
      "   970     0.159112   1.599918   0.622715                  \n",
      "   971     0.155945   1.607223   0.629917                  \n",
      "   972     0.161442   1.637248   0.627701                  \n",
      "   973     0.162567   1.609446   0.628809                  \n",
      "   974     0.160764   1.61058    0.627701                  \n",
      "   975     0.162782   1.636025   0.628255                  \n",
      "   976     0.158782   1.608948   0.628809                  \n",
      "   977     0.155293   1.627735   0.627701                  \n",
      "   978     0.158432   1.596975   0.623823                  \n",
      "   979     0.158934   1.617586   0.629917                  \n",
      "   980     0.159154   1.614889   0.627147                  \n",
      "   981     0.157825   1.620336   0.628255                  \n",
      "   982     0.158971   1.623524   0.626039                  \n",
      "   983     0.167109   1.635525   0.628809                  \n",
      "   984     0.175118   1.617585   0.622715                  \n",
      "   985     0.17411    1.613231   0.624931                  \n",
      "   986     0.169609   1.606979   0.621607                  \n",
      "   987     0.168042   1.62855    0.629363                  \n",
      "   988     0.16557    1.6248     0.626039                  \n",
      "   989     0.15923    1.595551   0.627701                  \n",
      "   990     0.159869   1.623744   0.626039                  \n",
      "   991     0.161664   1.616851   0.627147                  \n",
      "   992     0.166859   1.625786   0.626593                  \n",
      "   993     0.166926   1.602699   0.627147                  \n",
      "   994     0.169503   1.617556   0.629917                  \n",
      "   995     0.167547   1.592166   0.627147                  \n",
      "   996     0.167484   1.629376   0.629917                  \n",
      "   997     0.176786   1.616373   0.627701                  \n",
      "   998     0.174402   1.61768    0.628255                  \n",
      "   999     0.173679   1.608932   0.628255                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.60893]), 0.6282548445413647]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_csv = f'{PATH}5labels.csv'\n",
    "n = len(list(open(label_csv))) - 1\n",
    "val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1))\n",
    "data = get_data(sz, bs, val_idxs, label_csv)\n",
    "learn5 = ConvLearner.pretrained(arch, data, precompute=True, ps = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c099a5ae1a4872b31cdbefc10bc62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.80117    0.38425    0.854848  \n",
      "    1      0.769391   0.372719   0.854848                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit(1e-2, 1000)\n",
    "learn.precompute = False\n",
    "val_loss, val_acc = learn.fit(1e-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9  0.06 0.02 0.01 0.01]                  \n",
      " [0.13 0.81 0.03 0.02 0.01]\n",
      " [0.05 0.02 0.83 0.09 0.01]\n",
      " [0.02 0.03 0.09 0.84 0.02]\n",
      " [0.05 0.05 0.07 0.17 0.66]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('5class_86pctAcc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a600f3f897304bde8ede2d334f09e98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b816a2a0a014be69af9a32444e6a3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                \n",
      "    0      1.028125   0.923016   0.567442  \n",
      "    1      0.894474   0.830859   0.624186                \n",
      "    2      0.8178     0.798152   0.648372                 \n",
      "    3      0.755856   0.770429   0.683721                 \n",
      "    4      0.70618    0.804354   0.670698                 \n",
      "    5      0.665059   0.73825    0.695814                 \n",
      "    6      0.622761   0.728392   0.710698                 \n",
      "    7      0.58439    0.735244   0.715349                 \n",
      "    8      0.557876   0.718437   0.713488                 \n",
      "    9      0.533255   0.72237    0.712558                  \n",
      "    10     0.506495   0.70938    0.726512                  \n",
      "    11     0.481238   0.713204   0.724651                 \n",
      "    12     0.460287   0.722206   0.722791                  \n",
      "    13     0.442628   0.730683   0.729302                  \n",
      "    14     0.423683   0.729136   0.71907                  \n",
      "    15     0.405955   0.734955   0.732093                 \n",
      "    16     0.388056   0.727957   0.71907                   \n",
      "    17     0.372594   0.743316   0.733023                  \n",
      "    18     0.363901   0.773724   0.715349                  \n",
      "    19     0.349388   0.748711   0.732093                 \n",
      "    20     0.339257   0.75866    0.72                     \n",
      "    21     0.329082   0.782635   0.727442                 \n",
      "    22     0.320695   0.792562   0.734884                 \n",
      "    23     0.310937   0.78096    0.733953                 \n",
      "    24     0.308658   0.754004   0.726512                 \n",
      "    25     0.298512   0.787051   0.728372                 \n",
      "    26     0.288078   0.783682   0.726512                 \n",
      "    27     0.280454   0.806971   0.737674                 \n",
      "    28     0.27438    0.787767   0.730233                  \n",
      "    29     0.268077   0.826114   0.728372                  \n",
      "    30     0.263758   0.803133   0.729302                  \n",
      "    31     0.254976   0.804483   0.731163                 \n",
      "    32     0.259403   0.820704   0.733023                 \n",
      "    33     0.25998    0.840334   0.72186                  \n",
      "    34     0.251207   0.822581   0.731163                 \n",
      "    35     0.240832   0.838352   0.72                     \n",
      "    36     0.243424   0.861947   0.730233                 \n",
      "    37     0.240054   0.861504   0.738605                 \n",
      "    38     0.232798   0.87675    0.731163                 \n",
      "    39     0.232057   0.840977   0.731163                 \n",
      "    40     0.222698   0.842394   0.728372                  \n",
      "    41     0.218823   0.870994   0.72093                  \n",
      "    42     0.212642   0.875229   0.722791                  \n",
      "    43     0.208531   0.886288   0.731163                  \n",
      "    44     0.211573   0.86455    0.733953                  \n",
      "    45     0.208295   0.874499   0.725581                  \n",
      "    46     0.207467   0.884161   0.736744                  \n",
      "    47     0.208148   0.931022   0.728372                  \n",
      "    48     0.208548   0.911282   0.733023                 \n",
      "    49     0.200633   0.907897   0.726512                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e86a3006424a1e8748c3655c9aac08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.72571    0.256109   0.903256  \n"
     ]
    }
   ],
   "source": [
    "label_csv = f'{PATH}3labels.csv'\n",
    "n = len(list(open(label_csv))) - 1\n",
    "vacc =[]\n",
    "reps=5\n",
    "for rep in range(reps):\n",
    "    val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1))\n",
    "    data3 = get_data(sz, bs, val_idxs, label_csv)\n",
    "    learn3 = ConvLearner.pretrained(arch, data3, precompute=True, ps = 0.2)\n",
    "    #lrf=learn3.lr_find(1e-6,10)\n",
    "    #learn3.sched.plot()\n",
    "    learn3.fit(1e-2, 50)\n",
    "    learn3.precompute = False\n",
    "    val_loss, val_acc = learn3.fit(1e-2, 1)\n",
    "    vacc.append(val_acc)\n",
    "#lrf=learn3.lr_find(1e-9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9032558244328166\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(vacc))\n",
    "print(np.std(vacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92 0.05 0.04]                            \n",
      " [0.07 0.82 0.11]\n",
      " [0.06 0.09 0.85]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "log_preds,y = learn3.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data3.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca78f3104744a1593a961287723e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893d67b76703444aacb0261eb732e1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.06971    1.766299   0.426872  \n",
      "    1      1.760046   1.642587   0.462555                 \n",
      "    2      1.585652   1.551375   0.489868                 \n",
      "    3      1.461677   1.514532   0.494273                 \n",
      "    4      1.369123   1.460752   0.510132                 \n",
      "    5      1.292656   1.446941   0.521586                 \n",
      "    6      1.234018   1.461585   0.510573                 \n",
      "    7      1.178178   1.435854   0.518062                 \n",
      "    8      1.128714   1.417772   0.537445                 \n",
      "    9      1.085267   1.402355   0.534361                 \n",
      "    10     1.044509   1.408655   0.529956                 \n",
      "    11     1.01097    1.411556   0.542731                 \n",
      "    12     0.98047    1.404969   0.548899                  \n",
      "    13     0.953286   1.428946   0.536123                  \n",
      "    14     0.918258   1.406781   0.548018                  \n",
      "    15     0.903391   1.418691   0.545374                  \n",
      "    16     0.873654   1.402344   0.535242                  \n",
      "    17     0.846346   1.44181    0.543172                  \n",
      "    18     0.820649   1.41584    0.549339                  \n",
      "    19     0.801664   1.440347   0.542731                  \n",
      "    20     0.786111   1.472049   0.54978                   \n",
      "    21     0.761751   1.446298   0.548458                  \n",
      "    22     0.739097   1.46521    0.559912                  \n",
      "    23     0.734235   1.451391   0.547577                  \n",
      "    24     0.720924   1.486726   0.546696                  \n",
      "    25     0.710603   1.483989   0.542731                  \n",
      "    26     0.700382   1.481509   0.545374                  \n",
      "    27     0.685911   1.481879   0.549339                  \n",
      "    28     0.66559    1.504811   0.548018                  \n",
      "    29     0.65586    1.527525   0.550661                  \n",
      "    30     0.66138    1.50721    0.551982                  \n",
      "    31     0.632188   1.513967   0.547137                  \n",
      "    32     0.630107   1.53192    0.544493                  \n",
      "    33     0.624659   1.533719   0.544493                  \n",
      "    34     0.613175   1.549874   0.545374                  \n",
      "    35     0.60702    1.558175   0.555947                  \n",
      "    36     0.605211   1.55208    0.544493                  \n",
      "    37     0.59205    1.585254   0.547577                  \n",
      "    38     0.569773   1.589341   0.544053                  \n",
      "    39     0.575932   1.587905   0.54978                   \n",
      "    40     0.560602   1.602516   0.547137                  \n",
      "    41     0.560028   1.605869   0.546696                  \n",
      "    42     0.54758    1.608139   0.545374                  \n",
      "    43     0.542681   1.602745   0.546696                  \n",
      "    44     0.540412   1.639344   0.546256                  \n",
      "    45     0.53977    1.623428   0.540969                  \n",
      "    46     0.536108   1.623707   0.539207                  \n",
      "    47     0.529075   1.635691   0.543612                  \n",
      "    48     0.521391   1.630336   0.543612                  \n",
      "    49     0.516621   1.664624   0.542731                  \n",
      "    50     0.509189   1.639183   0.555507                  \n",
      "    51     0.505206   1.658616   0.540969                  \n",
      "    52     0.504395   1.655105   0.548899                  \n",
      "    53     0.499393   1.667712   0.551101                  \n",
      "    54     0.490294   1.653658   0.548018                  \n",
      "    55     0.492681   1.672402   0.547137                  \n",
      "    56     0.488236   1.696934   0.54185                   \n",
      "    57     0.477947   1.674588   0.553304                  \n",
      "    58     0.477522   1.723758   0.548018                  \n",
      "    59     0.474846   1.700364   0.543172                  \n",
      "    60     0.469659   1.70912    0.543612                  \n",
      "    61     0.470375   1.698063   0.54978                   \n",
      "    62     0.469792   1.695871   0.546256                  \n",
      "    63     0.468992   1.723706   0.543612                  \n",
      "    64     0.466212   1.704984   0.548018                  \n",
      "    65     0.454264   1.705746   0.543172                  \n",
      "    66     0.449229   1.727106   0.543172                  \n",
      "    67     0.450911   1.73747    0.543172                  \n",
      "    68     0.452221   1.71588    0.542291                  \n",
      "    69     0.454692   1.747539   0.544493                  \n",
      "    70     0.448627   1.732596   0.54185                   \n",
      "    71     0.444976   1.750985   0.544493                  \n",
      "    72     0.438476   1.741748   0.549339                  \n",
      "    73     0.443948   1.755004   0.547137                  \n",
      "    74     0.438028   1.756803   0.54978                   \n",
      "    75     0.437368   1.762235   0.546696                  \n",
      "    76     0.431466   1.741303   0.544934                  \n",
      "    77     0.427753   1.7536     0.534802                  \n",
      "    78     0.428512   1.75489    0.545815                  \n",
      "    79     0.432414   1.784924   0.544493                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b204ed6a7a7d4efaae6d7f3a1a79f1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      1.282391   0.594265   0.795595  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951e26671a424919bc471cbec5cca2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eca23351d6d493995febe8f06d11e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.061728   1.795786   0.426432  \n",
      "    1      1.755395   1.685698   0.46652                  \n",
      "    2      1.587559   1.635655   0.469604                 \n",
      "    3      1.474907   1.608113   0.48326                  \n",
      "    4      1.397089   1.596948   0.496035                 \n",
      "    5      1.319976   1.595817   0.49163                  \n",
      "    6      1.256806   1.592384   0.502203                 \n",
      "    7      1.202187   1.580602   0.505286                 \n",
      "    8      1.14933    1.595931   0.495595                 \n",
      "    9      1.100989   1.577221   0.507489                 \n",
      "    10     1.071483   1.594498   0.512335                 \n",
      "    11     1.028961   1.594366   0.515859                 \n",
      "    12     1.00204    1.600171   0.523348                  \n",
      "    13     0.978332   1.610342   0.514978                  \n",
      "    14     0.952719   1.603338   0.50793                   \n",
      "    15     0.930874   1.622043   0.513216                  \n",
      "    16     0.909669   1.663606   0.500441                  \n",
      "    17     0.883805   1.637263   0.512335                  \n",
      "    18     0.862293   1.64499    0.514097                  \n",
      "    19     0.841743   1.667442   0.5163                    \n",
      "    20     0.816683   1.645274   0.50793                   \n",
      "    21     0.799981   1.685241   0.520705                  \n",
      "    22     0.77874    1.725171   0.512775                  \n",
      "    23     0.763617   1.694097   0.515419                  \n",
      "    24     0.750709   1.726441   0.514097                  \n",
      "    25     0.734685   1.719345   0.511454                  \n",
      "    26     0.716673   1.742067   0.5163                    \n",
      "    27     0.709565   1.733996   0.509251                  \n",
      "    28     0.69806    1.775607   0.512335                  \n",
      "    29     0.681896   1.764547   0.50793                   \n",
      "    30     0.662728   1.813097   0.50837                   \n",
      "    31     0.645562   1.811009   0.504405                  \n",
      "    32     0.647513   1.809329   0.515859                  \n",
      "    33     0.64252    1.808285   0.511894                  \n",
      "    34     0.642586   1.839103   0.50837                   \n",
      "    35     0.624512   1.812078   0.511013                  \n",
      "    36     0.613543   1.849905   0.511894                  \n",
      "    37     0.597791   1.855732   0.517621                  \n",
      "    38     0.589227   1.869602   0.512335                  \n",
      "    39     0.586503   1.858515   0.514097                  \n",
      "    40     0.584281   1.894661   0.514537                  \n",
      "    41     0.586084   1.901776   0.50793                   \n",
      "    42     0.573175   1.888282   0.517181                  \n",
      "    43     0.561731   1.900159   0.518062                  \n",
      "    44     0.56215    1.952576   0.50837                   \n",
      "    45     0.560058   1.928683   0.508811                  \n",
      "    46     0.558343   1.935448   0.506167                  \n",
      "    47     0.55094    1.927667   0.512335                  \n",
      "    48     0.539866   1.941176   0.514537                  \n",
      "    49     0.533873   1.957075   0.509251                  \n",
      "    50     0.521228   1.947297   0.5163                    \n",
      "    51     0.524213   1.977869   0.514537                  \n",
      "    52     0.521053   1.980074   0.513656                  \n",
      "    53     0.521009   1.968345   0.517181                  \n",
      "    54     0.518534   1.976273   0.507048                  \n",
      "    55     0.516171   1.978394   0.503524                  \n",
      "    56     0.5099     2.003492   0.510132                  \n",
      "    57     0.494261   1.983652   0.512335                  \n",
      "    58     0.492744   1.975221   0.50793                   \n",
      "    59     0.495234   1.997851   0.512335                  \n",
      "    60     0.494209   1.994385   0.511013                  \n",
      "    61     0.479749   2.016289   0.511454                  \n",
      "    62     0.48568    2.016791   0.514097                  \n",
      "    63     0.471917   2.035946   0.505286                  \n",
      "    64     0.482547   2.037936   0.503084                  \n",
      "    65     0.473948   2.030733   0.514097                  \n",
      "    66     0.479574   2.037795   0.507048                  \n",
      "    67     0.468073   2.049316   0.507048                  \n",
      "    68     0.462301   2.045236   0.511013                  \n",
      "    69     0.464173   2.055695   0.511013                  \n",
      "    70     0.464704   2.074106   0.506608                  \n",
      "    71     0.4596     2.095504   0.503965                  \n",
      "    72     0.455432   2.07935    0.510573                  \n",
      "    73     0.458353   2.086725   0.505727                  \n",
      "    74     0.45133    2.083385   0.509251                  \n",
      "    75     0.449742   2.078767   0.511894                  \n",
      "    76     0.446073   2.064833   0.509692                  \n",
      "    77     0.438805   2.105868   0.506608                  \n",
      "    78     0.434975   2.097974   0.50837                   \n",
      "    79     0.442002   2.081725   0.503965                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75974e35a3c42f9829e98e625877e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      1.258289   0.672588   0.763436  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6454dc19fcc47ab92ce6248f472758d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2996b10c5f444989f01c9185fc35e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.081045   1.785898   0.44185   \n",
      "    1      1.760655   1.658731   0.457269                 \n",
      "    2      1.590812   1.602424   0.478414                 \n",
      "    3      1.473798   1.569339   0.481498                 \n",
      "    4      1.391232   1.537294   0.502203                 \n",
      "    5      1.314886   1.531841   0.5                      \n",
      "    6      1.252007   1.531006   0.511454                 \n",
      "    7      1.192314   1.507144   0.515419                 \n",
      "    8      1.143816   1.516268   0.519824                 \n",
      "    9      1.103477   1.502394   0.520264                 \n",
      "    10     1.061859   1.5105     0.519383                 \n",
      "    11     1.038099   1.514359   0.521586                 \n",
      "    12     1.000619   1.512512   0.531718                  \n",
      "    13     0.968713   1.52422    0.525551                  \n",
      "    14     0.944485   1.550964   0.523789                  \n",
      "    15     0.915713   1.53389    0.528194                  \n",
      "    16     0.898561   1.546568   0.525991                  \n",
      "    17     0.867666   1.567617   0.522907                  \n",
      "    18     0.843582   1.562708   0.524229                  \n",
      "    19     0.825431   1.552548   0.534361                  \n",
      "    20     0.80713    1.617782   0.51674                   \n",
      "    21     0.791424   1.587121   0.523789                  \n",
      "    22     0.767432   1.592119   0.534361                  \n",
      "    23     0.75117    1.620735   0.529956                  \n",
      "    24     0.738331   1.628828   0.527313                  \n",
      "    25     0.714861   1.626701   0.527313                  \n",
      "    26     0.709863   1.614863   0.529515                  \n",
      "    27     0.699724   1.646191   0.524229                  \n",
      "    28     0.686266   1.634969   0.537445                  \n",
      "    29     0.676541   1.651922   0.534361                  \n",
      "    30     0.669613   1.666373   0.527753                  \n",
      "    31     0.652045   1.661373   0.53304                   \n",
      "    32     0.648494   1.669895   0.533921                  \n",
      "    33     0.637085   1.722889   0.528194                  \n",
      "    34     0.626459   1.69966    0.536564                  \n",
      "    35     0.621366   1.705838   0.532599                  \n",
      "    36     0.609448   1.722429   0.533921                  \n",
      "    37     0.59921    1.766471   0.527313                  \n",
      "    38     0.59098    1.753771   0.533921                  \n",
      "    39     0.577581   1.782727   0.529075                  \n",
      "    40     0.567573   1.778885   0.522467                  \n",
      "    41     0.570577   1.769674   0.529075                  \n",
      "    42     0.568478   1.781744   0.532159                  \n",
      "    43     0.558079   1.801556   0.53304                   \n",
      "    44     0.550219   1.840914   0.528194                  \n",
      "    45     0.544694   1.806888   0.530396                  \n",
      "    46     0.540165   1.837122   0.528634                  \n",
      "    47     0.536973   1.821819   0.529515                  \n",
      "    48     0.532809   1.833223   0.531278                  \n",
      "    49     0.531293   1.836837   0.52511                   \n",
      "    50     0.518307   1.872808   0.527753                  \n",
      "    51     0.509658   1.849478   0.529515                  \n",
      "    52     0.508501   1.847564   0.526432                  \n",
      "    53     0.496897   1.848929   0.526432                  \n",
      "    54     0.49355    1.849172   0.518502                  \n",
      "    55     0.490598   1.849098   0.529956                  \n",
      "    56     0.486668   1.874405   0.522907                  \n",
      "    57     0.486658   1.861157   0.522026                  \n",
      "    58     0.487923   1.895737   0.525551                  \n",
      "    59     0.480616   1.885145   0.528634                  \n",
      "    60     0.486922   1.906277   0.526432                  \n",
      "    61     0.479804   1.900856   0.52467                   \n",
      "    62     0.482414   1.898503   0.522467                  \n",
      "    63     0.483366   1.930192   0.53304                   \n",
      "    64     0.470466   1.92945    0.519383                  \n",
      "    65     0.471856   1.902532   0.528634                  \n",
      "    66     0.469558   1.905066   0.533921                  \n",
      "    67     0.463229   1.934211   0.525551                  \n",
      "    68     0.465688   1.934902   0.526432                  \n",
      "    69     0.454285   1.946221   0.528194                  \n",
      "    70     0.448744   1.930479   0.530396                  \n",
      "    71     0.442017   1.920801   0.524229                  \n",
      "    72     0.44273    1.944257   0.531278                  \n",
      "    73     0.439623   1.940289   0.526432                  \n",
      "    74     0.444038   1.974588   0.528194                  \n",
      "    75     0.446334   1.943406   0.533921                  \n",
      "    76     0.443206   1.995069   0.529075                  \n",
      "    77     0.446887   1.989057   0.522026                  \n",
      "    78     0.437055   1.947201   0.532599                  \n",
      "    79     0.433902   1.974144   0.530837                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8128d94bcf9948188460070eae767b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      1.282355   0.612492   0.785022  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f77f4925e3e4a66910a3bc5407f923d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f6be6d1865436fa6324f1208505841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.097243   1.753407   0.422467  \n",
      "    1      1.77459    1.618782   0.463877                 \n",
      "    2      1.60266    1.570686   0.474449                 \n",
      "    3      1.473488   1.535243   0.494714                 \n",
      "    4      1.378437   1.527113   0.495154                 \n",
      "    5      1.305731   1.498263   0.504405                 \n",
      "    6      1.239139   1.477901   0.514537                 \n",
      "    7      1.185495   1.498745   0.507489                 \n",
      "    8      1.146243   1.461416   0.52511                  \n",
      "    9      1.107745   1.47727    0.520264                 \n",
      "    10     1.068972   1.452259   0.528634                 \n",
      "    11     1.034247   1.481345   0.53348                  \n",
      "    12     0.995607   1.480951   0.518943                  \n",
      "    13     0.967352   1.470302   0.52511                   \n",
      "    14     0.939357   1.4983     0.522026                  \n",
      "    15     0.912279   1.474806   0.530837                  \n",
      "    16     0.891298   1.490073   0.548018                  \n",
      "    17     0.86727    1.499048   0.540529                  \n",
      "    18     0.841949   1.492191   0.538767                  \n",
      "    19     0.82281    1.520215   0.540088                  \n",
      "    20     0.807511   1.515114   0.544934                  \n",
      "    21     0.788358   1.539258   0.538326                  \n",
      "    22     0.769025   1.538244   0.529956                  \n",
      "    23     0.738465   1.554482   0.540969                  \n",
      "    24     0.728987   1.560934   0.540529                  \n",
      "    25     0.712227   1.558021   0.538767                  \n",
      "    26     0.699545   1.588793   0.54141                   \n",
      "    27     0.690008   1.564878   0.538767                  \n",
      "    28     0.682243   1.597375   0.537004                  \n",
      "    29     0.668219   1.627619   0.530396                  \n",
      "    30     0.657676   1.624282   0.537004                  \n",
      "    31     0.654342   1.605036   0.540529                  \n",
      "    32     0.647016   1.622622   0.536123                  \n",
      "    33     0.626384   1.636332   0.531718                  \n",
      "    34     0.617285   1.646993   0.539207                  \n",
      "    35     0.608587   1.64259    0.539207                  \n",
      "    36     0.605171   1.639216   0.540088                  \n",
      "    37     0.589321   1.688654   0.535242                  \n",
      "    38     0.58014    1.663902   0.547137                  \n",
      "    39     0.58315    1.681286   0.532599                  \n",
      "    40     0.580219   1.700379   0.534361                  \n",
      "    41     0.569951   1.711365   0.53348                   \n",
      "    42     0.566215   1.701755   0.530396                  \n",
      "    43     0.552661   1.728499   0.537004                  \n",
      "    44     0.551012   1.734422   0.532599                  \n",
      "    45     0.532792   1.729399   0.536123                  \n",
      "    46     0.524325   1.722417   0.539207                  \n",
      "    47     0.531363   1.72099    0.531278                  \n",
      "    48     0.523322   1.734176   0.529515                  \n",
      "    49     0.524446   1.768552   0.540969                  \n",
      "    50     0.528557   1.74783    0.536564                  \n",
      "    51     0.514608   1.735363   0.539648                  \n",
      "    52     0.510344   1.785532   0.531718                  \n",
      "    53     0.509003   1.76275    0.536564                  \n",
      "    54     0.501616   1.798921   0.532599                  \n",
      "    55     0.491167   1.78638    0.531278                  \n",
      "    56     0.487648   1.773834   0.540529                  \n",
      "    57     0.486033   1.804813   0.53348                   \n",
      "    58     0.476108   1.803306   0.530396                  \n",
      "    59     0.47625    1.817934   0.531278                  \n",
      "    60     0.473985   1.802995   0.535683                  \n",
      "    61     0.472648   1.803405   0.534361                  \n",
      "    62     0.469861   1.796988   0.534802                  \n",
      "    63     0.463468   1.817885   0.538767                  \n",
      "    64     0.461532   1.828971   0.537445                  \n",
      "    65     0.463105   1.833171   0.534802                  \n",
      "    66     0.464311   1.830443   0.535242                  \n",
      "    67     0.453675   1.851259   0.536123                  \n",
      "    68     0.454594   1.829903   0.529515                  \n",
      "    69     0.456745   1.84692    0.53304                   \n",
      "    70     0.462458   1.863844   0.529515                  \n",
      "    71     0.457813   1.857869   0.531278                  \n",
      "    72     0.448931   1.8516     0.532599                  \n",
      "    73     0.447381   1.864101   0.539207                  \n",
      "    74     0.439094   1.850172   0.538326                  \n",
      "    75     0.436859   1.859789   0.533921                  \n",
      "    76     0.435063   1.869867   0.54185                   \n",
      "    77     0.436368   1.850165   0.538326                  \n",
      "    78     0.44       1.847757   0.53304                   \n",
      "    79     0.434235   1.855293   0.537445                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6f0838725d4ea59c305c95d638c385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      1.262129   0.584096   0.781938  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0959af9948db46c1a3b5a9d3af15e9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3927e92f344fe4b6184d1deb4749eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.075648   1.735327   0.430837  \n",
      "    1      1.752687   1.567021   0.470044                 \n",
      "    2      1.576107   1.516267   0.495154                 \n",
      "    3      1.453067   1.460351   0.507489                 \n",
      "    4      1.359282   1.425666   0.512775                 \n",
      "    5      1.290293   1.390877   0.52511                  \n",
      "    6      1.216117   1.37843    0.531718                 \n",
      "    7      1.158731   1.370648   0.537885                 \n",
      "    8      1.117834   1.341147   0.543612                 \n",
      "    9      1.084193   1.357706   0.547137                 \n",
      "    10     1.04471    1.33363    0.553744                 \n",
      "    11     1.00111    1.336209   0.552423                 \n",
      "    12     0.967954   1.307748   0.554185                  \n",
      "    13     0.934794   1.317956   0.552863                  \n",
      "    14     0.904897   1.307906   0.568722                  \n",
      "    15     0.884096   1.333043   0.550661                  \n",
      "    16     0.858439   1.353481   0.551982                  \n",
      "    17     0.837994   1.326105   0.557709                  \n",
      "    18     0.820306   1.342691   0.560352                  \n",
      "    19     0.797909   1.354257   0.551542                  \n",
      "    20     0.773264   1.358366   0.553304                  \n",
      "    21     0.76272    1.35776    0.568282                  \n",
      "    22     0.742914   1.342659   0.55815                   \n",
      "    23     0.732951   1.36825    0.564758                  \n",
      "    24     0.716273   1.390532   0.559471                  \n",
      "    25     0.695406   1.368584   0.560793                  \n",
      "    26     0.680146   1.374963   0.556388                  \n",
      "    27     0.668675   1.392423   0.562555                  \n",
      "    28     0.665145   1.395608   0.557709                  \n",
      "    29     0.650791   1.400054   0.563436                  \n",
      "    30     0.641766   1.420138   0.55859                   \n",
      "    31     0.624054   1.42425    0.559912                  \n",
      "    32     0.624346   1.406333   0.565198                  \n",
      "    33     0.61215    1.441479   0.559912                  \n",
      "    34     0.606169   1.430846   0.567841                  \n",
      "    35     0.596013   1.447093   0.56652                   \n",
      "    36     0.596401   1.492787   0.55815                   \n",
      "    37     0.578579   1.465525   0.563877                  \n",
      "    38     0.56781    1.476951   0.556388                  \n",
      "    39     0.555075   1.502272   0.564758                  \n",
      "    40     0.551725   1.477958   0.561674                  \n",
      "    41     0.555975   1.496535   0.550661                  \n",
      "    42     0.55229    1.500179   0.559031                  \n",
      "    43     0.540552   1.471387   0.562115                  \n",
      "    44     0.532304   1.511608   0.559471                  \n",
      "    45     0.528585   1.498277   0.563436                  \n",
      "    46     0.520785   1.528709   0.561674                  \n",
      "    47     0.521332   1.521712   0.561233                  \n",
      "    48     0.520087   1.532632   0.553745                  \n",
      "    49     0.517771   1.529958   0.556828                  \n",
      "    50     0.512944   1.519087   0.559912                  \n",
      "    51     0.500815   1.541635   0.560352                  \n",
      "    52     0.497584   1.553893   0.563436                  \n",
      "    53     0.499258   1.559077   0.553744                  \n",
      "    54     0.492093   1.578716   0.555507                  \n",
      "    55     0.48951    1.569412   0.557709                  \n",
      "    56     0.472576   1.574184   0.556388                  \n",
      "    57     0.474637   1.583834   0.561674                  \n",
      "    58     0.471652   1.582717   0.556388                  \n",
      "    59     0.473506   1.572569   0.552423                  \n",
      "    60     0.470724   1.573308   0.564758                  \n",
      "    61     0.475206   1.57589    0.564317                  \n",
      "    62     0.469443   1.591503   0.563877                  \n",
      "    63     0.466999   1.626485   0.55815                   \n",
      "    64     0.463349   1.581868   0.567841                  \n",
      "    65     0.451038   1.587208   0.55859                   \n",
      "    66     0.448422   1.582713   0.568722                  \n",
      "    67     0.454149   1.602176   0.554626                  \n",
      "    68     0.442623   1.609648   0.564317                  \n",
      "    69     0.439718   1.601653   0.559031                  \n",
      "    70     0.445479   1.602221   0.560352                  \n",
      "    71     0.443611   1.622144   0.555066                  \n",
      "    72     0.433613   1.615328   0.562996                  \n",
      "    73     0.427562   1.627385   0.561674                  \n",
      "    74     0.421581   1.61302    0.562115                  \n",
      "    75     0.426008   1.645767   0.555507                  \n",
      "    76     0.425286   1.644754   0.55859                   \n",
      "    77     0.425105   1.649043   0.561233                  \n",
      "    78     0.418164   1.642404   0.563436                  \n",
      "    79     0.426187   1.643802   0.557709                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e8dd721ebc45dd8ac8afb469a31e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      1.255307   0.577794   0.787225  \n"
     ]
    }
   ],
   "source": [
    "label_csv = f'{PATH}labels.csv'\n",
    "n = len(list(open(label_csv))) - 1\n",
    "vacc =[]\n",
    "reps=1\n",
    "for rep in range(reps):\n",
    "    val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1))\n",
    "    data12 = get_data(sz, bs, val_idxs, label_csv)\n",
    "    learn12 = ConvLearner.pretrained(arch, data12, precompute=True, ps = 0.2)\n",
    "    #lrf=learn3.lr_find(1e-6,10)\n",
    "    #learn3.sched.plot()\n",
    "    learn12.fit(1e-2, 80)\n",
    "    learn12.precompute = False\n",
    "    val_loss, val_acc = learn12.fit(1e-2, 1)\n",
    "    vacc.append(val_acc)\n",
    "#lrf=learn3.lr_find(1e-9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7826431738122445\n",
      "0.010618841127250402\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(vacc))\n",
    "print(np.std(vacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "log_preds,y = learn3.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data3.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-class\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa09ec5559cd44e5852281bbbe358033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab37401fd1c43218c97f730fbceda63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.42497    1.169546   0.531302  \n",
      "    1      1.229321   1.09283    0.573961                 \n",
      "    2      1.12997    1.054204   0.573961                 \n",
      "    3      1.038825   1.014063   0.592798                 \n",
      "    4      0.973951   1.005085   0.603324                  \n",
      "    5      0.925179   0.977274   0.60831                   \n",
      "    6      0.875097   0.976585   0.61385                   \n",
      "    7      0.831286   0.973784   0.624377                  \n",
      "    8      0.801342   0.961254   0.628809                  \n",
      "    9      0.771943   0.967243   0.633795                  \n",
      "    10     0.750462   1.00188    0.622715                  \n",
      "    11     0.718751   0.977237   0.630471                  \n",
      "    12     0.688838   0.954529   0.640997                  \n",
      "    13     0.666919   0.954334   0.633241                  \n",
      "    14     0.661654   0.950935   0.640997                  \n",
      "    15     0.648063   0.978371   0.635457                  \n",
      "    16     0.626583   0.992257   0.634349                  \n",
      "    17     0.601947   0.99053    0.631025                  \n",
      "    18     0.594346   1.002874   0.628255                  \n",
      "    19     0.582872   0.990095   0.633241                  \n",
      "    20     0.564689   0.986962   0.640443                  \n",
      "    21     0.539593   1.002177   0.642659                  \n",
      "    22     0.539706   0.986655   0.637119                  \n",
      "    23     0.535218   0.992282   0.640443                  \n",
      "    24     0.51184    0.993356   0.634349                  \n",
      "    25     0.502413   0.998939   0.645983                  \n",
      "    26     0.493207   1.022682   0.639889                  \n",
      "    27     0.493086   0.999535   0.646537                  \n",
      "    28     0.486883   1.006898   0.642659                  \n",
      "    29     0.475592   1.026882   0.647091                  \n",
      "    30     0.469537   1.027686   0.640997                  \n",
      "    31     0.462855   1.024086   0.643767                  \n",
      "    32     0.456011   1.026373   0.643767                  \n",
      "    33     0.451965   1.036874   0.643213                  \n",
      "    34     0.434778   1.055555   0.642105                  \n",
      "    35     0.430117   1.069818   0.640997                  \n",
      "    36     0.421711   1.082101   0.646537                  \n",
      "    37     0.422474   1.07586    0.632687                  \n",
      "    38     0.413431   1.072618   0.639889                  \n",
      "    39     0.408856   1.071001   0.645429                  \n",
      "    40     0.392743   1.064504   0.643767                  \n",
      "    41     0.38759    1.055344   0.647091                  \n",
      "    42     0.396225   1.101957   0.644321                  \n",
      "    43     0.403327   1.056601   0.648753                  \n",
      "    44     0.396705   1.064857   0.644875                  \n",
      "    45     0.390145   1.086321   0.642659                  \n",
      "    46     0.394818   1.07166    0.643767                  \n",
      "    47     0.383295   1.075602   0.647091                  \n",
      "    48     0.371787   1.086075   0.638227                  \n",
      "    49     0.368967   1.114043   0.648753                  \n",
      "    50     0.365395   1.138475   0.638781                  \n",
      "    51     0.362513   1.095361   0.643213                  \n",
      "    52     0.367418   1.147059   0.635457                  \n",
      "    53     0.365238   1.114219   0.642659                  \n",
      "    54     0.351417   1.075967   0.639889                  \n",
      "    55     0.348923   1.131802   0.643767                  \n",
      "    56     0.344437   1.124258   0.644321                  \n",
      "    57     0.338445   1.115282   0.654294                  \n",
      "    58     0.349334   1.122499   0.647645                  \n",
      "    59     0.34124    1.15677    0.647091                  \n",
      "    60     0.340854   1.149545   0.640997                  \n",
      "    61     0.337063   1.128048   0.640443                  \n",
      "    62     0.336828   1.148302   0.641551                  \n",
      "    63     0.338122   1.150484   0.640443                  \n",
      "    64     0.31953    1.144923   0.650416                  \n",
      "    65     0.319344   1.162779   0.640997                  \n",
      "    66     0.316677   1.166972   0.644321                  \n",
      "    67     0.31341    1.169669   0.639889                  \n",
      "    68     0.316505   1.165242   0.639335                  \n",
      "    69     0.318342   1.165074   0.641551                  \n",
      "    70     0.304199   1.183769   0.644875                  \n",
      "    71     0.297349   1.191093   0.643767                  \n",
      "    72     0.300689   1.194387   0.643213                  \n",
      "    73     0.305652   1.19178    0.645429                  \n",
      "    74     0.301684   1.181224   0.649307                  \n",
      "    75     0.30204    1.196921   0.644321                  \n",
      "    76     0.315461   1.179285   0.642659                  \n",
      "    77     0.314766   1.186571   0.641551                  \n",
      "    78     0.311301   1.171141   0.65374                   \n",
      "    79     0.309391   1.192044   0.651524                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1f03b6185845a0b57adb12fc1b631d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.929813   0.401703   0.86482   \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0e6502f6584c6fba94e18e49c0abde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8685cdce994e9ab4fe90086dd6adcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.412194   1.198569   0.522992  \n",
      "    1      1.223744   1.112899   0.555679                 \n",
      "    2      1.115489   1.068825   0.573407                 \n",
      "    3      1.035969   1.047163   0.606094                 \n",
      "    4      0.973722   1.036431   0.582825                  \n",
      "    5      0.932949   1.040126   0.599446                  \n",
      "    6      0.898922   1.011249   0.613296                  \n",
      "    7      0.855609   1.025993   0.616066                  \n",
      "    8      0.819141   1.012049   0.609418                  \n",
      "    9      0.787115   0.989778   0.622161                  \n",
      "    10     0.760138   1.007121   0.621053                  \n",
      "    11     0.732377   0.990125   0.624931                  \n",
      "    12     0.703001   1.01453    0.619391                  \n",
      "    13     0.685357   1.006867   0.624377                  \n",
      "    14     0.665349   1.02689    0.622715                  \n",
      "    15     0.643158   1.009756   0.633241                  \n",
      "    16     0.61476    1.033769   0.626593                  \n",
      "    17     0.604914   1.003473   0.632133                  \n",
      "    18     0.591151   1.029826   0.618283                  \n",
      "    19     0.581082   1.028141   0.638227                  \n",
      "    20     0.559303   1.043214   0.630471                  \n",
      "    21     0.561824   1.030996   0.624377                  \n",
      "    22     0.543655   1.052408   0.621607                  \n",
      "    23     0.522481   1.050607   0.628809                  \n",
      "    24     0.508025   1.067993   0.618837                  \n",
      "    25     0.506497   1.081522   0.626593                  \n",
      "    26     0.495644   1.082829   0.621607                  \n",
      "    27     0.493454   1.133863   0.612742                  \n",
      "    28     0.495469   1.072005   0.630471                  \n",
      "    29     0.493947   1.112455   0.619391                  \n",
      "    30     0.484884   1.086332   0.631579                  \n",
      "    31     0.473453   1.104809   0.624377                  \n",
      "    32     0.467983   1.096482   0.629917                  \n",
      "    33     0.459763   1.115368   0.632133                  \n",
      "    34     0.449228   1.08456    0.633241                  \n",
      "    35     0.444357   1.121012   0.628255                  \n",
      "    36     0.436183   1.13829    0.628255                  \n",
      "    37     0.43863    1.114035   0.624931                  \n",
      "    38     0.431998   1.115202   0.625485                  \n",
      "    39     0.419655   1.105924   0.644321                  \n",
      "    40     0.423478   1.154873   0.632133                  \n",
      "    41     0.427346   1.168311   0.616066                  \n",
      "    42     0.415897   1.137499   0.629917                  \n",
      "    43     0.39735    1.129489   0.636011                  \n",
      "    44     0.389118   1.157395   0.631025                  \n",
      "    45     0.385453   1.170529   0.629917                  \n",
      "    46     0.382836   1.177119   0.633241                  \n",
      "    47     0.3875     1.157929   0.628255                  \n",
      "    48     0.387051   1.143649   0.635457                  \n",
      "    49     0.387836   1.168163   0.624377                  \n",
      "    50     0.382724   1.176767   0.619945                  \n",
      "    51     0.369202   1.221896   0.630471                  \n",
      "    52     0.36627    1.202104   0.627147                  \n",
      "    53     0.36214    1.183611   0.631579                  \n",
      "    54     0.363596   1.179329   0.631025                  \n",
      "    55     0.369598   1.180042   0.631579                  \n",
      "    56     0.363631   1.206697   0.624377                  \n",
      "    57     0.370682   1.200963   0.638781                  \n",
      "    58     0.377292   1.210801   0.622715                  \n",
      "    59     0.364735   1.195389   0.631579                  \n",
      "    60     0.351085   1.197446   0.626593                  \n",
      "    61     0.348267   1.218795   0.624931                  \n",
      "    62     0.342304   1.201727   0.627701                  \n",
      "    63     0.335492   1.216067   0.629917                  \n",
      "    64     0.331688   1.256549   0.626039                  \n",
      "    65     0.323447   1.250831   0.631579                  \n",
      "    66     0.320653   1.234999   0.635457                  \n",
      "    67     0.314159   1.223393   0.640997                  \n",
      "    68     0.31097    1.228398   0.629363                  \n",
      "    69     0.312523   1.287503   0.629917                  \n",
      "    70     0.31283    1.23922    0.634349                  \n",
      "    71     0.311647   1.274073   0.631025                  \n",
      "    72     0.309295   1.250051   0.639889                  \n",
      "    73     0.319529   1.227065   0.637673                  \n",
      "    74     0.318001   1.271796   0.633241                  \n",
      "    75     0.320962   1.226688   0.628809                  \n",
      "    76     0.311425   1.255523   0.631579                 \n",
      "    77     0.300493   1.225259   0.637673                  \n",
      "    78     0.297431   1.239027   0.637119                  \n",
      "    79     0.295334   1.239453   0.637119                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e8845ba2d944e29714f6fe27769434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.924083   0.370779   0.85097   \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a2a6e481364b1aa467f12508f1b7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8c54ad583a41e19c0f1695d0fd2011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.429689   1.162036   0.536288  \n",
      "    1      1.224177   1.078683   0.567313                 \n",
      "    2      1.111099   1.03352    0.583934                 \n",
      "    3      1.036597   1.017912   0.59169                  \n",
      "    4      0.974851   0.990602   0.612188                  \n",
      "    5      0.92067    0.971769   0.612742                  \n",
      "    6      0.879824   0.95872    0.619391                  \n",
      "    7      0.844166   0.956747   0.629363                  \n",
      "    8      0.805343   0.951952   0.621053                  \n",
      "    9      0.77648    0.981513   0.626039                  \n",
      "    10     0.754122   0.977977   0.628255                  \n",
      "    11     0.733219   0.921964   0.634349                  \n",
      "    12     0.71301    0.950574   0.647645                  \n",
      "    13     0.688313   0.937593   0.645429                  \n",
      "    14     0.667231   0.946213   0.637673                  \n",
      "    15     0.637537   0.936818   0.641551                  \n",
      "    16     0.615944   0.949597   0.634903                  \n",
      "    17     0.596248   0.948995   0.654848                  \n",
      "    18     0.588285   0.934071   0.651524                  \n",
      "    19     0.582536   0.959554   0.646537                  \n",
      "    20     0.575871   0.964031   0.645429                  \n",
      "    21     0.556468   0.937834   0.652078                  \n",
      "    22     0.535618   0.948001   0.648753                  \n",
      "    23     0.531233   0.939185   0.647645                  \n",
      "    24     0.522525   0.969658   0.655956                  \n",
      "    25     0.516928   0.967634   0.649307                  \n",
      "    26     0.506213   0.956222   0.653186                  \n",
      "    27     0.496748   0.9561     0.655402                  \n",
      "    28     0.494552   0.986597   0.647091                  \n",
      "    29     0.502661   0.987229   0.644321                  \n",
      "    30     0.484186   0.977331   0.658172                  \n",
      "    31     0.467859   0.969721   0.644875                  \n",
      "    32     0.458176   1.00144    0.642105                  \n",
      "    33     0.445501   0.993949   0.65097                   \n",
      "    34     0.447865   0.990987   0.653186                  \n",
      "    35     0.435125   0.989125   0.645429                  \n",
      "    36     0.435113   1.001098   0.652078                  \n",
      "    37     0.436469   1.004016   0.65374                   \n",
      "    38     0.419821   1.022384   0.648753                  \n",
      "    39     0.40829    1.049468   0.645429                  \n",
      "    40     0.399246   1.051957   0.649861                  \n",
      "    41     0.407075   1.063609   0.649861                  \n",
      "    42     0.412459   1.042301   0.652078                  \n",
      "    43     0.407533   1.018592   0.645429                  \n",
      "    44     0.386998   1.037197   0.65374                   \n",
      "    45     0.373962   1.069798   0.637673                  \n",
      "    46     0.365271   1.055754   0.647091                  \n",
      "    47     0.366286   1.062448   0.645983                  \n",
      "    48     0.359448   1.047129   0.652078                  \n",
      "    49     0.365625   1.083599   0.65374                   \n",
      "    50     0.37773    1.062593   0.649861                  \n",
      "    51     0.373022   1.046493   0.648753                  \n",
      "    52     0.367834   1.052992   0.655402                  \n",
      "    53     0.355619   1.058078   0.642659                  \n",
      "    54     0.352111   1.066053   0.647091                  \n",
      "    55     0.341877   1.07768    0.648753                  \n",
      "    56     0.338887   1.091271   0.650416                  \n",
      "    57     0.339026   1.078127   0.652632                  \n",
      "    58     0.331661   1.117226   0.645429                  \n",
      "    59     0.32907    1.098911   0.653186                  \n",
      "    60     0.327141   1.087745   0.649307                  \n",
      "    61     0.337968   1.109159   0.655956                  \n",
      "    62     0.3349     1.084768   0.657618                  \n",
      "    63     0.331437   1.117857   0.654294                  \n",
      "    64     0.322944   1.114111   0.651524                  \n",
      "    65     0.319904   1.098869   0.649307                  \n",
      "    66     0.335253   1.108624   0.644875                  \n",
      "    67     0.324929   1.089439   0.650416                  \n",
      "    68     0.318499   1.114929   0.652632                  \n",
      "    69     0.315413   1.117742   0.65374                   \n",
      "    70     0.323805   1.146426   0.65097                   \n",
      "    71     0.326105   1.106464   0.651524                  \n",
      "    72     0.314127   1.130051   0.652078                 \n",
      "    73     0.314923   1.14608    0.65651                   \n",
      "    74     0.313282   1.124641   0.65928                   \n",
      "    75     0.314682   1.10007    0.65097                   \n",
      "    76     0.304499   1.128463   0.651524                  \n",
      "    77     0.30566    1.112631   0.652632                  \n",
      "    78     0.304199   1.136676   0.651524                  \n",
      "    79     0.305553   1.134786   0.652078                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a54d80575264446b4a96f7c7e0347a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.910183   0.375534   0.845983  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9b4acdffe24a149e7665c6692e99b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f10020b9c443dba82a8868d8633ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.395116   1.178383   0.531302  \n",
      "    1      1.219918   1.105429   0.562881                 \n",
      "    2      1.11928    1.066785   0.582271                 \n",
      "    3      1.040254   1.046407   0.58615                  \n",
      "    4      0.994505   1.021475   0.59723                   \n",
      "    5      0.938706   1.028119   0.601108                  \n",
      "    6      0.899046   1.035055   0.609972                  \n",
      "    7      0.849334   1.001971   0.61108                   \n",
      "    8      0.815074   0.99998    0.621607                  \n",
      "    9      0.784653   1.001286   0.623269                  \n",
      "    10     0.758871   0.992234   0.623269                  \n",
      "    11     0.729219   1.009683   0.624377                  \n",
      "    12     0.712347   1.016913   0.624931                  \n",
      "    13     0.697688   0.998736   0.622161                  \n",
      "    14     0.687145   0.998161   0.631025                  \n",
      "    15     0.669628   1.014446   0.622161                  \n",
      "    16     0.637229   1.017284   0.622161                  \n",
      "    17     0.622008   1.038784   0.628809                  \n",
      "    18     0.609796   1.019375   0.633241                  \n",
      "    19     0.5958     1.047762   0.626039                  \n",
      "    20     0.587386   1.049795   0.633795                  \n",
      "    21     0.568801   1.041404   0.626593                  \n",
      "    22     0.558566   1.069986   0.622715                  \n",
      "    23     0.544786   1.052222   0.632133                  \n",
      "    24     0.530431   1.056257   0.631579                  \n",
      "    25     0.532253   1.068771   0.624931                  \n",
      "    26     0.521342   1.122617   0.60831                   \n",
      "    27     0.506763   1.092737   0.621053                  \n",
      "    28     0.492887   1.0947     0.620499                  \n",
      "    29     0.499831   1.044648   0.633795                  \n",
      "    30     0.490852   1.079366   0.634349                  \n",
      "    31     0.474308   1.07671    0.635457                  \n",
      "    32     0.475602   1.107167   0.624377                  \n",
      "    33     0.459224   1.118088   0.633241                  \n",
      "    34     0.448099   1.110592   0.629917                  \n",
      "    35     0.441393   1.128709   0.623269                  \n",
      "    36     0.441342   1.116268   0.630471                  \n",
      "    37     0.434562   1.141041   0.623269                  \n",
      "    38     0.428048   1.125577   0.624377                  \n",
      "    39     0.426099   1.148127   0.626593                  \n",
      "    40     0.435412   1.142512   0.624377                  \n",
      "    41     0.438181   1.143431   0.630471                  \n",
      "    42     0.430971   1.131497   0.634349                  \n",
      "    43     0.420416   1.158007   0.625485                  \n",
      "    44     0.401172   1.155988   0.631579                  \n",
      "    45     0.391728   1.158609   0.620499                  \n",
      "    46     0.395935   1.158125   0.627701                  \n",
      "    47     0.390973   1.147204   0.630471                  \n",
      "    48     0.382957   1.170067   0.627701                  \n",
      "    49     0.387481   1.169086   0.631025                  \n",
      "    50     0.379067   1.177161   0.627701                  \n",
      "    51     0.3752     1.175542   0.629917                  \n",
      "    52     0.369819   1.190971   0.628809                  \n",
      "    53     0.359399   1.172584   0.632687                  \n",
      "    54     0.351307   1.193858   0.633241                  \n",
      "    55     0.354603   1.198268   0.628809                  \n",
      "    56     0.357324   1.191732   0.633241                  \n",
      "    57     0.353409   1.2144     0.631025                  \n",
      "    58     0.341674   1.191724   0.639335                  \n",
      "    59     0.343491   1.227309   0.638227                  \n",
      "    60     0.34516    1.236147   0.624931                  \n",
      "    61     0.345733   1.215254   0.637673                  \n",
      "    62     0.338205   1.234487   0.630471                  \n",
      "    63     0.3358     1.238654   0.634349                  \n",
      "    64     0.334729   1.249471   0.632687                  \n",
      "    65     0.336997   1.228884   0.631025                  \n",
      "    66     0.334833   1.239005   0.629363                  \n",
      "    67     0.330801   1.229873   0.644321                  \n",
      "    68     0.324679   1.250236   0.631025                  \n",
      "    69     0.328358   1.236698   0.622715                  \n",
      "    70     0.334192   1.224451   0.640997                  \n",
      "    71     0.333467   1.228013   0.637119                  \n",
      "    72     0.327859   1.230511   0.629917                  \n",
      "    73     0.324967   1.258523   0.638227                  \n",
      "    74     0.31014    1.257782   0.623823                  \n",
      "    75     0.316585   1.262541   0.632133                  \n",
      "    76     0.321141   1.242083   0.629917                  \n",
      "    77     0.321763   1.261583   0.631579                  \n",
      "    78     0.321544   1.25901    0.638227                  \n",
      "    79     0.313609   1.266863   0.633795                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362e604fd17248cf869fe997ba1d8dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.930055   0.392607   0.844875  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692a7fc36ff34f6fa3c53a931adcd7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a441508468ac40c6b385985fa2e055f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.384506   1.153186   0.532964  \n",
      "    1      1.212089   1.076495   0.564543                 \n",
      "    2      1.128055   1.04574    0.583379                 \n",
      "    3      1.048112   1.026511   0.593906                 \n",
      "    4      0.988016   1.001281   0.607202                  \n",
      "    5      0.928224   0.995154   0.622161                  \n",
      "    6      0.872303   0.979979   0.622715                  \n",
      "    7      0.836535   0.979482   0.620499                  \n",
      "    8      0.803444   0.961107   0.637673                  \n",
      "    9      0.774346   0.963116   0.634349                  \n",
      "    10     0.759471   0.957151   0.641551                  \n",
      "    11     0.725286   0.961418   0.638227                  \n",
      "    12     0.708755   0.93569    0.643213                  \n",
      "    13     0.681668   0.943194   0.640443                  \n",
      "    14     0.666023   0.972      0.634349                  \n",
      "    15     0.657869   0.950371   0.634349                  \n",
      "    16     0.634866   1.000528   0.635457                  \n",
      "    17     0.61526    0.975124   0.642105                  \n",
      "    18     0.595152   0.976603   0.645983                  \n",
      "    19     0.580216   0.968857   0.646537                  \n",
      "    20     0.575014   0.971431   0.642105                  \n",
      "    21     0.569708   0.969654   0.627147                  \n",
      "    22     0.554492   0.986049   0.629917                  \n",
      "    23     0.548176   0.979118   0.631025                  \n",
      "    24     0.538004   0.992322   0.640997                  \n",
      "    25     0.529774   1.002016   0.638227                  \n",
      "    26     0.511911   0.988649   0.644321                  \n",
      "    27     0.492347   0.993441   0.645983                  \n",
      "    28     0.485254   0.988041   0.640997                  \n",
      "    29     0.47866    1.021505   0.643213                  \n",
      "    30     0.47316    0.984318   0.648753                  \n",
      "    31     0.455528   1.02742    0.640443                  \n",
      "    32     0.457799   1.047387   0.638227                  \n",
      "    33     0.447606   1.056308   0.642105                  \n",
      "    34     0.44048    1.033991   0.631025                  \n",
      "    35     0.434011   1.067275   0.638227                  \n",
      "    36     0.426793   1.092085   0.641551                  \n",
      "    37     0.432229   1.037289   0.644875                  \n",
      "    38     0.43315    1.081128   0.640997                  \n",
      "    39     0.426791   1.07126    0.635457                  \n",
      "    40     0.422193   1.037985   0.642659                  \n",
      "    41     0.4065     1.053249   0.645429                  \n",
      "    42     0.396102   1.077085   0.643213                  \n",
      "    43     0.391217   1.061785   0.637673                  \n",
      "    44     0.387847   1.090372   0.648753                  \n",
      "    45     0.386049   1.068203   0.647091                  \n",
      "    46     0.379574   1.068432   0.642105                  \n",
      "    47     0.373907   1.077995   0.639889                  \n",
      "    48     0.376058   1.078927   0.65374                   \n",
      "    49     0.375788   1.102892   0.645983                  \n",
      "    50     0.377166   1.096177   0.647645                  \n",
      "    51     0.362291   1.097024   0.649307                  \n",
      "    52     0.352962   1.12916    0.643767                  \n",
      "    53     0.357893   1.123956   0.651524                  \n",
      "    54     0.356938   1.124894   0.644321                  \n",
      "    55     0.356523   1.14557    0.637673                  \n",
      "    56     0.350971   1.119605   0.644321                  \n",
      "    57     0.357856   1.122224   0.641551                  \n",
      "    58     0.35893    1.118273   0.643213                  \n",
      "    59     0.352301   1.124681   0.640443                  \n",
      "    60     0.341061   1.143171   0.638781                  \n",
      "    61     0.33536    1.126493   0.65097                   \n",
      "    62     0.334289   1.173376   0.639335                  \n",
      "    63     0.327871   1.157416   0.644875                  \n",
      "    64     0.327171   1.164875   0.643767                  \n",
      "    65     0.321955   1.176674   0.638227                  \n",
      "    66     0.330438   1.168326   0.647645                  \n",
      "    67     0.341129   1.178231   0.635457                  \n",
      "    68     0.341211   1.151851   0.640997                  \n",
      "    69     0.335899   1.130039   0.649307                  \n",
      "    70     0.330762   1.147688   0.644875                  \n",
      "    71     0.328451   1.133322   0.648199                  \n",
      "    72     0.329714   1.14814    0.646537                  \n",
      "    73     0.334707   1.155277   0.644321                  \n",
      "    74     0.323319   1.18076    0.641551                  \n",
      "    75     0.314236   1.148019   0.641551                  \n",
      "    76     0.301697   1.17433    0.649307                  \n",
      "    77     0.297628   1.16413    0.646537                  \n",
      "    78     0.296123   1.17423    0.651524                  \n",
      "    79     0.290664   1.190322   0.643213                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654900832ad94cbd97c761b08bb7c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.907114   0.402468   0.843767  \n",
      "\n",
      "0.8500831028100857\n",
      "0.007768881039224904\n"
     ]
    }
   ],
   "source": [
    "label_csv = f'{PATH}5labels.csv'\n",
    "n = len(list(open(label_csv))) - 1\n",
    "vacc =[]\n",
    "reps=5\n",
    "for rep in range(reps):\n",
    "    val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1))\n",
    "    data5 = get_data(sz, bs, val_idxs, label_csv)\n",
    "    learn5 = ConvLearner.pretrained(arch, data5, precompute=True, ps = 0.2)\n",
    "    #lrf=learn3.lr_find(1e-6,10)\n",
    "    #learn3.sched.plot()\n",
    "    learn5.fit(1e-2, 80)\n",
    "    learn5.precompute = False\n",
    "    val_loss, val_acc = learn5.fit(1e-2, 1)\n",
    "    vacc.append(val_acc)\n",
    "#lrf=learn3.lr_find(1e-9,10)\n",
    "print(np.mean(vacc))\n",
    "print(np.std(vacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843767313580764\n",
      "0.005671575782813434\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(vacc))\n",
    "print(np.std(vacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84 0.08 0.04 0.03 0.01]                  \n",
      " [0.11 0.79 0.04 0.03 0.01]\n",
      " [0.03 0.03 0.81 0.12 0.  ]\n",
      " [0.02 0.02 0.08 0.86 0.02]\n",
      " [0.04 0.02 0.06 0.27 0.6 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "log_preds,y = learn5.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data5.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 class, precompute=T, then false, then unfreeze early layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b203bb218d024f39ba40cf997b48ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 46/46 [01:18<00:00,  1.38s/it]\n",
      "100%|| 12/12 [00:20<00:00,  1.38s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc0ebec1146456784e6e299e5290897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.032811   1.794492   0.407048  \n",
      "    1      1.736068   1.605781   0.448899                 \n",
      "    2      1.559224   1.520814   0.467841                 \n",
      "    3      1.443779   1.464474   0.49207                  \n",
      "    4      1.345518   1.440043   0.507489                 \n",
      "    5      1.275196   1.389538   0.522026                 \n",
      "    6      1.209192   1.359814   0.526872                 \n",
      "    7      1.145864   1.339373   0.521145                 \n",
      "    8      1.096367   1.32656    0.528194                 \n",
      "    9      1.052433   1.323673   0.526432                 \n",
      "    10     1.017301   1.307913   0.52511                  \n",
      "    11     0.975453   1.308037   0.533921                  \n",
      "    12     0.950404   1.30224    0.540969                  \n",
      "    13     0.916672   1.280027   0.55022                   \n",
      "    14     0.881787   1.305694   0.54978                   \n",
      "    15     0.858536   1.325594   0.537445                  \n",
      "    16     0.830984   1.311673   0.544934                  \n",
      "    17     0.808685   1.311579   0.538767                  \n",
      "    18     0.798372   1.318865   0.54185                   \n",
      "    19     0.769911   1.321065   0.542731                  \n",
      "    20     0.751165   1.319175   0.543172                  \n",
      "    21     0.744011   1.325642   0.537885                  \n",
      "    22     0.729257   1.307829   0.540529                  \n",
      "    23     0.711099   1.338537   0.542731                  \n",
      "    24     0.700492   1.346153   0.546696                  \n",
      "    25     0.678426   1.377562   0.53348                   \n",
      "    26     0.662326   1.349532   0.540088                  \n",
      "    27     0.648065   1.354458   0.548018                  \n",
      "    28     0.65282    1.364448   0.543612                  \n",
      "    29     0.633934   1.372886   0.537885                  \n",
      "    30     0.626534   1.390186   0.548018                  \n",
      "    31     0.612073   1.383675   0.544053                  \n",
      "    32     0.602209   1.396597   0.540088                  \n",
      "    33     0.597146   1.377494   0.543172                  \n",
      "    34     0.584305   1.4054     0.544934                  \n",
      "    35     0.575424   1.428669   0.542731                  \n",
      "    36     0.564818   1.41917    0.544934                  \n",
      "    37     0.553      1.436654   0.542291                  \n",
      "    38     0.543687   1.446822   0.535242                  \n",
      "    39     0.540996   1.436648   0.538767                  \n",
      "    40     0.54332    1.455394   0.545815                  \n",
      "    41     0.536386   1.460187   0.535683                  \n",
      "    42     0.534281   1.450347   0.542291                  \n",
      "    43     0.535245   1.45057    0.546696                  \n",
      "    44     0.523574   1.453358   0.546696                  \n",
      "    45     0.513861   1.46181    0.544934                  \n",
      "    46     0.510203   1.486914   0.540969                  \n",
      "    47     0.502149   1.461623   0.547137                  \n",
      "    48     0.493597   1.484574   0.544934                  \n",
      "    49     0.486044   1.496463   0.544493                  \n",
      "    50     0.483801   1.487345   0.542291                  \n",
      "    51     0.490643   1.497296   0.546696                  \n",
      "    52     0.487836   1.500815   0.539207                  \n",
      "    53     0.48199    1.504437   0.540969                  \n",
      "    54     0.473304   1.513289   0.551101                  \n",
      "    55     0.465406   1.543676   0.540969                  \n",
      "    56     0.461234   1.516215   0.548458                  \n",
      "    57     0.462962   1.56001    0.536123                  \n",
      "    58     0.45641    1.521912   0.538326                  \n",
      "    59     0.459085   1.551402   0.540088                  \n",
      "    60     0.460372   1.553093   0.544053                  \n",
      "    61     0.448119   1.584339   0.537004                  \n",
      "    62     0.44414    1.561929   0.542731                  \n",
      "    63     0.439002   1.537253   0.544934                  \n",
      "    64     0.442515   1.573154   0.545374                  \n",
      "    65     0.435358   1.568868   0.537004                  \n",
      "    66     0.430207   1.569678   0.537885                  \n",
      "    67     0.428184   1.576852   0.536564                  \n",
      "    68     0.430118   1.58482    0.542731                  \n",
      "    69     0.42394    1.578531   0.539648                  \n",
      "    70     0.42647    1.5811     0.542731                  \n",
      "    71     0.423891   1.580324   0.530837                  \n",
      "    72     0.419721   1.609807   0.542291                  \n",
      "    73     0.421993   1.583233   0.544934                  \n",
      "    74     0.423531   1.583561   0.537885                  \n",
      "    75     0.417194   1.580537   0.543172                  \n",
      "    76     0.414726   1.608829   0.544493                  \n",
      "    77     0.416293   1.597984   0.540088                  \n",
      "    78     0.415085   1.584548   0.540529                  \n",
      "    79     0.404483   1.5846     0.544053                  \n",
      "    80     0.410328   1.604624   0.539648                  \n",
      "    81     0.411311   1.60842    0.546696                  \n",
      "    82     0.404768   1.590776   0.549339                  \n",
      "    83     0.402957   1.599327   0.553744                  \n",
      "    84     0.399764   1.610727   0.542731                  \n",
      "    85     0.38981    1.611282   0.542731                  \n",
      "    86     0.391147   1.60369    0.545815                  \n",
      "    87     0.396152   1.619056   0.542731                  \n",
      "    88     0.392425   1.622984   0.544934                  \n",
      "    89     0.391913   1.63914    0.546696                  \n",
      "    90     0.390008   1.621009   0.540088                  \n",
      "    91     0.392377   1.618527   0.544934                  \n",
      "    92     0.38407    1.630203   0.537445                  \n",
      "    93     0.385762   1.622875   0.542291                  \n",
      "    94     0.390548   1.635664   0.542291                  \n",
      "    95     0.386608   1.631533   0.540088                  \n",
      "    96     0.381169   1.625561   0.540969                  \n",
      "    97     0.376728   1.646207   0.54185                   \n",
      "    98     0.374022   1.636825   0.54185                   \n",
      "    99     0.367528   1.642971   0.544934                  \n",
      "   100     0.370996   1.650763   0.542731                  \n",
      "   101     0.373566   1.634906   0.543172                  \n",
      "   102     0.378662   1.641703   0.537445                  \n",
      "   103     0.374113   1.642643   0.549339                  \n",
      "   104     0.372318   1.631909   0.543612                  \n",
      "   105     0.371109   1.647006   0.545374                  \n",
      "   106     0.372511   1.671068   0.540529                  \n",
      "   107     0.369508   1.671306   0.54141                   \n",
      "   108     0.365961   1.6763     0.547577                  \n",
      "   109     0.368824   1.651644   0.542291                  \n",
      "   110     0.364937   1.659934   0.540529                  \n",
      "   111     0.364806   1.653989   0.547137                  \n",
      "   112     0.363728   1.649066   0.545374                  \n",
      "   113     0.36002    1.674609   0.537004                  \n",
      "   114     0.362688   1.660438   0.542291                  \n",
      "   115     0.363557   1.629214   0.54141                   \n",
      "   116     0.363015   1.646778   0.545374                  \n",
      "   117     0.362103   1.645984   0.550661                  \n",
      "   118     0.353876   1.677613   0.546696                  \n",
      "   119     0.356522   1.683662   0.543172                  \n",
      "   120     0.352965   1.66162    0.540088                  \n",
      "   121     0.346092   1.668917   0.545374                  \n",
      "   122     0.347941   1.667438   0.544493                  \n",
      "   123     0.351094   1.681604   0.543612                  \n",
      "   124     0.349641   1.675981   0.542731                  \n",
      "   125     0.352085   1.693579   0.54185                   \n",
      "   126     0.353298   1.671227   0.546696                  \n",
      "   127     0.353997   1.672195   0.546255                  \n",
      "   128     0.344104   1.683374   0.542291                  \n",
      "   129     0.346264   1.67562    0.544493                  \n",
      "   130     0.344973   1.677801   0.542291                  \n",
      "   131     0.345746   1.684827   0.546256                  \n",
      "   132     0.348169   1.676527   0.542731                  \n",
      "   133     0.345352   1.664068   0.543612                  \n",
      "   134     0.345358   1.681533   0.54141                   \n",
      "   135     0.344237   1.67922    0.542291                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   136     0.343225   1.679648   0.54141                   \n",
      "   137     0.344796   1.683749   0.544493                  \n",
      "   138     0.343337   1.670722   0.544053                  \n",
      "   139     0.339586   1.688448   0.540088                  \n",
      "   140     0.341864   1.682743   0.544934                  \n",
      "   141     0.337374   1.68518    0.551982                  \n",
      "   142     0.334818   1.688388   0.548018                  \n",
      "   143     0.336977   1.690154   0.544493                  \n",
      "   144     0.335386   1.692541   0.544493                  \n",
      "   145     0.33492    1.702051   0.543172                  \n",
      "   146     0.33501    1.686798   0.544934                  \n",
      "   147     0.337137   1.714866   0.540969                  \n",
      "   148     0.343846   1.686728   0.552423                  \n",
      "   149     0.339509   1.693699   0.544053                  \n",
      "   150     0.325421   1.695279   0.548458                  \n",
      "   151     0.33009    1.688351   0.542731                  \n",
      "   152     0.334118   1.693768   0.546696                  \n",
      "   153     0.327975   1.705602   0.544053                  \n",
      "   154     0.327567   1.706087   0.544934                  \n",
      "   155     0.323825   1.714856   0.540088                  \n",
      "   156     0.326542   1.711048   0.55022                   \n",
      "   157     0.327377   1.710863   0.542731                  \n",
      "   158     0.324882   1.727023   0.544053                  \n",
      "   159     0.324828   1.718893   0.54185                   \n",
      "   160     0.325716   1.710827   0.542291                  \n",
      "   161     0.326387   1.724183   0.54141                   \n",
      "   162     0.327432   1.749918   0.543172                  \n",
      "   163     0.328732   1.720794   0.542731                  \n",
      "   164     0.323195   1.735861   0.540529                  \n",
      "   165     0.322143   1.730426   0.544053                  \n",
      "   166     0.333884   1.723016   0.546696                  \n",
      "   167     0.325696   1.728598   0.546696                  \n",
      "   168     0.32309    1.710372   0.54978                   \n",
      "   169     0.324059   1.70932    0.545374                  \n",
      "   170     0.32638    1.737538   0.542731                  \n",
      "   171     0.321154   1.715649   0.547137                  \n",
      "   172     0.322323   1.712283   0.544493                  \n",
      "   173     0.316488   1.729243   0.543172                  \n",
      "   174     0.32196    1.727287   0.548458                  \n",
      "   175     0.323535   1.736003   0.542291                  \n",
      "   176     0.325802   1.717312   0.543612                  \n",
      "   177     0.323543   1.718764   0.545374                  \n",
      "   178     0.324367   1.738312   0.548018                  \n",
      "   179     0.314212   1.71588    0.546256                  \n",
      "   180     0.315118   1.7255     0.54185                   \n",
      "   181     0.321903   1.737564   0.543172                  \n",
      "   182     0.324182   1.717409   0.549339                  \n",
      "   183     0.319483   1.738875   0.546696                  \n",
      "   184     0.311075   1.732551   0.544934                  \n",
      "   185     0.313454   1.723757   0.545815                  \n",
      "   186     0.315644   1.730239   0.544934                  \n",
      "   187     0.313397   1.742599   0.543612                  \n",
      "   188     0.309913   1.742551   0.548899                  \n",
      "   189     0.309413   1.748073   0.537445                  \n",
      "   190     0.306797   1.752324   0.54141                   \n",
      "   191     0.306573   1.738052   0.545374                  \n",
      "   192     0.314779   1.72564    0.544934                  \n",
      "   193     0.316723   1.727018   0.544493                  \n",
      "   194     0.318594   1.747623   0.543612                  \n",
      "   195     0.318699   1.725767   0.548458                  \n",
      "   196     0.316134   1.730211   0.548018                  \n",
      "   197     0.313823   1.734014   0.546696                  \n",
      "   198     0.310472   1.739083   0.546696                  \n",
      "   199     0.30884    1.731362   0.548458                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.73136]), 0.5484581521429154]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_csv = f'{PATH}labels.csv'\n",
    "n = len(list(open(label_csv))) - 1\n",
    "val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1))\n",
    "data = get_data(sz, bs, val_idxs, label_csv)\n",
    "learn12 = ConvLearner.pretrained(arch, data, precompute=True, ps = 0.2)\n",
    "learn12.fit(1e-2, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c304947c8bdb4cd7a83e6408e7b63c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.995005   1.674127   0.547137  \n",
      "    1      0.981991   1.609238   0.553744                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn12.precompute = False\n",
    "val_loss, val_acc = learn12.fit(1e-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds,y = learn12.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50d144f552d4278acc8159ced3a2363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9417a7f31efe442db7833fe83c041f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.351154   1.205269   0.506925  \n",
      "    1      1.180097   1.111439   0.542936                 \n",
      "    2      1.077595   1.047404   0.577839                 \n",
      "    3      1.017367   1.035693   0.570637                 \n",
      "    4      0.961159   1.012975   0.590582                  \n",
      "    5      0.905736   0.993671   0.598892                  \n",
      "    6      0.884161   0.979979   0.602216                  \n",
      "    7      0.849423   0.987245   0.607202                  \n",
      "    8      0.813008   1.004718   0.608864                  \n",
      "    9      0.793767   0.98756    0.597784                  \n",
      "    10     0.761666   1.001628   0.609418                  \n",
      "    11     0.751801   0.954475   0.619391                  \n",
      "    12     0.733616   0.949089   0.627147                  \n",
      "    13     0.715964   0.959277   0.642105                  \n",
      "    14     0.705067   0.945306   0.624377                  \n",
      "    15     0.689781   0.973974   0.636565                  \n",
      "    16     0.67276    0.929158   0.635457                  \n",
      "    17     0.663217   0.934706   0.635457                  \n",
      "    18     0.65222    0.970852   0.623269                  \n",
      "    19     0.637296   0.947938   0.638227                  \n",
      "    20     0.622922   0.954662   0.642105                  \n",
      "    21     0.60819    0.910978   0.636011                  \n",
      "    22     0.603097   0.93614    0.648199                  \n",
      "    23     0.593651   0.938989   0.640997                  \n",
      "    24     0.580852   0.926143   0.642659                  \n",
      "    25     0.565527   0.950242   0.646537                  \n",
      "    26     0.563865   0.945118   0.648753                  \n",
      "    27     0.549725   0.943073   0.652078                  \n",
      "    28     0.539233   0.937225   0.657618                  \n",
      "    29     0.529546   0.93883    0.65651                   \n",
      "    30     0.528921   0.955463   0.642105                  \n",
      "    31     0.516536   0.985988   0.652078                  \n",
      "    32     0.522538   0.936401   0.657618                  \n",
      "    33     0.503636   0.938876   0.652632                  \n",
      "    34     0.496199   0.945655   0.652078                  \n",
      "    35     0.499846   0.968074   0.658726                  \n",
      "    36     0.491035   0.947981   0.65374                   \n",
      "    37     0.497388   0.990073   0.638781                  \n",
      "    38     0.500532   0.922904   0.669806                  \n",
      "    39     0.49272    0.945975   0.660942                  \n",
      "    40     0.482752   0.944503   0.663158                  \n",
      "    41     0.46629    0.971313   0.66482                   \n",
      "    42     0.455293   0.982832   0.654294                  \n",
      "    43     0.449638   0.954272   0.652078                  \n",
      "    44     0.442554   0.969527   0.65928                   \n",
      "    45     0.436214   0.98453    0.668698                  \n",
      "    46     0.438572   0.954078   0.658172                  \n",
      "    47     0.437278   0.953607   0.65928                   \n",
      "    48     0.440666   0.938941   0.669806                  \n",
      "    49     0.443345   0.977769   0.660942                  \n",
      "    50     0.447432   1.026104   0.660942                  \n",
      "    51     0.449724   0.970595   0.663158                  \n",
      "    52     0.447658   0.973558   0.665374                  \n",
      "    53     0.438098   0.988005   0.661496                  \n",
      "    54     0.434734   0.979359   0.668144                  \n",
      "    55     0.426948   0.986013   0.660388                  \n",
      "    56     0.419061   0.97752    0.659834                  \n",
      "    57     0.411103   0.973857   0.668698                  \n",
      "    58     0.423279   0.993737   0.669252                  \n",
      "    59     0.436926   1.043785   0.65928                   \n",
      "    60     0.422026   1.007801   0.657618                  \n",
      "    61     0.421789   1.006676   0.652078                  \n",
      "    62     0.427547   0.973292   0.670914                  \n",
      "    63     0.418837   0.992886   0.663712                  \n",
      "    64     0.407998   1.022609   0.653186                  \n",
      "    65     0.395934   0.995952   0.669806                  \n",
      "    66     0.393831   0.977321   0.661496                  \n",
      "    67     0.394046   0.97106    0.673684                  \n",
      "    68     0.396569   0.997756   0.6759                    \n",
      "    69     0.401133   1.007622   0.668144                  \n",
      "    70     0.386451   1.001369   0.66759                   \n",
      "    71     0.371712   0.996516   0.669252                  \n",
      "    72     0.369551   0.989713   0.672022                  \n",
      "    73     0.375808   1.003087   0.675346                  \n",
      "    74     0.382424   1.032345   0.66205                   \n",
      "    75     0.386273   1.005122   0.67867                   \n",
      "    76     0.38728    1.013128   0.669252                  \n",
      "    77     0.374246   1.007938   0.674792                  \n",
      "    78     0.376099   0.987944   0.672576                  \n",
      "    79     0.370962   1.029552   0.67867                   \n",
      "    80     0.373107   1.002917   0.670914                  \n",
      "    81     0.367655   1.02888    0.671468                  \n",
      "    82     0.354954   1.026699   0.679224                  \n",
      "    83     0.352314   0.995682   0.678116                  \n",
      "    84     0.346246   1.019962   0.67867                   \n",
      "    85     0.339578   1.016899   0.675346                  \n",
      "    86     0.344722   1.013842   0.675346                  \n",
      "    87     0.349719   1.040038   0.67036                   \n",
      "    88     0.356344   0.994441   0.674792                  \n",
      "    89     0.35672    0.99181    0.674238                  \n",
      "    90     0.355566   1.021624   0.672022                  \n",
      "    91     0.35841    1.036602   0.667036                  \n",
      "    92     0.355616   1.014931   0.668698                  \n",
      "    93     0.358697   1.024397   0.663712                  \n",
      "    94     0.364376   1.063409   0.652078                  \n",
      "    95     0.363321   1.022566   0.67036                   \n",
      "    96     0.356305   1.004503   0.67313                   \n",
      "    97     0.345567   1.030453   0.670914                  \n",
      "    98     0.342129   1.041169   0.668698                  \n",
      "    99     0.337876   1.041065   0.670914                  \n",
      "   100     0.340236   1.026158   0.66759                   \n",
      "   101     0.349477   1.029089   0.66759                   \n",
      "   102     0.34417    1.020714   0.668144                  \n",
      "   103     0.340046   1.061957   0.665928                  \n",
      "   104     0.333942   1.020097   0.674792                  \n",
      "   105     0.329257   1.048496   0.668698                  \n",
      "   106     0.329032   1.038583   0.665374                  \n",
      "   107     0.331766   1.041076   0.669806                  \n",
      "   108     0.327489   1.057572   0.677008                  \n",
      "   109     0.327932   1.078795   0.671468                  \n",
      "   110     0.330049   1.09739    0.6759                    \n",
      "   111     0.330206   1.049861   0.663712                  \n",
      "   112     0.319492   1.04155    0.670914                  \n",
      "   113     0.316506   1.057528   0.654294                  \n",
      "   114     0.315467   1.049533   0.668144                  \n",
      "   115     0.312568   1.032787   0.683657                  \n",
      "   116     0.317318   1.047391   0.674238                  \n",
      "   117     0.316806   1.076191   0.67036                   \n",
      "   118     0.316605   1.055032   0.671468                  \n",
      "   119     0.324136   1.035925   0.670914                  \n",
      "   120     0.333347   1.045222   0.66759                   \n",
      "   121     0.334964   1.049675   0.670914                  \n",
      "   122     0.330413   1.033352   0.6759                    \n",
      "   123     0.319765   1.045834   0.672022                  \n",
      "   124     0.315521   1.047078   0.67313                   \n",
      "   125     0.324083   1.04764    0.683102                  \n",
      "   126     0.325332   1.032072   0.675346                  \n",
      "   127     0.322049   1.045133   0.679224                  \n",
      "   128     0.314576   1.047111   0.674238                  \n",
      "   129     0.302711   1.049025   0.671468                  \n",
      "   130     0.293409   1.063001   0.679778                  \n",
      "   131     0.297385   1.084674   0.665928                  \n",
      "   132     0.305477   1.085271   0.670914                  \n",
      "   133     0.2999     1.064161   0.673684                  \n",
      "   134     0.292511   1.059991   0.675346                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.303532   1.078764   0.686427                  \n",
      "   136     0.31397    1.062164   0.672576                  \n",
      "   137     0.306613   1.051761   0.676454                  \n",
      "   138     0.306723   1.068705   0.666482                  \n",
      "   139     0.300891   1.063959   0.669806                  \n",
      "   140     0.309447   1.065982   0.677008                  \n",
      "   141     0.301948   1.066802   0.669252                  \n",
      "   142     0.293509   1.061185   0.674238                  \n",
      "   143     0.295704   1.092141   0.66759                   \n",
      "   144     0.29687    1.074323   0.667036                  \n",
      "   145     0.287737   1.061355   0.670914                  \n",
      "   146     0.285465   1.102027   0.675346                  \n",
      "   147     0.289965   1.06871    0.669252                  \n",
      "   148     0.288962   1.096386   0.668144                  \n",
      "   149     0.292737   1.066285   0.674792                  \n",
      "   150     0.297008   1.079361   0.680886                  \n",
      "   151     0.29886    1.076613   0.68144                   \n",
      "   152     0.299128   1.076989   0.681994                  \n",
      "   153     0.301759   1.068172   0.677562                  \n",
      "   154     0.302267   1.094297   0.66482                   \n",
      "   155     0.305938   1.077228   0.671468                  \n",
      "   156     0.30916    1.078011   0.673684                  \n",
      "   157     0.303805   1.04376    0.674238                  \n",
      "   158     0.293731   1.060857   0.671468                  \n",
      "   159     0.287595   1.076567   0.677562                  \n",
      "   160     0.292314   1.060613   0.67867                   \n",
      "   161     0.298396   1.056926   0.676454                  \n",
      "   162     0.288426   1.058945   0.675346                  \n",
      "   163     0.277702   1.078798   0.671468                  \n",
      "   164     0.28487    1.094796   0.673684                  \n",
      "   165     0.292735   1.101598   0.667036                  \n",
      "   166     0.291042   1.055453   0.683102                  \n",
      "   167     0.287295   1.072908   0.668698                  \n",
      "   168     0.292537   1.086595   0.674792                  \n",
      "   169     0.282643   1.095064   0.684765                  \n",
      "   170     0.292321   1.112259   0.683102                  \n",
      "   171     0.300275   1.056745   0.670914                  \n",
      "   172     0.303827   1.062752   0.680886                  \n",
      "   173     0.296153   1.057935   0.679778                  \n",
      "   174     0.299669   1.069105   0.67867                   \n",
      "   175     0.299033   1.065594   0.678116                  \n",
      "   176     0.292184   1.057706   0.68144                   \n",
      "   177     0.288247   1.096867   0.68144                   \n",
      "   178     0.280532   1.076866   0.684211                  \n",
      "   179     0.282624   1.09316    0.680332                  \n",
      "   180     0.291399   1.055874   0.686427                  \n",
      "   181     0.282668   1.056994   0.683657                  \n",
      "   182     0.271802   1.0763     0.677008                  \n",
      "   183     0.279193   1.058058   0.67313                   \n",
      "   184     0.280897   1.080969   0.673684                  \n",
      "   185     0.28121    1.087183   0.679224                  \n",
      "   186     0.27313    1.073215   0.673684                  \n",
      "   187     0.273931   1.070047   0.68144                   \n",
      "   188     0.282153   1.078068   0.67867                   \n",
      "   189     0.283654   1.085914   0.672576                  \n",
      "   190     0.283677   1.081007   0.672576                  \n",
      "   191     0.276345   1.074569   0.67036                   \n",
      "   192     0.270606   1.078646   0.673684                  \n",
      "   193     0.269123   1.084335   0.674238                  \n",
      "   194     0.268558   1.075529   0.680886                  \n",
      "   195     0.266702   1.085695   0.672022                  \n",
      "   196     0.25962    1.099041   0.6759                    \n",
      "   197     0.265057   1.084638   0.678116                  \n",
      "   198     0.264172   1.123101   0.670914                  \n",
      "   199     0.271419   1.103391   0.6759                    \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9913c6785654639a0516d6efb4d4cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eca5ef975b4aada7bb4d710718b1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.355585   1.186845   0.520222  \n",
      "    1      1.175654   1.102992   0.544598                 \n",
      "    2      1.076179   1.055588   0.572853                 \n",
      "    3      1.018614   1.025754   0.592244                 \n",
      "    4      0.960315   1.025083   0.580609                  \n",
      "    5      0.917994   0.983743   0.625485                  \n",
      "    6      0.867884   0.985132   0.618283                  \n",
      "    7      0.832758   0.977016   0.616066                  \n",
      "    8      0.800838   0.96179    0.612742                  \n",
      "    9      0.781683   0.945686   0.637673                  \n",
      "    10     0.758908   0.957411   0.628255                  \n",
      "    11     0.728326   0.964955   0.637673                  \n",
      "    12     0.714099   0.955316   0.641551                  \n",
      "    13     0.702583   0.927216   0.648753                  \n",
      "    14     0.682466   0.955354   0.639889                  \n",
      "    15     0.66021    0.933463   0.648753                  \n",
      "    16     0.6501     0.88843    0.647091                  \n",
      "    17     0.635014   0.932247   0.642105                  \n",
      "    18     0.615035   0.894679   0.649307                  \n",
      "    19     0.606705   0.911121   0.655402                  \n",
      "    20     0.602106   0.91492    0.648199                  \n",
      "    21     0.589177   0.907309   0.658172                  \n",
      "    22     0.581265   0.956749   0.634903                  \n",
      "    23     0.571712   0.951065   0.639335                  \n",
      "    24     0.56108    0.939136   0.658172                  \n",
      "    25     0.552993   0.903828   0.652632                  \n",
      "    26     0.53978    0.886041   0.668144                  \n",
      "    27     0.531554   0.908102   0.657618                  \n",
      "    28     0.523729   0.900246   0.662604                  \n",
      "    29     0.52391    0.920702   0.672022                  \n",
      "    30     0.515423   0.906557   0.674792                  \n",
      "    31     0.51656    0.907874   0.668698                  \n",
      "    32     0.513197   0.899817   0.667036                  \n",
      "    33     0.504426   0.892906   0.673684                  \n",
      "    34     0.496546   0.90643    0.669252                  \n",
      "    35     0.491637   0.901687   0.677008                  \n",
      "    36     0.484576   0.919662   0.669252                  \n",
      "    37     0.479772   0.892007   0.677562                  \n",
      "    38     0.46616    0.890717   0.681994                  \n",
      "    39     0.453724   0.940068   0.671468                  \n",
      "    40     0.456135   0.915151   0.680886                  \n",
      "    41     0.449078   0.973913   0.667036                  \n",
      "    42     0.451942   0.917147   0.663158                  \n",
      "    43     0.449315   0.941182   0.674792                  \n",
      "    44     0.435964   0.912671   0.67867                   \n",
      "    45     0.431894   0.989747   0.66759                   \n",
      "    46     0.436575   0.924581   0.671468                  \n",
      "    47     0.442841   0.925438   0.683102                  \n",
      "    48     0.439239   0.899776   0.670914                  \n",
      "    49     0.431461   0.928524   0.679778                  \n",
      "    50     0.428719   0.937389   0.679224                  \n",
      "    51     0.419148   0.943764   0.674238                  \n",
      "    52     0.41746    0.918314   0.685319                  \n",
      "    53     0.413321   0.939029   0.670914                  \n",
      "    54     0.417353   0.935441   0.673684                  \n",
      "    55     0.426616   0.90957    0.685319                  \n",
      "    56     0.41163    0.968535   0.66759                   \n",
      "    57     0.396383   0.956185   0.68144                   \n",
      "    58     0.400327   0.928148   0.687535                  \n",
      "    59     0.413763   0.981753   0.68144                   \n",
      "    60     0.398671   0.943349   0.675346                  \n",
      "    61     0.389238   0.939081   0.680886                  \n",
      "    62     0.381607   0.988      0.673684                  \n",
      "    63     0.379992   0.960873   0.676454                  \n",
      "    64     0.371928   0.991726   0.67036                   \n",
      "    65     0.370495   0.984011   0.677008                  \n",
      "    66     0.376759   0.958764   0.678116                  \n",
      "    67     0.381126   0.968603   0.679778                  \n",
      "    68     0.372849   0.933574   0.676454                  \n",
      "    69     0.36985    0.95425    0.677008                  \n",
      "    70     0.361323   0.96564    0.669806                  \n",
      "    71     0.365264   0.920113   0.683657                  \n",
      "    72     0.357097   0.950597   0.685319                  \n",
      "    73     0.362428   0.962791   0.68144                   \n",
      "    74     0.37753    0.988223   0.671468                  \n",
      "    75     0.390021   0.936686   0.685319                  \n",
      "    76     0.380089   0.941558   0.680332                  \n",
      "    77     0.367652   0.954932   0.684765                  \n",
      "    78     0.362806   0.979189   0.680886                  \n",
      "    79     0.359804   0.971193   0.683657                  \n",
      "    80     0.363323   0.991644   0.674238                  \n",
      "    81     0.36055    0.965902   0.683102                  \n",
      "    82     0.360411   0.990173   0.676454                  \n",
      "    83     0.356517   0.976683   0.6759                    \n",
      "    84     0.356018   0.96618    0.693629                  \n",
      "    85     0.349541   0.953101   0.679778                  \n",
      "    86     0.345814   0.991456   0.668698                  \n",
      "    87     0.356994   0.977951   0.695845                  \n",
      "    88     0.35637    0.944904   0.680886                  \n",
      "    89     0.363472   0.994144   0.683657                  \n",
      "    90     0.361892   1.014208   0.674238                  \n",
      "    91     0.361311   0.984863   0.671468                  \n",
      "    92     0.354194   0.959444   0.685319                  \n",
      "    93     0.343855   1.021911   0.674238                  \n",
      "    94     0.344288   0.98987    0.684211                  \n",
      "    95     0.353991   0.977333   0.676454                  \n",
      "    96     0.337353   0.956773   0.679224                  \n",
      "    97     0.325799   0.952737   0.675346                  \n",
      "    98     0.336012   0.987143   0.683657                  \n",
      "    99     0.341548   0.966739   0.684765                  \n",
      "   100     0.33369    0.972862   0.682548                  \n",
      "   101     0.335569   0.958128   0.689197                  \n",
      "   102     0.331784   0.943107   0.691413                  \n",
      "   103     0.327811   0.963935   0.679224                  \n",
      "   104     0.33611    0.977639   0.675346                  \n",
      "   105     0.336506   0.974021   0.6759                    \n",
      "   106     0.339943   0.944773   0.683102                  \n",
      "   107     0.34316    0.982976   0.679224                  \n",
      "   108     0.33832    0.946102   0.677562                  \n",
      "   109     0.330302   0.990799   0.68144                   \n",
      "   110     0.334978   0.945782   0.681994                  \n",
      "   111     0.330134   0.978005   0.682548                  \n",
      "   112     0.328902   0.94491    0.685873                  \n",
      "   113     0.332313   1.008511   0.676454                  \n",
      "   114     0.325448   0.993694   0.680332                  \n",
      "   115     0.325788   0.981927   0.686427                  \n",
      "   116     0.322232   0.95947    0.685319                  \n",
      "   117     0.315178   1.001272   0.671468                  \n",
      "   118     0.303197   0.991419   0.686981                  \n",
      "   119     0.309162   0.987862   0.675346                  \n",
      "   120     0.314996   0.988679   0.681994                  \n",
      "   121     0.31131    0.995766   0.681994                  \n",
      "   122     0.302541   0.991781   0.68144                   \n",
      "   123     0.307415   0.992192   0.688643                  \n",
      "   124     0.325996   1.016335   0.672022                  \n",
      "   125     0.3244     0.999314   0.677562                  \n",
      "   126     0.307875   0.994426   0.687535                  \n",
      "   127     0.294962   0.997731   0.67867                   \n",
      "   128     0.292783   0.977719   0.690305                  \n",
      "   129     0.29009    0.994106   0.685873                  \n",
      "   130     0.301233   1.001466   0.686427                  \n",
      "   131     0.307361   0.996944   0.691967                  \n",
      "   132     0.315275   0.988135   0.685319                  \n",
      "   133     0.306731   0.97688    0.687535                  \n",
      "   134     0.30419    0.988615   0.680332                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.304864   0.979311   0.68144                   \n",
      "   136     0.298242   0.98052    0.684211                  \n",
      "   137     0.292281   0.99487    0.679224                  \n",
      "   138     0.288817   1.015873   0.685319                  \n",
      "   139     0.287161   1.031892   0.684211                  \n",
      "   140     0.278852   1.019148   0.685319                  \n",
      "   141     0.281563   0.990004   0.684765                  \n",
      "   142     0.289575   1.015547   0.68144                   \n",
      "   143     0.301838   1.050328   0.683657                  \n",
      "   144     0.307643   0.98568    0.690859                  \n",
      "   145     0.289468   0.972053   0.691413                  \n",
      "   146     0.281877   0.991866   0.688643                  \n",
      "   147     0.278929   0.979582   0.688643                  \n",
      "   148     0.287795   0.97861    0.686981                  \n",
      "   149     0.287278   1.008588   0.686981                  \n",
      "   150     0.286867   1.003747   0.677562                  \n",
      "   151     0.282308   1.004346   0.685873                  \n",
      "   152     0.284523   1.032175   0.685873                  \n",
      "   153     0.293804   1.049476   0.686427                  \n",
      "   154     0.296271   1.043672   0.685319                  \n",
      "   155     0.301795   1.024578   0.689751                  \n",
      "   156     0.312341   1.019175   0.680332                  \n",
      "   157     0.302673   1.043596   0.681994                  \n",
      "   158     0.295414   1.00829    0.685319                  \n",
      "   159     0.286424   1.032371   0.682548                  \n",
      "   160     0.289692   1.036313   0.684765                  \n",
      "   161     0.277739   1.018834   0.687535                  \n",
      "   162     0.281452   1.031183   0.691967                  \n",
      "   163     0.284178   1.014059   0.686981                  \n",
      "   164     0.282708   1.011568   0.684211                  \n",
      "   165     0.283563   1.026095   0.681994                  \n",
      "   166     0.280343   1.017827   0.688089                  \n",
      "   167     0.287639   1.030002   0.682548                  \n",
      "   168     0.291669   1.03375    0.684765                  \n",
      "   169     0.288961   1.065593   0.682548                  \n",
      "   170     0.289401   1.042971   0.677008                  \n",
      "   171     0.285079   1.043819   0.682548                  \n",
      "   172     0.288836   1.039537   0.686981                  \n",
      "   173     0.283499   1.047473   0.684211                  \n",
      "   174     0.277943   1.027367   0.677562                  \n",
      "   175     0.287346   1.043221   0.688089                  \n",
      "   176     0.285862   1.057318   0.684765                  \n",
      "   177     0.278122   1.045399   0.685319                  \n",
      "   178     0.273488   1.034837   0.685873                  \n",
      "   179     0.26957    1.044532   0.674238                  \n",
      "   180     0.267647   1.040142   0.685873                  \n",
      "   181     0.272701   1.042114   0.684765                  \n",
      "   182     0.283219   1.033333   0.684211                  \n",
      "   183     0.276222   1.03149    0.686981                  \n",
      "   184     0.269657   1.046569   0.688089                  \n",
      "   185     0.269728   1.050273   0.676454                  \n",
      "   186     0.281269   1.049729   0.677562                  \n",
      "   187     0.280984   1.04517    0.6759                    \n",
      "   188     0.285628   1.04278    0.684765                  \n",
      "   189     0.280019   1.041829   0.679778                  \n",
      "   190     0.284657   1.040034   0.6759                    \n",
      "   191     0.278777   1.027297   0.68144                   \n",
      "   192     0.277686   1.029201   0.679224                  \n",
      "   193     0.27121    1.045086   0.680886                  \n",
      "   194     0.270879   1.059516   0.681994                  \n",
      "   195     0.266227   1.024385   0.687535                  \n",
      "   196     0.263297   1.023783   0.684765                  \n",
      "   197     0.267824   1.021911   0.686427                  \n",
      "   198     0.276225   1.055437   0.684211                  \n",
      "   199     0.268037   1.028985   0.688089                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc0d4aaa33349c981bb5856f0c52b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dropout=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034e2cc43a4642c6a3a413abaa39840d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.35204    1.192261   0.518006  \n",
      "    1      1.189391   1.109155   0.554571                 \n",
      "    2      1.094114   1.087779   0.560111                 \n",
      "    3      1.022332   1.051953   0.592798                 \n",
      "    4      0.962953   1.029739   0.587258                  \n",
      "    5      0.925758   1.015493   0.598892                  \n",
      "    6      0.889812   0.998809   0.614404                  \n",
      "    7      0.858178   0.978511   0.616621                  \n",
      "    8      0.816681   0.979643   0.624931                  \n",
      "    9      0.785683   0.996029   0.622715                  \n",
      "    10     0.769408   0.945062   0.639335                  \n",
      "    11     0.755164   0.957911   0.619391                  \n",
      "    12     0.72995    0.978325   0.636011                  \n",
      "    13     0.705256   0.961553   0.642105                  \n",
      "    14     0.677293   0.973828   0.640443                  \n",
      "    15     0.666116   0.954322   0.651524                  \n",
      "    16     0.659747   0.95313    0.653186                  \n",
      "    17     0.65847    0.970404   0.642105                  \n",
      "    18     0.642911   0.938386   0.646537                  \n",
      "    19     0.630455   0.931722   0.657618                  \n",
      "    20     0.63318    0.956666   0.644875                  \n",
      "    21     0.615694   0.94227    0.650416                  \n",
      "    22     0.610327   0.94523    0.665374                  \n",
      "    23     0.600646   0.98065    0.642105                  \n",
      "    24     0.594582   0.919439   0.672576                  \n",
      "    25     0.582761   0.925369   0.662604                  \n",
      "    26     0.56653    0.935763   0.660388                  \n",
      "    27     0.550471   0.96741    0.652078                  \n",
      "    28     0.553931   0.95156    0.669252                  \n",
      "    29     0.540094   0.960806   0.66205                   \n",
      "    30     0.531766   0.945937   0.66482                   \n",
      "    31     0.524551   0.966157   0.65651                   \n",
      "    32     0.513477   0.951191   0.66482                   \n",
      "    33     0.500263   1.019781   0.657064                  \n",
      "    34     0.499262   0.956289   0.65651                   \n",
      "    35     0.501139   0.977887   0.655402                  \n",
      "    36     0.49336    0.918899   0.680332                  \n",
      "    37     0.492891   0.97808    0.675346                  \n",
      "    38     0.485406   0.958729   0.668144                  \n",
      "    39     0.476991   0.936804   0.677008                  \n",
      "    40     0.472264   0.964879   0.672022                  \n",
      "    41     0.481373   0.989818   0.665374                  \n",
      "    42     0.491817   0.988184   0.660942                  \n",
      "    43     0.495651   0.941393   0.668698                  \n",
      "    44     0.480062   0.969623   0.667036                  \n",
      "    45     0.461603   0.942011   0.672576                  \n",
      "    46     0.445196   0.963165   0.679778                  \n",
      "    47     0.443685   0.986386   0.670914                  \n",
      "    48     0.435069   0.997512   0.663712                  \n",
      "    49     0.425461   0.989057   0.67313                   \n",
      "    50     0.426646   1.01497    0.668698                  \n",
      "    51     0.423508   0.982356   0.672576                  \n",
      "    52     0.423503   0.961993   0.677562                  \n",
      "    53     0.432239   0.998189   0.672576                  \n",
      "    54     0.428121   0.97753    0.67036                   \n",
      "    55     0.425368   0.975003   0.677562                  \n",
      "    56     0.418375   0.960201   0.683102                  \n",
      "    57     0.417079   0.978196   0.67313                   \n",
      "    58     0.414357   0.99919    0.67036                   \n",
      "    59     0.412085   1.025599   0.672022                  \n",
      "    60     0.410044   0.982986   0.669252                  \n",
      "    61     0.405359   0.957925   0.689197                  \n",
      "    62     0.407246   0.99114    0.681994                  \n",
      "    63     0.413228   1.012871   0.669252                  \n",
      "    64     0.393837   0.98609    0.684765                  \n",
      "    65     0.385618   0.99418    0.680886                  \n",
      "    66     0.384951   0.995365   0.674238                  \n",
      "    67     0.398001   1.013105   0.677008                  \n",
      "    68     0.383766   0.986083   0.681994                  \n",
      "    69     0.374036   0.973392   0.687535                  \n",
      "    70     0.375334   0.988141   0.67313                   \n",
      "    71     0.38049    1.002059   0.693629                  \n",
      "    72     0.391198   0.982095   0.680886                  \n",
      "    73     0.388618   0.996325   0.685873                  \n",
      "    74     0.382328   0.991695   0.6759                    \n",
      "    75     0.372595   0.982898   0.683102                  \n",
      "    76     0.362331   1.016043   0.688089                  \n",
      "    77     0.363796   0.994949   0.686981                  \n",
      "    78     0.36283    1.024065   0.692521                  \n",
      "    79     0.35634    1.069663   0.688643                  \n",
      "    80     0.355833   1.041499   0.678116                  \n",
      "    81     0.363438   1.01617    0.684211                  \n",
      "    82     0.359476   1.015709   0.689197                  \n",
      "    83     0.360832   1.033216   0.676454                  \n",
      "    84     0.366627   1.020613   0.683657                  \n",
      "    85     0.365017   1.014152   0.680886                  \n",
      "    86     0.364903   0.998302   0.682548                  \n",
      "    87     0.368714   1.036519   0.680332                  \n",
      "    88     0.358617   1.038464   0.680332                  \n",
      "    89     0.369618   1.004505   0.692521                  \n",
      "    90     0.360312   1.024113   0.682548                  \n",
      "    91     0.346892   1.027676   0.683102                  \n",
      "    92     0.341109   1.019853   0.68144                   \n",
      "    93     0.343106   1.026429   0.677562                  \n",
      "    94     0.334446   1.055043   0.677008                  \n",
      "    95     0.33387    1.038184   0.688089                  \n",
      "    96     0.347338   1.047485   0.685319                  \n",
      "    97     0.34423    1.022      0.692521                  \n",
      "    98     0.343277   1.024842   0.688089                  \n",
      "    99     0.337226   1.048138   0.689197                  \n",
      "   100     0.342384   1.025328   0.691413                  \n",
      "   101     0.342681   1.046008   0.681994                  \n",
      "   102     0.337488   1.069979   0.683102                  \n",
      "   103     0.339124   1.012379   0.686981                  \n",
      "   104     0.328621   1.045924   0.685319                  \n",
      "   105     0.340006   1.042921   0.682548                  \n",
      "   106     0.34206    1.056389   0.674238                  \n",
      "   107     0.329219   1.02326    0.67867                   \n",
      "   108     0.332525   1.055881   0.689197                  \n",
      "   109     0.325549   1.028325   0.689197                  \n",
      "   110     0.314258   1.03872    0.683657                  \n",
      "   111     0.309972   1.060178   0.68144                   \n",
      "   112     0.311923   1.066851   0.684765                  \n",
      "   113     0.324255   1.043332   0.681994                  \n",
      "   114     0.318752   1.061844   0.679224                  \n",
      "   115     0.322024   1.061496   0.677008                  \n",
      "   116     0.308921   1.072506   0.679224                  \n",
      "   117     0.308114   1.077111   0.680332                  \n",
      "   118     0.302521   1.052826   0.680332                  \n",
      "   119     0.318213   1.065879   0.685873                  \n",
      "   120     0.317418   1.057388   0.683102                  \n",
      "   121     0.313263   1.06037    0.687535                  \n",
      "   122     0.313005   1.059633   0.689197                  \n",
      "   123     0.310744   1.059706   0.682548                  \n",
      "   124     0.318637   1.05635    0.67867                   \n",
      "   125     0.325739   1.066597   0.689197                  \n",
      "   126     0.320643   1.053211   0.689751                  \n",
      "   127     0.321501   1.054495   0.680332                  \n",
      "   128     0.324071   1.057099   0.686981                  \n",
      "   129     0.319001   1.057595   0.683102                  \n",
      "   130     0.312197   1.040993   0.689197                  \n",
      "   131     0.311904   1.063426   0.687535                  \n",
      "   132     0.312531   1.059566   0.690859                  \n",
      "   133     0.301453   1.050953   0.690859                  \n",
      "   134     0.295524   1.025461   0.697507                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.295956   1.055574   0.690305                  \n",
      "   136     0.296088   1.053147   0.690859                  \n",
      "   137     0.299405   1.06094    0.691413                  \n",
      "   138     0.301928   1.021276   0.696953                  \n",
      "   139     0.310559   1.066077   0.691413                  \n",
      "   140     0.309612   1.067531   0.687535                  \n",
      "   141     0.310599   1.071198   0.689751                  \n",
      "   142     0.31927    1.08054    0.686981                  \n",
      "   143     0.311524   1.044939   0.689197                  \n",
      "   144     0.307729   1.046085   0.688089                  \n",
      "   145     0.305293   1.039591   0.693629                  \n",
      "   146     0.304393   1.055086   0.690305                  \n",
      "   147     0.294417   1.068952   0.694737                  \n",
      "   148     0.285283   1.087706   0.693075                  \n",
      "   149     0.284763   1.051659   0.68144                   \n",
      "   150     0.28212    1.066846   0.700277                  \n",
      "   151     0.284721   1.070317   0.694183                  \n",
      "   152     0.290224   1.073785   0.695291                  \n",
      "   153     0.284668   1.087614   0.690859                  \n",
      "   154     0.288382   1.073575   0.692521                  \n",
      "   155     0.291665   1.081371   0.689751                  \n",
      "   156     0.28331    1.074938   0.691967                  \n",
      "   157     0.274223   1.072912   0.701939                  \n",
      "   158     0.28019    1.06777    0.694737                  \n",
      "   159     0.283766   1.095233   0.692521                  \n",
      "   160     0.28265    1.074531   0.689751                  \n",
      "   161     0.278639   1.082291   0.689751                  \n",
      "   162     0.287552   1.077857   0.684765                  \n",
      "   163     0.281711   1.071405   0.689197                  \n",
      "   164     0.281439   1.067451   0.695845                  \n",
      "   165     0.284589   1.09207    0.691967                  \n",
      "   166     0.292493   1.064423   0.693075                  \n",
      "   167     0.286226   1.084113   0.694737                  \n",
      "   168     0.284714   1.073746   0.693075                  \n",
      "   169     0.290356   1.090245   0.694737                  \n",
      "   170     0.292987   1.119271   0.691967                  \n",
      "   171     0.303392   1.095037   0.696953                  \n",
      "   172     0.295418   1.121628   0.690859                  \n",
      "   173     0.280561   1.104686   0.686981                  \n",
      "   174     0.281721   1.114464   0.685319                  \n",
      "   175     0.281398   1.114255   0.684211                  \n",
      "   176     0.27506    1.103407   0.685873                  \n",
      "   177     0.268435   1.078818   0.690305                  \n",
      "   178     0.282031   1.108006   0.692521                  \n",
      "   179     0.283097   1.091833   0.692521                  \n",
      "   180     0.284728   1.089272   0.690859                  \n",
      "   181     0.28096    1.085628   0.692521                  \n",
      "   182     0.277371   1.079906   0.700831                  \n",
      "   183     0.280653   1.079677   0.693629                  \n",
      "   184     0.285656   1.107435   0.683657                  \n",
      "   185     0.279485   1.107551   0.693629                  \n",
      "   186     0.281122   1.091111   0.686427                  \n",
      "   187     0.280916   1.077947   0.692521                  \n",
      "   188     0.279182   1.095175   0.694183                  \n",
      "   189     0.283962   1.098653   0.695291                  \n",
      "   190     0.282857   1.086699   0.691413                  \n",
      "   191     0.279363   1.08221    0.695845                  \n",
      "   192     0.271069   1.088658   0.689197                  \n",
      "   193     0.276436   1.083004   0.686427                  \n",
      "   194     0.276255   1.076724   0.691413                  \n",
      "   195     0.278535   1.109109   0.682548                  \n",
      "   196     0.278549   1.128067   0.684765                  \n",
      "   197     0.279339   1.108167   0.691413                  \n",
      "   198     0.272539   1.088013   0.694183                  \n",
      "   199     0.26753    1.093746   0.690305                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4ffaeef361492088a017daa775a7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dropout=0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6cce25f2694bffbdf272935f9a4f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.414224   1.261176   0.493075  \n",
      "    1      1.255934   1.165494   0.539058                 \n",
      "    2      1.17718    1.127533   0.557895                 \n",
      "    3      1.123647   1.103429   0.553463                 \n",
      "    4      1.080977   1.091929   0.562881                 \n",
      "    5      1.052019   1.070601   0.569529                 \n",
      "    6      1.03493    1.04334    0.578947                 \n",
      "    7      1.007769   1.015912   0.59723                  \n",
      "    8      0.980961   1.01282    0.599446                  \n",
      "    9      0.960035   1.002191   0.604432                  \n",
      "    10     0.936893   0.98851    0.6                       \n",
      "    11     0.927014   0.981329   0.606648                  \n",
      "    12     0.912847   0.975862   0.602216                  \n",
      "    13     0.908338   0.98514    0.601662                  \n",
      "    14     0.891627   0.957037   0.614404                  \n",
      "    15     0.877034   0.956927   0.617729                  \n",
      "    16     0.86503    0.951676   0.632687                  \n",
      "    17     0.860436   0.987625   0.619391                  \n",
      "    18     0.851188   0.959553   0.621607                  \n",
      "    19     0.830661   0.924335   0.626593                  \n",
      "    20     0.819888   0.931353   0.635457                  \n",
      "    21     0.803961   0.937186   0.632687                  \n",
      "    22     0.797149   0.939199   0.631579                  \n",
      "    23     0.799907   0.934727   0.629917                  \n",
      "    24     0.787052   0.929828   0.642105                  \n",
      "    25     0.783219   0.929651   0.636011                  \n",
      "    26     0.7738     0.925424   0.638781                  \n",
      "    27     0.765367   0.924391   0.638781                  \n",
      "    28     0.757519   0.921923   0.638227                  \n",
      "    29     0.756923   0.903648   0.642659                  \n",
      "    30     0.761676   0.927094   0.647091                  \n",
      "    31     0.752794   0.913317   0.649861                  \n",
      "    32     0.753527   0.888444   0.652078                  \n",
      "    33     0.73659    0.902123   0.642659                  \n",
      "    34     0.739804   0.919546   0.652632                  \n",
      "    35     0.726575   0.902097   0.649861                  \n",
      "    36     0.711676   0.89289    0.657064                  \n",
      "    37     0.698573   0.913559   0.655956                  \n",
      "    38     0.696692   0.908266   0.66205                   \n",
      "    39     0.696953   0.912253   0.652078                  \n",
      "    40     0.696885   0.880012   0.654294                  \n",
      "    41     0.68787    0.891284   0.660942                  \n",
      "    42     0.677028   0.911555   0.662604                  \n",
      "    43     0.672943   0.886647   0.658726                  \n",
      "    44     0.675663   0.887497   0.660388                  \n",
      "    45     0.674188   0.878988   0.668144                  \n",
      "    46     0.672551   0.88388    0.669806                  \n",
      "    47     0.670481   0.886148   0.666482                  \n",
      "    48     0.66691    0.894201   0.663712                  \n",
      "    49     0.659367   0.888974   0.663158                  \n",
      "    50     0.649903   0.891229   0.660942                  \n",
      "    51     0.646326   0.886573   0.665928                  \n",
      "    52     0.642755   0.89819    0.662604                  \n",
      "    53     0.64715    0.880883   0.668144                  \n",
      "    54     0.649542   0.89644    0.665374                  \n",
      "    55     0.64448    0.886607   0.660388                  \n",
      "    56     0.639904   0.863739   0.673684                  \n",
      "    57     0.63548    0.895248   0.663158                  \n",
      "    58     0.639459   0.89006    0.679224                  \n",
      "    59     0.619548   0.899079   0.660942                  \n",
      "    60     0.614693   0.898159   0.665374                  \n",
      "    61     0.613103   0.892101   0.66482                   \n",
      "    62     0.613433   0.911837   0.668698                  \n",
      "    63     0.623887   0.873597   0.677008                  \n",
      "    64     0.624041   0.885773   0.663712                  \n",
      "    65     0.619133   0.881174   0.670914                  \n",
      "    66     0.603197   0.880015   0.666482                  \n",
      "    67     0.609549   0.877663   0.667036                  \n",
      "    68     0.602354   0.874135   0.67313                   \n",
      "    69     0.593968   0.874452   0.675346                  \n",
      "    70     0.589154   0.910017   0.668144                  \n",
      "    71     0.579752   0.907264   0.668698                  \n",
      "    72     0.582553   0.886512   0.674238                  \n",
      "    73     0.574685   0.878667   0.678116                  \n",
      "    74     0.581746   0.897851   0.679224                  \n",
      "    75     0.58179    0.885807   0.670914                  \n",
      "    76     0.577155   0.890173   0.672576                  \n",
      "    77     0.5687     0.865878   0.668144                  \n",
      "    78     0.561321   0.904707   0.66759                   \n",
      "    79     0.567673   0.881413   0.669806                  \n",
      "    80     0.564831   0.886731   0.672576                  \n",
      "    81     0.563612   0.893533   0.665374                  \n",
      "    82     0.569416   0.882669   0.674238                  \n",
      "    83     0.563523   0.880258   0.678116                  \n",
      "    84     0.554421   0.861229   0.672576                  \n",
      "    85     0.557626   0.885696   0.674238                  \n",
      "    86     0.545589   0.889448   0.667036                  \n",
      "    87     0.54773    0.875793   0.67036                   \n",
      "    88     0.53792    0.889023   0.665928                  \n",
      "    89     0.527885   0.87336    0.6759                    \n",
      "    90     0.522465   0.873318   0.676454                  \n",
      "    91     0.522681   0.892301   0.678116                  \n",
      "    92     0.525015   0.903084   0.666482                  \n",
      "    93     0.55081    0.890015   0.670914                  \n",
      "    94     0.545502   0.902645   0.673684                  \n",
      "    95     0.536518   0.880764   0.678116                  \n",
      "    96     0.537503   0.89932    0.674238                  \n",
      "    97     0.531016   0.89845    0.666482                  \n",
      "    98     0.529331   0.883545   0.672022                  \n",
      "    99     0.528182   0.891215   0.66759                   \n",
      "   100     0.517669   0.901472   0.668698                  \n",
      "   101     0.515228   0.891409   0.672022                  \n",
      "   102     0.505977   0.883755   0.674792                  \n",
      "   103     0.510663   0.893549   0.669252                  \n",
      "   104     0.510801   0.879743   0.67867                   \n",
      "   105     0.500336   0.890006   0.67036                   \n",
      "   106     0.499162   0.893932   0.677008                  \n",
      "   107     0.498224   0.894533   0.671468                  \n",
      "   108     0.507583   0.890861   0.6759                    \n",
      "   109     0.493057   0.901182   0.669252                  \n",
      "   110     0.486437   0.897856   0.671468                  \n",
      "   111     0.496169   0.901148   0.669806                  \n",
      "   112     0.494198   0.916574   0.673684                  \n",
      "   113     0.486797   0.893503   0.677562                  \n",
      "   114     0.486666   0.882108   0.677562                  \n",
      "   115     0.481625   0.89997    0.677562                  \n",
      "   116     0.484726   0.898635   0.677562                  \n",
      "   117     0.487545   0.918976   0.677008                  \n",
      "   118     0.488225   0.915398   0.678116                  \n",
      "   119     0.488588   0.889903   0.684211                  \n",
      "   120     0.481091   0.890335   0.685319                  \n",
      "   121     0.478407   0.926981   0.67036                   \n",
      "   122     0.477803   0.894493   0.679778                  \n",
      "   123     0.485216   0.907196   0.679224                  \n",
      "   124     0.482221   0.894857   0.685319                  \n",
      "   125     0.473108   0.901045   0.674238                  \n",
      "   126     0.46833    0.896103   0.681994                  \n",
      " 16%|        | 6/37 [00:12<01:04,  2.07s/it, loss=0.467]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7f5f78d497e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dropouts = [0, 0, 0.2, 0.2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mv_a1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfld_loop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_3cls_pf_200ep.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mv_a2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfld_loop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_3cls_pf_200ep.model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-fc466fcea621>\u001b[0m in \u001b[0;36mkfld_loop3\u001b[0;34m(k, epochs, name, precomp, bs, dropouts, label_csv, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropout='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecomp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_DO_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxtra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/models/resnext_101_64x4d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLambdaMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambdaBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLambdaReduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambdaBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl1/fastai/models/resnext_101_64x4d.py\u001b[0m in \u001b[0;36mforward_prepare\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 282\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropouts = [0,0.2,0.4,0.5,0.6]\n",
    "label_csv = f'{PATH}5labels.csv'\n",
    "n = len(list(open(label_csv))) - 1\n",
    "#dropouts = [0, 0, 0.2, 0.2]\n",
    "v_a1, learn1 = kfld_loop3(3, 200, '_3cls_pf_200ep.model', False, 200, dropouts, label_csv, n) \n",
    "v_a2, learn2 = kfld_loop3(3, 500, '_3cls_pf_200ep.model', False, 200, dropouts, label_csv, n) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1f7299e3b94b42badca74b601e36fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7aa3175e6048aa87101d1de71fe2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.070548   1.669185   0.429075  \n",
      "    1      1.76888    1.556218   0.46652                  \n",
      "    2      1.645123   1.498228   0.482379                  \n",
      "    3      1.545225   1.455022   0.494273                  \n",
      "    4      1.480391   1.424      0.497357                 \n",
      "    5      1.422637   1.385548   0.511454                 \n",
      "    6      1.381897   1.341969   0.517181                 \n",
      "    7      1.324599   1.327606   0.532159                 \n",
      "    8      1.290708   1.325242   0.532159                 \n",
      "    9      1.267365   1.307441   0.54141                   \n",
      "    10     1.229282   1.292712   0.534361                  \n",
      "    11     1.208318   1.281906   0.547137                  \n",
      "    12     1.167715   1.278463   0.546696                 \n",
      "    13     1.14537    1.268767   0.548018                 \n",
      "    14     1.109101   1.2657     0.548018                 \n",
      "    15     1.101349   1.247311   0.562555                 \n",
      "    16     1.082755   1.267614   0.553744                 \n",
      "    17     1.063979   1.247473   0.559912                 \n",
      "    18     1.034137   1.240732   0.573128                 \n",
      "    19     1.034894   1.234701   0.554626                 \n",
      "    20     1.025751   1.244344   0.559912                 \n",
      "    21     0.997286   1.230501   0.572687                  \n",
      "    22     0.975862   1.228382   0.568282                  \n",
      "    23     0.965878   1.238517   0.555947                  \n",
      "    24     0.948999   1.229392   0.565198                  \n",
      "    25     0.93679    1.231834   0.570044                  \n",
      "    26     0.93418    1.248596   0.555066                  \n",
      "    27     0.913114   1.232479   0.56696                   \n",
      "    28     0.909396   1.233259   0.562115                  \n",
      "    29     0.896538   1.227418   0.574449                  \n",
      "    30     0.857907   1.217581   0.577533                  \n",
      "    31     0.861527   1.224589   0.56652                   \n",
      "    32     0.85462    1.237073   0.565639                  \n",
      "    33     0.840957   1.229171   0.565639                  \n",
      "    34     0.826524   1.214126   0.574009                  \n",
      "    35     0.826948   1.241328   0.577093                  \n",
      "    36     0.809648   1.236241   0.562996                  \n",
      "    37     0.791149   1.239395   0.573128                  \n",
      "    38     0.802589   1.249129   0.56652                   \n",
      "    39     0.782439   1.222229   0.578414                  \n",
      "    40     0.77204    1.235456   0.570925                  \n",
      "    41     0.780646   1.235673   0.572687                  \n",
      "    42     0.768195   1.248821   0.57533                   \n",
      "    43     0.759036   1.236744   0.57533                   \n",
      "    44     0.744036   1.236491   0.576652                  \n",
      "    45     0.73689    1.265776   0.578855                  \n",
      "    46     0.728577   1.242155   0.577093                  \n",
      "    47     0.709852   1.259064   0.571806                  \n",
      "    48     0.704631   1.24679    0.574009                  \n",
      "    49     0.702818   1.256391   0.574009                  \n",
      "    50     0.702037   1.272853   0.576211                  \n",
      "    51     0.687737   1.249614   0.575771                  \n",
      "    52     0.676178   1.258206   0.572247                  \n",
      "    53     0.68634    1.281474   0.573568                  \n",
      "    54     0.668184   1.276732   0.580617                  \n",
      "    55     0.685294   1.269072   0.585463                  \n",
      "    56     0.677062   1.268034   0.578414                  \n",
      "    57     0.671846   1.245691   0.58326                   \n",
      "    58     0.65749    1.267036   0.579736                  \n",
      "    59     0.654705   1.27405    0.577093                  \n",
      "    60     0.639779   1.284162   0.573568                  \n",
      "    61     0.636122   1.293149   0.57533                   \n",
      "    62     0.623986   1.294272   0.577974                  \n",
      "    63     0.622222   1.290746   0.577533                  \n",
      "    64     0.631609   1.280938   0.580176                  \n",
      "    65     0.606202   1.298517   0.57533                   \n",
      "    66     0.607577   1.301216   0.576652                  \n",
      "    67     0.608291   1.314219   0.581498                  \n",
      "    68     0.61089    1.297083   0.585903                  \n",
      "    69     0.611714   1.299743   0.586344                  \n",
      "    70     0.603989   1.292454   0.581057                  \n",
      "    71     0.596725   1.32695    0.574009                  \n",
      "    72     0.599184   1.330556   0.586344                  \n",
      "    73     0.602055   1.30428    0.584581                  \n",
      "    74     0.586397   1.317852   0.579736                  \n",
      "    75     0.573427   1.319958   0.577974                  \n",
      "    76     0.588207   1.316612   0.582819                  \n",
      "    77     0.581229   1.319798   0.588546                  \n",
      "    78     0.571777   1.342913   0.581498                  \n",
      "    79     0.568303   1.319494   0.581057                  \n",
      "    80     0.567248   1.329763   0.579736                  \n",
      "    81     0.565798   1.332212   0.581057                  \n",
      "    82     0.569499   1.323464   0.588546                  \n",
      "    83     0.562022   1.336884   0.586784                  \n",
      "    84     0.558586   1.348433   0.581498                  \n",
      "    85     0.553271   1.336645   0.587225                  \n",
      "    86     0.541567   1.312578   0.580176                  \n",
      "    87     0.545476   1.337063   0.585903                  \n",
      "    88     0.534421   1.333658   0.586784                  \n",
      "    89     0.537711   1.340964   0.582379                  \n",
      "    90     0.534839   1.348511   0.581938                  \n",
      "    91     0.517404   1.349101   0.588987                  \n",
      "    92     0.5294     1.337855   0.582819                  \n",
      "    93     0.529848   1.348112   0.579295                  \n",
      "    94     0.526318   1.344196   0.588106                  \n",
      "    95     0.508803   1.361369   0.586344                  \n",
      "    96     0.525605   1.361392   0.581938                  \n",
      "    97     0.52395    1.374073   0.587665                  \n",
      "    98     0.517148   1.367663   0.574449                  \n",
      "    99     0.508821   1.363858   0.581498                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98535abca6d047a1a9347eb231ec7f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c043902367ce49fa9acfab98e3b8c19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.082087   1.680048   0.448018  \n",
      "    1      1.790978   1.573119   0.445815                 \n",
      "    2      1.633364   1.496212   0.478414                 \n",
      "    3      1.541974   1.448939   0.488987                 \n",
      "    4      1.492486   1.421354   0.502203                 \n",
      "    5      1.438499   1.395803   0.511894                 \n",
      "    6      1.383618   1.377353   0.508811                 \n",
      "    7      1.351633   1.365814   0.501762                 \n",
      "    8      1.306841   1.33187    0.528634                 \n",
      "    9      1.276673   1.331444   0.534361                 \n",
      "    10     1.234611   1.311445   0.534361                 \n",
      "    11     1.201428   1.3091     0.530396                 \n",
      "    12     1.181753   1.291834   0.539207                 \n",
      "    13     1.172887   1.275719   0.545815                 \n",
      "    14     1.148957   1.282912   0.546696                 \n",
      "    15     1.109257   1.271912   0.543172                 \n",
      "    16     1.099581   1.269104   0.551982                 \n",
      "    17     1.067157   1.266187   0.559031                 \n",
      "    18     1.060859   1.261197   0.557709                 \n",
      "    19     1.038969   1.234647   0.562996                 \n",
      "    20     1.021561   1.238359   0.557709                 \n",
      "    21     1.010328   1.243594   0.554626                  \n",
      "    22     0.992421   1.240141   0.562115                  \n",
      "    23     0.972052   1.237733   0.555066                  \n",
      "    24     0.965946   1.236956   0.550661                  \n",
      "    25     0.94041    1.231184   0.553744                  \n",
      "    26     0.940847   1.225769   0.568282                  \n",
      "    27     0.934773   1.230642   0.566079                  \n",
      "    28     0.900665   1.264242   0.559031                  \n",
      "    29     0.898406   1.235651   0.564758                  \n",
      "    30     0.878996   1.237124   0.56652                   \n",
      "    31     0.872598   1.231957   0.563877                  \n",
      "    32     0.871906   1.229773   0.567401                  \n",
      "    33     0.86188    1.252807   0.554626                  \n",
      "    34     0.846314   1.245944   0.556388                  \n",
      "    35     0.830004   1.259591   0.559031                  \n",
      "    36     0.824792   1.217773   0.572687                  \n",
      "    37     0.804777   1.246344   0.565639                  \n",
      "    38     0.793097   1.249604   0.570044                  \n",
      "    39     0.779065   1.246571   0.568282                  \n",
      "    40     0.780615   1.24613    0.571366                  \n",
      "    41     0.767487   1.251246   0.573128                  \n",
      "    42     0.775359   1.251243   0.56696                   \n",
      "    43     0.768438   1.248403   0.56652                   \n",
      "    44     0.759263   1.255526   0.573128                  \n",
      "    45     0.735464   1.244421   0.572687                  \n",
      "    46     0.73502    1.242629   0.572687                  \n",
      "    47     0.729917   1.247973   0.570925                  \n",
      "    48     0.720608   1.255668   0.571366                  \n",
      "    49     0.717724   1.265483   0.565639                  \n",
      "    50     0.701853   1.271361   0.562115                  \n",
      "    51     0.698755   1.281674   0.565198                  \n",
      "    52     0.699338   1.266948   0.56652                   \n",
      "    53     0.696096   1.263962   0.562996                  \n",
      "    54     0.676007   1.267152   0.57489                   \n",
      "    55     0.674603   1.273301   0.570485                  \n",
      "    56     0.669297   1.292154   0.579295                  \n",
      "    57     0.66781    1.278089   0.571366                  \n",
      "    58     0.657483   1.265588   0.572247                  \n",
      "    59     0.640502   1.286131   0.572247                  \n",
      "    60     0.65851    1.273859   0.574009                  \n",
      "    61     0.654834   1.280584   0.574449                  \n",
      "    62     0.640052   1.291755   0.581057                  \n",
      "    63     0.634973   1.295771   0.578414                  \n",
      "    64     0.624108   1.278348   0.580617                  \n",
      "    65     0.628796   1.294121   0.574449                  \n",
      "    66     0.623765   1.298513   0.575771                  \n",
      "    67     0.625182   1.312995   0.581057                  \n",
      "    68     0.61409    1.30421    0.576211                  \n",
      "    69     0.610714   1.299369   0.574449                  \n",
      "    70     0.59732    1.303085   0.581498                  \n",
      "    71     0.592681   1.318062   0.571806                  \n",
      "    72     0.594813   1.311456   0.576211                  \n",
      "    73     0.593784   1.305031   0.577533                  \n",
      "    74     0.582318   1.311525   0.584581                  \n",
      "    75     0.577402   1.318194   0.580617                  \n",
      "    76     0.578964   1.31475    0.570485                  \n",
      "    77     0.569062   1.328922   0.57489                   \n",
      "    78     0.583728   1.323458   0.576652                  \n",
      "    79     0.566984   1.319646   0.572687                  \n",
      "    80     0.553818   1.330268   0.574449                  \n",
      "    81     0.561557   1.321542   0.582819                  \n",
      "    82     0.560519   1.313866   0.579736                  \n",
      "    83     0.555644   1.320708   0.578855                  \n",
      "    84     0.563933   1.330707   0.58326                   \n",
      "    85     0.562533   1.319714   0.579295                  \n",
      "    86     0.551492   1.326786   0.57533                   \n",
      "    87     0.542695   1.344191   0.570485                  \n",
      "    88     0.546974   1.353783   0.578414                  \n",
      "    89     0.548072   1.360694   0.579736                  \n",
      "    90     0.555586   1.348176   0.576652                  \n",
      "    91     0.54847    1.344415   0.576652                  \n",
      "    92     0.531599   1.372165   0.569163                  \n",
      "    93     0.521741   1.345386   0.579295                  \n",
      "    94     0.535891   1.352297   0.580617                  \n",
      "    95     0.525201   1.356886   0.581498                  \n",
      "    96     0.526208   1.353858   0.579736                  \n",
      "    97     0.524389   1.350399   0.576211                  \n",
      "    98     0.532762   1.345039   0.580617                  \n",
      "    99     0.535307   1.367706   0.577093                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe74efb0b064340934837b9f50f17a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fea627bd2fa4c41bdd1152e51d7d1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.075945   1.67589    0.43348   \n",
      "    1      1.768355   1.555436   0.463436                 \n",
      "    2      1.628673   1.482706   0.481938                 \n",
      "    3      1.542327   1.453784   0.486784                 \n",
      "    4      1.473394   1.400499   0.509692                 \n",
      "    5      1.419219   1.368242   0.51674                  \n",
      "    6      1.36255    1.365166   0.518502                 \n",
      "    7      1.318509   1.335945   0.533921                 \n",
      "    8      1.288755   1.330634   0.526432                 \n",
      "    9      1.255152   1.320169   0.530396                 \n",
      "    10     1.224379   1.296715   0.548018                 \n",
      "    11     1.200319   1.280242   0.557709                 \n",
      "    12     1.168186   1.265933   0.559912                 \n",
      "    13     1.145757   1.264515   0.555947                 \n",
      "    14     1.124273   1.275745   0.555066                 \n",
      "    15     1.101011   1.265241   0.549339                 \n",
      "    16     1.074317   1.266828   0.554185                 \n",
      "    17     1.062034   1.260732   0.551542                 \n",
      "    18     1.053987   1.242983   0.554626                 \n",
      "    19     1.030799   1.245646   0.563436                 \n",
      "    20     1.017224   1.249517   0.559471                  \n",
      "    21     0.996457   1.252782   0.559912                  \n",
      "    22     0.995468   1.243151   0.571366                  \n",
      "    23     0.973596   1.236776   0.567401                  \n",
      "    24     0.960767   1.216651   0.574449                  \n",
      "    25     0.948706   1.219184   0.56652                   \n",
      "    26     0.922333   1.239757   0.572687                  \n",
      "    27     0.905001   1.222907   0.568282                  \n",
      "    28     0.889068   1.238663   0.562555                  \n",
      "    29     0.894036   1.232819   0.571806                  \n",
      "    30     0.876454   1.225042   0.571806                  \n",
      "    31     0.870697   1.232423   0.56652                   \n",
      "    32     0.846949   1.230896   0.568722                  \n",
      "    33     0.851309   1.22774    0.570925                  \n",
      "    34     0.834051   1.239061   0.572687                  \n",
      "    35     0.819771   1.255322   0.579295                  \n",
      "    36     0.815389   1.229643   0.576211                  \n",
      "    37     0.81184    1.23873    0.575771                  \n",
      "    38     0.797088   1.238646   0.56652                   \n",
      "    39     0.779605   1.23664    0.570485                  \n",
      "    40     0.767446   1.248147   0.572687                  \n",
      "    41     0.76354    1.264133   0.570044                  \n",
      "    42     0.758566   1.252791   0.575771                  \n",
      "    43     0.743801   1.254786   0.570925                  \n",
      "    44     0.736743   1.244689   0.573128                  \n",
      "    45     0.737313   1.258209   0.580617                  \n",
      "    46     0.725815   1.260139   0.572687                  \n",
      "    47     0.716057   1.267855   0.584141                  \n",
      "    48     0.718051   1.270633   0.5837                    \n",
      "    49     0.703176   1.269347   0.578855                  \n",
      "    50     0.697806   1.261478   0.582819                  \n",
      "    51     0.679867   1.249103   0.588106                  \n",
      "    52     0.677528   1.263053   0.581498                  \n",
      "    53     0.67936    1.285932   0.579295                  \n",
      "    54     0.674636   1.271208   0.5837                    \n",
      "    55     0.660194   1.279263   0.581938                  \n",
      "    56     0.664846   1.285196   0.573568                  \n",
      "    57     0.666549   1.282417   0.569163                  \n",
      "    58     0.660277   1.290093   0.582379                  \n",
      "    59     0.653416   1.283684   0.576652                  \n",
      "    60     0.658145   1.285947   0.581938                  \n",
      "    61     0.635582   1.299329   0.582379                  \n",
      "    62     0.62691    1.292937   0.575771                  \n",
      "    63     0.623102   1.298447   0.581938                  \n",
      "    64     0.620303   1.311918   0.57533                   \n",
      "    65     0.617619   1.31322    0.579736                  \n",
      "    66     0.594585   1.307389   0.573128                  \n",
      "    67     0.609709   1.314146   0.572247                  \n",
      "    68     0.616523   1.299848   0.577533                  \n",
      "    69     0.60861    1.313483   0.582819                  \n",
      "    70     0.592707   1.305174   0.584581                  \n",
      "    71     0.586753   1.314613   0.577093                  \n",
      "    72     0.580636   1.326354   0.582379                  \n",
      "    73     0.586453   1.30848    0.581498                  \n",
      "    74     0.595795   1.3275     0.580176                  \n",
      "    75     0.577933   1.316858   0.576652                  \n",
      "    76     0.585238   1.322986   0.584581                  \n",
      "    77     0.576615   1.324546   0.581057                  \n",
      "    78     0.573005   1.311983   0.579736                  \n",
      "    79     0.573813   1.327986   0.581498                  \n",
      "    80     0.567143   1.333149   0.581057                  \n",
      "    81     0.555832   1.321373   0.584141                  \n",
      "    82     0.54208    1.341774   0.577093                  \n",
      "    83     0.554719   1.35388    0.57533                   \n",
      "    84     0.554646   1.343735   0.580617                  \n",
      "    85     0.55436    1.333868   0.581057                  \n",
      "    86     0.542222   1.338862   0.581938                  \n",
      "    87     0.53701    1.343015   0.579736                  \n",
      "    88     0.543657   1.350021   0.577533                  \n",
      "    89     0.54259    1.357207   0.576652                  \n",
      "    90     0.545487   1.352239   0.581498                  \n",
      "    91     0.525728   1.341318   0.573128                  \n",
      "    92     0.53005    1.35571    0.579736                  \n",
      "    93     0.530803   1.358299   0.578855                  \n",
      "    94     0.523364   1.359931   0.578414                  \n",
      "    95     0.517208   1.347478   0.584141                  \n",
      "    96     0.517322   1.36986    0.578414                  \n",
      "    97     0.517853   1.35725    0.584141                  \n",
      "    98     0.513433   1.383332   0.584581                  \n",
      "    99     0.513121   1.368347   0.582379                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3041021ce8754895b22c477fc7fa0b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df51b298d18b48bcb654267e5512a7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.058092   1.682293   0.431278  \n",
      "    1      1.770803   1.590136   0.459912                 \n",
      "    2      1.63962    1.517988   0.467401                 \n",
      "    3      1.552602   1.463593   0.485022                 \n",
      "    4      1.480642   1.419397   0.503084                 \n",
      "    5      1.42187    1.393795   0.517621                 \n",
      "    6      1.366614   1.363929   0.514097                 \n",
      "    7      1.338306   1.355018   0.519824                 \n",
      "    8      1.29881    1.330372   0.52511                  \n",
      "    9      1.26282    1.338571   0.532159                 \n",
      "    10     1.23036    1.306958   0.53304                  \n",
      "    11     1.19982    1.294975   0.537445                 \n",
      "    12     1.182761   1.296886   0.546696                 \n",
      "    13     1.152609   1.266105   0.544493                 \n",
      "    14     1.128042   1.264656   0.550661                 \n",
      "    15     1.126551   1.258455   0.551542                 \n",
      "    16     1.095404   1.258473   0.550661                 \n",
      "    17     1.071569   1.255531   0.551982                 \n",
      "    18     1.055783   1.275347   0.554185                  \n",
      "    19     1.034462   1.265396   0.555947                 \n",
      "    20     1.015556   1.24226    0.555066                 \n",
      "    21     1.009003   1.238422   0.557709                  \n",
      "    22     0.996686   1.233629   0.564317                  \n",
      "    23     0.973592   1.228494   0.56696                   \n",
      "    24     0.971168   1.241822   0.560352                  \n",
      "    25     0.957282   1.238618   0.563877                  \n",
      "    26     0.938812   1.231183   0.557709                  \n",
      "    27     0.926318   1.221112   0.57533                   \n",
      "    28     0.906546   1.229854   0.56696                   \n",
      "    29     0.896056   1.227099   0.56652                   \n",
      "    30     0.87859    1.22947    0.57489                   \n",
      "    31     0.87204    1.224626   0.571366                  \n",
      "    32     0.860895   1.21713    0.561674                  \n",
      "    33     0.844745   1.21745    0.577533                  \n",
      "    34     0.832687   1.229452   0.569163                  \n",
      "    35     0.82378    1.224427   0.572247                  \n",
      "    36     0.825932   1.24209    0.570485                  \n",
      "    37     0.81289    1.236787   0.57533                   \n",
      "    38     0.798552   1.246952   0.572247                  \n",
      "    39     0.793686   1.24505    0.562555                  \n",
      "    40     0.783544   1.245837   0.56652                   \n",
      "    41     0.782885   1.249022   0.574009                  \n",
      "    42     0.775237   1.25852    0.571806                  \n",
      "    43     0.769885   1.243598   0.581498                  \n",
      "    44     0.753377   1.255012   0.570925                  \n",
      "    45     0.753791   1.252172   0.580617                  \n",
      "    46     0.738719   1.259305   0.574009                  \n",
      "    47     0.729831   1.255132   0.577974                  \n",
      "    48     0.71431    1.254873   0.562996                  \n",
      "    49     0.707666   1.255095   0.579736                  \n",
      "    50     0.699401   1.239445   0.581498                  \n",
      "    51     0.697333   1.25356    0.571806                  \n",
      "    52     0.708012   1.261252   0.573568                  \n",
      "    53     0.702659   1.263038   0.577093                  \n",
      "    54     0.681019   1.26893    0.570044                  \n",
      "    55     0.662832   1.263353   0.57533                   \n",
      "    56     0.673633   1.278048   0.570925                  \n",
      "    57     0.667728   1.282785   0.570925                  \n",
      "    58     0.651325   1.276603   0.57489                   \n",
      "    59     0.663499   1.280292   0.579295                  \n",
      "    60     0.660047   1.276523   0.577093                  \n",
      "    61     0.652751   1.273528   0.571806                  \n",
      "    62     0.632752   1.294588   0.573128                  \n",
      "    63     0.628787   1.29375    0.569163                  \n",
      "    64     0.625665   1.289677   0.574009                  \n",
      "    65     0.614137   1.292706   0.570925                  \n",
      "    66     0.617316   1.305364   0.578414                  \n",
      "    67     0.622837   1.28692    0.579736                  \n",
      "    68     0.613882   1.291688   0.576652                  \n",
      "    69     0.599105   1.303609   0.577533                  \n",
      "    70     0.602008   1.308936   0.572247                  \n",
      "    71     0.599438   1.308717   0.571366                  \n",
      "    72     0.598963   1.314151   0.580176                  \n",
      "    73     0.598428   1.316516   0.576211                  \n",
      "    74     0.602351   1.29983    0.576211                  \n",
      "    75     0.592518   1.310525   0.572687                  \n",
      "    76     0.579384   1.323698   0.57533                   \n",
      "    77     0.58479    1.305322   0.57533                   \n",
      "    78     0.583005   1.316112   0.580176                  \n",
      "    79     0.579712   1.342918   0.572247                  \n",
      "    80     0.582151   1.319135   0.584581                  \n",
      "    81     0.57501    1.324165   0.571806                  \n",
      "    82     0.570384   1.33287    0.578855                  \n",
      "    83     0.570252   1.329389   0.576211                  \n",
      "    84     0.562217   1.347803   0.578855                  \n",
      "    85     0.552523   1.337754   0.570044                  \n",
      "    86     0.546674   1.316006   0.5837                    \n",
      "    87     0.533242   1.34333    0.574449                  \n",
      "    88     0.536989   1.351716   0.582379                  \n",
      "    89     0.543825   1.372278   0.57489                   \n",
      "    90     0.542236   1.353865   0.574009                  \n",
      "    91     0.536949   1.329185   0.58326                   \n",
      "    92     0.538413   1.353985   0.572687                  \n",
      "    93     0.54087    1.353143   0.574009                  \n",
      "    94     0.536465   1.358303   0.570485                  \n",
      "    95     0.530186   1.363013   0.576211                  \n",
      "    96     0.530747   1.357378   0.582379                  \n",
      "    97     0.522659   1.360267   0.577533                  \n",
      "    98     0.521587   1.368294   0.576652                  \n",
      "    99     0.532006   1.357942   0.580176                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f7e0c5076544dfb526b4b9e1275d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3e4fdbd2fa48ef9770fdfda6d1b634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.075494   1.683606   0.424229  \n",
      "    1      1.779713   1.578465   0.454626                 \n",
      "    2      1.641441   1.512408   0.472687                 \n",
      "    3      1.540287   1.453068   0.493833                 \n",
      "    4      1.470331   1.439515   0.503524                 \n",
      "    5      1.424656   1.390363   0.509251                 \n",
      "    6      1.375353   1.372593   0.514097                 \n",
      "    7      1.344349   1.346982   0.519383                 \n",
      "    8      1.298948   1.339111   0.526872                 \n",
      "    9      1.263226   1.330409   0.535242                 \n",
      "    10     1.227147   1.311091   0.545374                 \n",
      "    11     1.211222   1.303971   0.533921                 \n",
      "    12     1.177912   1.291893   0.540088                 \n",
      "    13     1.154132   1.288125   0.543172                 \n",
      "    14     1.114563   1.26843    0.552423                  \n",
      "    15     1.107407   1.26738    0.548458                  \n",
      "    16     1.094031   1.266437   0.549339                  \n",
      "    17     1.078174   1.26773    0.54978                  \n",
      "    18     1.043073   1.265745   0.551101                 \n",
      "    19     1.026795   1.248664   0.553744                 \n",
      "    20     1.013034   1.254378   0.55815                  \n",
      "    21     1.007307   1.253382   0.550661                  \n",
      "    22     0.984859   1.227852   0.570925                  \n",
      "    23     0.968145   1.251252   0.556828                  \n",
      "    24     0.963934   1.244758   0.556388                  \n",
      "    25     0.950469   1.239331   0.562555                  \n",
      "    26     0.933601   1.243837   0.559031                  \n",
      "    27     0.915774   1.256032   0.564758                  \n",
      "    28     0.899818   1.251378   0.56652                   \n",
      "    29     0.873223   1.258078   0.559471                  \n",
      "    30     0.874891   1.248853   0.55815                   \n",
      "    31     0.879234   1.252155   0.570044                  \n",
      "    32     0.844714   1.264034   0.552863                  \n",
      "    33     0.848132   1.253485   0.572687                  \n",
      "    34     0.834591   1.248647   0.570925                  \n",
      "    35     0.822701   1.250498   0.570925                  \n",
      "    36     0.809815   1.253437   0.562115                  \n",
      "    37     0.803793   1.256872   0.567841                  \n",
      "    38     0.80096    1.277903   0.571806                  \n",
      "    39     0.793926   1.246758   0.572687                  \n",
      "    40     0.783387   1.260489   0.57489                   \n",
      "    41     0.779126   1.264857   0.573568                  \n",
      "    42     0.772708   1.268687   0.568282                  \n",
      "    43     0.753853   1.265424   0.567841                  \n",
      "    44     0.730712   1.270076   0.575771                  \n",
      "    45     0.738977   1.264173   0.578855                  \n",
      "    46     0.737918   1.268816   0.571806                  \n",
      "    47     0.73373    1.263419   0.581498                  \n",
      "    48     0.729436   1.254355   0.571366                  \n",
      "    49     0.71666    1.278543   0.570485                  \n",
      "    50     0.712939   1.288562   0.580617                  \n",
      "    51     0.699816   1.266059   0.573568                  \n",
      "    52     0.707247   1.287114   0.570925                  \n",
      "    53     0.687895   1.296884   0.574009                  \n",
      "    54     0.672657   1.291242   0.577974                  \n",
      "    55     0.667676   1.298142   0.580176                  \n",
      "    56     0.663747   1.289106   0.580176                  \n",
      "    57     0.654048   1.282678   0.582379                  \n",
      "    58     0.645767   1.299237   0.577974                  \n",
      "    59     0.643469   1.321084   0.582379                  \n",
      "    60     0.649617   1.298607   0.57533                   \n",
      "    61     0.629038   1.307962   0.57533                   \n",
      "    62     0.632213   1.318479   0.570485                  \n",
      "    63     0.627474   1.313982   0.578414                  \n",
      "    64     0.625985   1.3062     0.575771                  \n",
      "    65     0.614482   1.315393   0.576211                  \n",
      "    66     0.594757   1.313655   0.572687                  \n",
      "    67     0.612589   1.329012   0.573128                  \n",
      "    68     0.608096   1.311635   0.579295                  \n",
      "    69     0.604778   1.315446   0.580176                  \n",
      "    70     0.595048   1.327016   0.569604                  \n",
      "    71     0.612125   1.321792   0.573568                  \n",
      "    72     0.604764   1.346719   0.574009                  \n",
      "    73     0.591549   1.347218   0.575771                  \n",
      "    74     0.583668   1.34822    0.570044                  \n",
      "    75     0.58555    1.35275    0.579295                  \n",
      "    76     0.588123   1.342449   0.571366                  \n",
      "    77     0.580142   1.346865   0.573128                  \n",
      "    78     0.572497   1.343061   0.578855                  \n",
      "    79     0.565284   1.362719   0.577533                  \n",
      "    80     0.564629   1.357983   0.577974                  \n",
      "    81     0.552595   1.357771   0.578855                  \n",
      "    82     0.560275   1.358738   0.579295                  \n",
      "    83     0.5608     1.364782   0.577974                  \n",
      "    84     0.559938   1.360532   0.581938                  \n",
      "    85     0.539334   1.380655   0.570925                  \n",
      "    86     0.553282   1.359173   0.581057                  \n",
      "    87     0.542872   1.362663   0.576652                  \n",
      "    88     0.53685    1.380894   0.577093                  \n",
      "    89     0.535484   1.366176   0.57533                   \n",
      "    90     0.538974   1.368608   0.579736                  \n",
      "    91     0.538992   1.372932   0.573128                  \n",
      "    92     0.537804   1.391321   0.57489                   \n",
      "    93     0.523144   1.385607   0.575771                  \n",
      "    94     0.530428   1.377466   0.573128                  \n",
      "    95     0.526618   1.38219    0.581938                  \n",
      "    96     0.510191   1.370065   0.578855                  \n",
      "    97     0.52837    1.383226   0.574449                  \n",
      "    98     0.517411   1.394408   0.5837                    \n",
      "    99     0.515679   1.372456   0.578855                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcb78b42b7f49d0a4e7f821866a39a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9941c1e12c443e8971c551606c49bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.062443   1.663398   0.444493  \n",
      "    1      1.776037   1.548098   0.463436                 \n",
      "    2      1.638831   1.489166   0.476211                 \n",
      "    3      1.545082   1.43153    0.493392                 \n",
      "    4      1.470115   1.401145   0.504846                 \n",
      "    5      1.410552   1.374518   0.519383                 \n",
      "    6      1.352297   1.355153   0.515419                 \n",
      "    7      1.322183   1.355286   0.520705                 \n",
      "    8      1.287429   1.327886   0.52467                  \n",
      "    9      1.243965   1.312452   0.537445                 \n",
      "    10     1.227162   1.303163   0.528634                 \n",
      "    11     1.190456   1.282146   0.539648                  \n",
      "    12     1.169072   1.300047   0.543612                 \n",
      "    13     1.163871   1.280539   0.54141                  \n",
      "    14     1.13802    1.266065   0.54978                  \n",
      "    15     1.102924   1.285507   0.540969                 \n",
      "    16     1.075641   1.251691   0.556388                 \n",
      "    17     1.071395   1.259543   0.548899                 \n",
      "    18     1.052473   1.243318   0.555066                 \n",
      "    19     1.012981   1.246201   0.548899                 \n",
      "    20     1.007469   1.257393   0.551982                  \n",
      "    21     1.009554   1.259031   0.55859                   \n",
      "    22     0.990582   1.252038   0.559912                  \n",
      "    23     0.966252   1.247038   0.560352                  \n",
      "    24     0.958306   1.245659   0.555507                  \n",
      "    25     0.948318   1.235333   0.567401                  \n",
      "    26     0.93578    1.233426   0.566079                  \n",
      "    27     0.935838   1.238859   0.561233                  \n",
      "    28     0.907775   1.224096   0.567401                  \n",
      "    29     0.906599   1.226677   0.567841                  \n",
      "    30     0.891718   1.241153   0.565639                  \n",
      "    31     0.863218   1.248854   0.566079                  \n",
      "    32     0.859152   1.248365   0.569163                  \n",
      "    33     0.855383   1.233875   0.577093                  \n",
      "    34     0.834122   1.238611   0.570044                  \n",
      "    35     0.828239   1.248949   0.572687                  \n",
      "    36     0.824683   1.260305   0.55815                   \n",
      "    37     0.815769   1.245186   0.573128                  \n",
      "    38     0.79544    1.268465   0.559471                  \n",
      "    39     0.780015   1.261211   0.560352                  \n",
      "    40     0.780307   1.253084   0.571806                  \n",
      "    41     0.77633    1.25749    0.567401                  \n",
      "    42     0.76967    1.263784   0.569604                  \n",
      "    43     0.762318   1.252177   0.571366                  \n",
      "    44     0.744778   1.262979   0.573568                  \n",
      "    45     0.745106   1.260878   0.577974                  \n",
      "    46     0.715381   1.252233   0.573128                  \n",
      "    47     0.716904   1.26145    0.5837                    \n",
      "    48     0.718529   1.267549   0.571366                  \n",
      "    49     0.709913   1.248146   0.574449                  \n",
      "    50     0.709889   1.289206   0.570925                  \n",
      "    51     0.690309   1.276471   0.57489                   \n",
      "    52     0.689102   1.289293   0.574009                  \n",
      "    53     0.683471   1.288398   0.57489                   \n",
      "    54     0.689755   1.281035   0.573568                  \n",
      "    55     0.670795   1.285897   0.573128                  \n",
      "    56     0.672403   1.269652   0.577974                  \n",
      "    57     0.651142   1.291605   0.568282                  \n",
      "    58     0.656809   1.287369   0.568282                  \n",
      "    59     0.644861   1.299079   0.573128                  \n",
      "    60     0.641681   1.311967   0.578414                  \n",
      "    61     0.64902    1.312032   0.575771                  \n",
      "    62     0.634813   1.306359   0.575771                  \n",
      "    63     0.631549   1.318573   0.571806                  \n",
      "    64     0.623085   1.312025   0.57533                   \n",
      "    65     0.632055   1.299325   0.573568                  \n",
      "    66     0.62364    1.302249   0.581498                  \n",
      "    67     0.607671   1.328748   0.570044                  \n",
      "    68     0.605436   1.327204   0.57489                   \n",
      "    69     0.599604   1.334359   0.573568                  \n",
      "    70     0.602465   1.328671   0.570925                  \n",
      "    71     0.605789   1.332571   0.57489                   \n",
      "    72     0.597582   1.319375   0.571806                  \n",
      "    73     0.600089   1.324841   0.574449                  \n",
      "    74     0.591556   1.326467   0.57489                   \n",
      "    75     0.585044   1.319636   0.57533                   \n",
      "    76     0.586158   1.341046   0.572687                  \n",
      "    77     0.588452   1.351774   0.571806                  \n",
      "    78     0.579756   1.328531   0.577093                  \n",
      "    79     0.5727     1.32358    0.579295                  \n",
      "    80     0.555496   1.344226   0.574449                  \n",
      "    81     0.561796   1.33941    0.575771                  \n",
      "    82     0.552922   1.343233   0.571806                  \n",
      "    83     0.559871   1.336888   0.578855                  \n",
      "    84     0.556379   1.35734    0.57489                   \n",
      "    85     0.553673   1.343586   0.577533                  \n",
      "    86     0.544566   1.352722   0.577093                  \n",
      "    87     0.552387   1.351653   0.572247                  \n",
      "    88     0.549699   1.363618   0.57533                   \n",
      "    89     0.548799   1.37392    0.578414                  \n",
      "    90     0.542539   1.372811   0.581057                  \n",
      "    91     0.542891   1.361707   0.585463                  \n",
      "    92     0.531063   1.35893    0.579736                  \n",
      "    93     0.537831   1.373719   0.57489                   \n",
      "    94     0.519922   1.352708   0.580617                  \n",
      "    95     0.528215   1.357008   0.576652                  \n",
      "    96     0.536591   1.373817   0.570044                  \n",
      "    97     0.520008   1.358973   0.577974                  \n",
      "    98     0.510228   1.377737   0.57489                   \n",
      "    99     0.516118   1.388451   0.580617                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad8c16be5094f00aeaa5426c0366d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a074e5dd3d04b90be41dc158fa36b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.063038   1.685731   0.414978  \n",
      "    1      1.764446   1.562178   0.446256                 \n",
      "    2      1.629279   1.484249   0.489868                 \n",
      "    3      1.532227   1.438738   0.500881                 \n",
      "    4      1.477964   1.408984   0.501322                  \n",
      "    5      1.423282   1.376824   0.515419                 \n",
      "    6      1.368953   1.359803   0.518502                 \n",
      "    7      1.332311   1.32568    0.527313                 \n",
      "    8      1.301508   1.331131   0.531278                 \n",
      "    9      1.270664   1.316045   0.52511                  \n",
      "    10     1.232052   1.309361   0.540969                 \n",
      "    11     1.203517   1.283983   0.551542                 \n",
      "    12     1.178116   1.283642   0.540969                 \n",
      "    13     1.146628   1.274553   0.542291                 \n",
      "    14     1.122978   1.273328   0.544493                 \n",
      "    15     1.103877   1.254887   0.546256                 \n",
      "    16     1.098853   1.245758   0.553304                 \n",
      "    17     1.064548   1.241274   0.556388                 \n",
      "    18     1.056443   1.239816   0.552863                 \n",
      "    19     1.025071   1.247898   0.562115                 \n",
      "    20     1.009709   1.236014   0.554626                  \n",
      "    21     0.995386   1.234132   0.565639                  \n",
      "    22     0.976999   1.237378   0.56652                   \n",
      "    23     0.968265   1.235901   0.559471                  \n",
      "    24     0.944112   1.232805   0.565639                  \n",
      "    25     0.932632   1.252535   0.570485                  \n",
      "    26     0.924317   1.251092   0.568282                  \n",
      "    27     0.900806   1.247077   0.573568                  \n",
      "    28     0.887689   1.253485   0.563436                  \n",
      "    29     0.877175   1.229163   0.566079                  \n",
      "    30     0.869667   1.236237   0.562555                  \n",
      "    31     0.858448   1.231937   0.564758                  \n",
      "    32     0.851275   1.237297   0.560793                  \n",
      "    33     0.848658   1.25615    0.559912                  \n",
      "    34     0.833622   1.237706   0.57489                   \n",
      "    35     0.818996   1.232768   0.575771                  \n",
      "    36     0.817022   1.241483   0.576211                  \n",
      "    37     0.812401   1.242255   0.562996                  \n",
      "    38     0.790562   1.24235    0.571806                  \n",
      "    39     0.779722   1.240996   0.567401                  \n",
      "    40     0.780765   1.237669   0.574449                  \n",
      "    41     0.77874    1.229291   0.577974                  \n",
      "    42     0.761704   1.252035   0.579736                  \n",
      "    43     0.747165   1.250186   0.578855                  \n",
      "    44     0.741849   1.251826   0.572687                  \n",
      "    45     0.744496   1.251719   0.577093                  \n",
      "    46     0.731371   1.261012   0.577533                  \n",
      "    47     0.71691    1.26125    0.573568                  \n",
      "    48     0.711      1.258218   0.578414                  \n",
      "    49     0.717946   1.267067   0.577533                  \n",
      "    50     0.695143   1.250676   0.578855                  \n",
      "    51     0.685033   1.259428   0.582819                  \n",
      "    52     0.673646   1.250991   0.579736                  \n",
      "    53     0.674086   1.269505   0.567841                  \n",
      "    54     0.668926   1.293354   0.571806                  \n",
      "    55     0.655099   1.274897   0.57533                   \n",
      "    56     0.660832   1.26469    0.579736                  \n",
      "    57     0.65028    1.285875   0.578414                  \n",
      "    58     0.649868   1.301253   0.570044                  \n",
      "    59     0.656104   1.295233   0.57489                   \n",
      "    60     0.63573    1.304906   0.576211                  \n",
      "    61     0.642161   1.30534    0.573568                  \n",
      "    62     0.634431   1.303905   0.581498                  \n",
      "    63     0.635568   1.304759   0.576652                  \n",
      "    64     0.612396   1.316779   0.573128                  \n",
      "    65     0.617074   1.31277    0.571806                  \n",
      "    66     0.61839    1.290045   0.586344                  \n",
      "    67     0.604378   1.30293    0.585463                  \n",
      "    68     0.606583   1.285521   0.587665                  \n",
      "    69     0.596622   1.316977   0.574009                  \n",
      "    70     0.588816   1.332106   0.579295                  \n",
      "    71     0.596144   1.317058   0.584581                  \n",
      "    72     0.599094   1.331294   0.567841                  \n",
      "    73     0.598356   1.320495   0.576211                  \n",
      "    74     0.59475    1.346378   0.577974                  \n",
      "    75     0.58267    1.33141    0.580176                  \n",
      "    76     0.585739   1.322227   0.573128                  \n",
      "    77     0.575765   1.322111   0.579736                  \n",
      "    78     0.574471   1.336176   0.577093                  \n",
      "    79     0.576711   1.335577   0.577093                  \n",
      "    80     0.565435   1.338353   0.580176                  \n",
      "    81     0.554857   1.33795    0.585903                  \n",
      "    82     0.56374    1.340567   0.584581                  \n",
      "    83     0.553309   1.348809   0.576211                  \n",
      "    84     0.551412   1.358848   0.581057                  \n",
      "    85     0.558042   1.357684   0.574009                  \n",
      "    86     0.551905   1.36028    0.581498                  \n",
      "    87     0.550556   1.348372   0.580617                  \n",
      "    88     0.542914   1.381632   0.570044                  \n",
      "    89     0.555214   1.356747   0.57533                   \n",
      "    90     0.542039   1.364326   0.577093                  \n",
      "    91     0.539687   1.351619   0.584581                  \n",
      "    92     0.53683    1.3601     0.582379                  \n",
      "    93     0.527664   1.356562   0.577974                  \n",
      "    94     0.525304   1.369646   0.58326                   \n",
      "    95     0.518841   1.374023   0.57489                   \n",
      "    96     0.517762   1.38612    0.576211                  \n",
      "    97     0.517446   1.391403   0.580617                  \n",
      "    98     0.521129   1.39756    0.577093                  \n",
      "    99     0.521802   1.400581   0.575771                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9075340acffa4092919493931038bbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865efc42ee2342e1ae0f5594e4ef53b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.092233   1.66641    0.439207  \n",
      "    1      1.77948    1.570702   0.466079                 \n",
      "    2      1.640784   1.530033   0.473128                 \n",
      "    3      1.553381   1.45206    0.505727                 \n",
      "    4      1.483861   1.407587   0.502643                 \n",
      "    5      1.408491   1.382125   0.514978                 \n",
      "    6      1.369511   1.356664   0.518062                 \n",
      "    7      1.335667   1.330133   0.526872                 \n",
      "    8      1.297144   1.331242   0.530837                 \n",
      "    9      1.26276    1.325361   0.535242                 \n",
      "    10     1.226886   1.302318   0.543172                 \n",
      "    11     1.19639    1.287066   0.537445                 \n",
      "    12     1.179546   1.274258   0.551101                 \n",
      "    13     1.142277   1.286925   0.542731                 \n",
      "    14     1.119497   1.278839   0.552423                 \n",
      "    15     1.113673   1.276885   0.543172                 \n",
      "    16     1.101442   1.274828   0.549339                 \n",
      "    17     1.07559    1.268418   0.547137                 \n",
      "    18     1.046303   1.257605   0.55859                  \n",
      "    19     1.046479   1.255339   0.548899                 \n",
      "    20     1.026121   1.251414   0.552423                 \n",
      "    21     1.005967   1.24022    0.566079                  \n",
      "    22     0.979808   1.243236   0.564758                  \n",
      "    23     0.965909   1.243516   0.562555                  \n",
      "    24     0.950411   1.23327    0.561233                  \n",
      "    25     0.950121   1.251428   0.552863                  \n",
      "    26     0.930295   1.237937   0.56696                   \n",
      "    27     0.912668   1.216823   0.567841                  \n",
      "    28     0.897113   1.238984   0.554626                  \n",
      "    29     0.8972     1.224945   0.574009                  \n",
      "    30     0.878712   1.22579    0.567401                  \n",
      "    31     0.868047   1.239424   0.571366                  \n",
      "    32     0.869927   1.230105   0.567401                  \n",
      "    33     0.849951   1.250502   0.577093                  \n",
      "    34     0.83534    1.238273   0.570044                  \n",
      "    35     0.831019   1.228913   0.572247                  \n",
      "    36     0.822903   1.243414   0.56696                   \n",
      "    37     0.807009   1.217993   0.574449                  \n",
      "    38     0.796615   1.239111   0.571366                  \n",
      "    39     0.804418   1.235998   0.572687                  \n",
      "    40     0.783717   1.230258   0.581057                  \n",
      "    41     0.75567    1.264375   0.569163                  \n",
      "    42     0.758225   1.253266   0.580176                  \n",
      "    43     0.751797   1.246273   0.574009                  \n",
      "    44     0.740641   1.270221   0.578414                  \n",
      "    45     0.738565   1.259341   0.577974                  \n",
      "    46     0.721246   1.268334   0.576652                  \n",
      "    47     0.725466   1.264951   0.562555                  \n",
      "    48     0.709181   1.261459   0.581938                  \n",
      "    49     0.70521    1.269069   0.577974                  \n",
      "    50     0.702128   1.258461   0.581938                  \n",
      "    51     0.691415   1.284905   0.574009                  \n",
      "    52     0.690411   1.286622   0.574449                  \n",
      "    53     0.669258   1.280392   0.572247                  \n",
      "    54     0.668009   1.280091   0.569604                  \n",
      "    55     0.666808   1.289539   0.578414                  \n",
      "    56     0.665906   1.264117   0.574449                  \n",
      "    57     0.65869    1.272242   0.57533                   \n",
      "    58     0.658447   1.284299   0.57489                   \n",
      "    59     0.654495   1.292735   0.576211                  \n",
      "    60     0.653054   1.282331   0.578414                  \n",
      "    61     0.640931   1.285013   0.579736                  \n",
      "    62     0.637917   1.296714   0.580617                  \n",
      "    63     0.633387   1.300758   0.57533                   \n",
      "    64     0.627186   1.315552   0.584581                  \n",
      "    65     0.632659   1.30224    0.574009                  \n",
      "    66     0.620838   1.303425   0.57533                   \n",
      "    67     0.625691   1.306981   0.579295                  \n",
      "    68     0.603353   1.312878   0.575771                  \n",
      "    69     0.610883   1.304383   0.579736                  \n",
      "    70     0.603214   1.308455   0.573128                  \n",
      "    71     0.60852    1.317215   0.581057                  \n",
      "    72     0.594098   1.32505    0.577974                  \n",
      "    73     0.590706   1.335055   0.581498                  \n",
      "    74     0.596135   1.319191   0.585463                  \n",
      "    75     0.573533   1.325983   0.582819                  \n",
      "    76     0.571882   1.340843   0.577533                  \n",
      "    77     0.572486   1.334301   0.576211                  \n",
      "    78     0.571341   1.34501    0.57489                   \n",
      "    79     0.572318   1.354834   0.579295                  \n",
      "    80     0.573924   1.326541   0.580617                  \n",
      "    81     0.562373   1.335304   0.587225                  \n",
      "    82     0.557024   1.350793   0.576211                  \n",
      "    83     0.545388   1.344333   0.581938                  \n",
      "    84     0.554698   1.346695   0.577533                  \n",
      "    85     0.553998   1.340306   0.581938                  \n",
      "    86     0.556351   1.345865   0.580176                  \n",
      "    87     0.542009   1.352629   0.57533                   \n",
      "    88     0.537526   1.365588   0.582819                  \n",
      "    89     0.547919   1.347948   0.571806                  \n",
      "    90     0.541706   1.347712   0.581057                  \n",
      "    91     0.542822   1.366781   0.575771                  \n",
      "    92     0.542444   1.34839    0.581498                  \n",
      "    93     0.534594   1.365605   0.584141                  \n",
      "    94     0.539153   1.359789   0.582819                  \n",
      "    95     0.532851   1.37398    0.575771                  \n",
      "    96     0.522977   1.377583   0.572247                  \n",
      "    97     0.529173   1.367637   0.580617                  \n",
      "    98     0.52192    1.384791   0.574449                  \n",
      "    99     0.515404   1.374972   0.5837                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b75f1f8a634648ba5ed349cf55f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acc1be4db584fee9b6e267e40e2bba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.073422   1.672556   0.421145  \n",
      "    1      1.78585    1.555015   0.467401                  \n",
      "    2      1.6399     1.500961   0.480176                 \n",
      "    3      1.535749   1.469025   0.496035                 \n",
      "    4      1.471203   1.413403   0.505286                 \n",
      "    5      1.416234   1.393401   0.5163                   \n",
      "    6      1.371151   1.362424   0.518943                 \n",
      "    7      1.333243   1.342789   0.515859                 \n",
      "    8      1.297632   1.323096   0.525551                 \n",
      "    9      1.261791   1.310451   0.527313                 \n",
      "    10     1.246923   1.316659   0.530396                  \n",
      "    11     1.202701   1.285658   0.542731                 \n",
      "    12     1.176799   1.281872   0.531718                 \n",
      "    13     1.150727   1.283171   0.559031                 \n",
      "    14     1.13559    1.270559   0.54185                  \n",
      "    15     1.115756   1.252159   0.556388                 \n",
      "    16     1.087247   1.266576   0.556388                 \n",
      "    17     1.067686   1.25056    0.54978                  \n",
      "    18     1.043652   1.245754   0.553304                 \n",
      "    19     1.033374   1.245411   0.549339                  \n",
      "    20     1.015014   1.249342   0.559471                 \n",
      "    21     1.002509   1.255472   0.55859                   \n",
      "    22     0.981496   1.231795   0.562555                  \n",
      "    23     0.96672    1.260306   0.555066                  \n",
      "    24     0.962011   1.234443   0.566079                  \n",
      "    25     0.940698   1.228773   0.562115                  \n",
      "    26     0.919939   1.244318   0.559912                  \n",
      "    27     0.913558   1.229251   0.55859                   \n",
      "    28     0.904666   1.238135   0.566079                  \n",
      "    29     0.886347   1.234784   0.556828                  \n",
      "    30     0.872066   1.231709   0.573568                  \n",
      "    31     0.862967   1.233465   0.574009                  \n",
      "    32     0.852109   1.229551   0.565198                  \n",
      "    33     0.842835   1.241642   0.569604                  \n",
      "    34     0.833483   1.250433   0.572247                  \n",
      "    35     0.829854   1.235461   0.562555                  \n",
      "    36     0.817812   1.231803   0.567841                  \n",
      "    37     0.805246   1.236993   0.570485                  \n",
      "    38     0.794509   1.239937   0.572687                  \n",
      "    39     0.796835   1.242397   0.57533                   \n",
      "    40     0.779457   1.22953    0.576211                  \n",
      "    41     0.767685   1.254809   0.572247                  \n",
      "    42     0.756751   1.241125   0.573568                  \n",
      "    43     0.744145   1.254719   0.571806                  \n",
      "    44     0.731371   1.26692    0.570925                  \n",
      "    45     0.729119   1.263138   0.571806                  \n",
      "    46     0.722479   1.280348   0.567841                  \n",
      "    47     0.727511   1.269451   0.57489                   \n",
      "    48     0.718363   1.278384   0.572687                  \n",
      "    49     0.708417   1.24448    0.574009                  \n",
      "    50     0.70539    1.254054   0.571806                  \n",
      "    51     0.702167   1.261879   0.576211                  \n",
      "    52     0.696332   1.277509   0.572247                  \n",
      "    53     0.680082   1.277425   0.570044                  \n",
      "    54     0.684374   1.285754   0.571366                  \n",
      "    55     0.676749   1.27217    0.569604                  \n",
      "    56     0.661007   1.279353   0.572247                  \n",
      "    57     0.641599   1.289801   0.564758                  \n",
      "    58     0.646633   1.290869   0.568282                  \n",
      "    59     0.642315   1.275819   0.579295                  \n",
      "    60     0.648918   1.286891   0.575771                  \n",
      "    61     0.653052   1.299599   0.577974                  \n",
      "    62     0.640775   1.285095   0.577974                  \n",
      "    63     0.626958   1.298711   0.571806                  \n",
      "    64     0.621375   1.293724   0.571806                  \n",
      "    65     0.619946   1.298505   0.578414                  \n",
      "    66     0.618514   1.292624   0.56652                   \n",
      "    67     0.613679   1.308967   0.573128                  \n",
      "    68     0.614419   1.303993   0.574009                  \n",
      "    69     0.608449   1.327613   0.577533                  \n",
      "    70     0.613563   1.307713   0.581498                  \n",
      "    71     0.597105   1.294812   0.582819                  \n",
      "    72     0.604979   1.310511   0.571806                  \n",
      "    73     0.600645   1.299575   0.581938                  \n",
      "    74     0.58177    1.297141   0.585022                  \n",
      "    75     0.585393   1.319328   0.573128                  \n",
      "    76     0.577059   1.330453   0.577974                  \n",
      "    77     0.572723   1.314296   0.576211                  \n",
      "    78     0.575624   1.336531   0.576211                  \n",
      "    79     0.570904   1.337913   0.580617                  \n",
      "    80     0.560902   1.351961   0.574009                  \n",
      "    81     0.554852   1.34582    0.576652                  \n",
      "    82     0.555505   1.346387   0.572687                  \n",
      "    83     0.550647   1.341424   0.577093                  \n",
      "    84     0.558332   1.341034   0.573568                  \n",
      "    85     0.55662    1.332443   0.576211                  \n",
      "    86     0.552798   1.355181   0.577533                  \n",
      "    87     0.529936   1.359744   0.581057                  \n",
      "    88     0.537456   1.344739   0.582819                  \n",
      "    89     0.531228   1.345493   0.57533                   \n",
      "    90     0.538963   1.356747   0.576652                  \n",
      "    91     0.528927   1.34415    0.580617                  \n",
      "    92     0.529964   1.345165   0.579295                  \n",
      "    93     0.529157   1.359101   0.580176                  \n",
      "    94     0.526426   1.363447   0.582379                  \n",
      "    95     0.539749   1.364333   0.581057                  \n",
      "    96     0.540684   1.360261   0.579295                  \n",
      "    97     0.524493   1.35109    0.570044                  \n",
      "    98     0.511249   1.356715   0.574009                  \n",
      "    99     0.5169     1.373261   0.576652                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e721c438e6a48538324f609fb16294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce1df361d5f4bdc8ca8ed85f91f1ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.079844   1.667367   0.437885  \n",
      "    1      1.78055    1.552633   0.460352                 \n",
      "    2      1.629926   1.490917   0.473128                 \n",
      "    3      1.545215   1.448304   0.486344                 \n",
      "    4      1.472309   1.40425    0.501762                 \n",
      "    5      1.414946   1.394346   0.512775                 \n",
      "    6      1.36355    1.383962   0.5163                   \n",
      "    7      1.319522   1.321849   0.523348                 \n",
      "    8      1.275387   1.317617   0.534802                 \n",
      "    9      1.250435   1.306414   0.537445                 \n",
      "    10     1.23389    1.315801   0.537445                 \n",
      "    11     1.198433   1.29526    0.53348                  \n",
      "    12     1.172579   1.276422   0.551982                 \n",
      "    13     1.13823    1.261909   0.548458                 \n",
      "    14     1.134498   1.275434   0.551542                 \n",
      "    15     1.109187   1.265567   0.55815                  \n",
      "    16     1.070774   1.244929   0.560352                 \n",
      "    17     1.053425   1.26178    0.555066                 \n",
      "    18     1.051553   1.255539   0.55859                  \n",
      "    19     1.028659   1.252547   0.569604                 \n",
      "    20     1.020991   1.241583   0.568282                 \n",
      "    21     1.002164   1.247752   0.554626                  \n",
      "    22     0.980886   1.231503   0.564758                  \n",
      "    23     0.978752   1.23358    0.557709                  \n",
      "    24     0.968178   1.228073   0.563877                  \n",
      "    25     0.93823    1.233811   0.568722                  \n",
      "    26     0.928189   1.218977   0.560352                  \n",
      "    27     0.911055   1.233415   0.565198                  \n",
      "    28     0.892692   1.234655   0.56696                   \n",
      "    29     0.905471   1.240137   0.553304                  \n",
      "    30     0.887063   1.227952   0.570925                  \n",
      "    31     0.856495   1.223512   0.573568                  \n",
      "    32     0.851112   1.232345   0.563436                  \n",
      "    33     0.837491   1.236649   0.569163                  \n",
      "    34     0.829297   1.230773   0.561674                  \n",
      "    35     0.814918   1.2382     0.563436                  \n",
      "    36     0.808984   1.241721   0.574009                  \n",
      "    37     0.802426   1.245236   0.575771                  \n",
      "    38     0.791533   1.254625   0.563877                  \n",
      "    39     0.787869   1.235395   0.569604                  \n",
      "    40     0.769272   1.251357   0.572247                  \n",
      "    41     0.76611    1.252135   0.567841                  \n",
      "    42     0.761276   1.259336   0.572247                  \n",
      "    43     0.756012   1.258321   0.570485                  \n",
      "    44     0.744707   1.265302   0.57533                   \n",
      "    45     0.748821   1.27174    0.577974                  \n",
      "    46     0.722121   1.268621   0.563877                  \n",
      "    47     0.715512   1.25019    0.577533                  \n",
      "    48     0.704279   1.274435   0.571806                  \n",
      "    49     0.69861    1.258397   0.57489                   \n",
      "    50     0.700187   1.275637   0.57533                   \n",
      "    51     0.695658   1.277425   0.581938                  \n",
      "    52     0.688594   1.265035   0.576652                  \n",
      "    53     0.697904   1.263035   0.576652                  \n",
      "    54     0.684133   1.267234   0.577533                  \n",
      "    55     0.672494   1.278875   0.571366                  \n",
      "    56     0.672813   1.286761   0.578855                  \n",
      "    57     0.652733   1.277251   0.57489                   \n",
      "    58     0.652456   1.280341   0.584141                  \n",
      "    59     0.648974   1.274637   0.579295                  \n",
      "    60     0.6541     1.274243   0.580617                  \n",
      "    61     0.640797   1.283525   0.576652                  \n",
      "    62     0.63522    1.303861   0.581057                  \n",
      "    63     0.64064    1.284992   0.570925                  \n",
      "    64     0.628456   1.300272   0.578414                  \n",
      "    65     0.624142   1.305006   0.574449                  \n",
      "    66     0.616739   1.29351    0.584581                  \n",
      "    67     0.60451    1.300512   0.574449                  \n",
      "    68     0.618969   1.301423   0.581938                  \n",
      "    69     0.602469   1.291651   0.571806                  \n",
      "    70     0.59667    1.295743   0.573128                  \n",
      "    71     0.57996    1.313535   0.5837                    \n",
      "    72     0.596301   1.316917   0.5837                    \n",
      "    73     0.60016    1.330236   0.57489                   \n",
      "    74     0.586173   1.326819   0.577093                  \n",
      "    75     0.58119    1.332691   0.580176                  \n",
      "    76     0.580265   1.320914   0.582379                  \n",
      "    77     0.580929   1.337448   0.577533                  \n",
      "    78     0.560998   1.320129   0.580617                  \n",
      "    79     0.562706   1.334938   0.580176                  \n",
      "    80     0.568109   1.333266   0.579295                  \n",
      "    81     0.561945   1.344288   0.575771                  \n",
      "    82     0.56563    1.353176   0.580617                  \n",
      "    83     0.563814   1.328835   0.581498                  \n",
      "    84     0.553628   1.3299     0.573568                  \n",
      "    85     0.556329   1.339529   0.58326                   \n",
      "    86     0.562127   1.333494   0.575771                  \n",
      "    87     0.543208   1.334137   0.573568                  \n",
      "    88     0.528103   1.333981   0.586344                  \n",
      "    89     0.538499   1.353932   0.582819                  \n",
      "    90     0.535755   1.342434   0.582379                  \n",
      "    91     0.534948   1.349747   0.578855                  \n",
      "    92     0.544857   1.343952   0.578855                  \n",
      "    93     0.534169   1.351617   0.577533                  \n",
      "    94     0.523051   1.343115   0.574009                  \n",
      "    95     0.522192   1.361591   0.571366                  \n",
      "    96     0.532037   1.356464   0.574449                  \n",
      "    97     0.513974   1.354159   0.574009                  \n",
      "    98     0.510911   1.364187   0.579295                  \n",
      "    99     0.523939   1.365585   0.579736                  \n"
     ]
    }
   ],
   "source": [
    "v_a = k_fold_cross_loop1(10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=0.5796446\n",
      "stdev.=0.0024449785765932545\n"
     ]
    }
   ],
   "source": [
    "valacc = [0.581498,0.57709,0.58239,0.580176,0.578855,0.580617,0.57577,0.5837,0.57665,0.5797]\n",
    "print('mean='+str(np.mean(valacc)))\n",
    "print('stdev.='+str(np.std(valacc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9cc390f3c63c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7d70d1fd7ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTTA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703069703d7749b4959b3ec116063a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 71/71 [01:18<00:00,  1.05s/it]\n",
      "100%|| 18/18 [00:19<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "learn = ConvLearner.pretrained(arch, data, precompute=True, ps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588fdcee66474da884b46eb79201ef19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.267667   1.289242   0.543172  \n",
      "    1      1.24221    1.293281   0.544493                 \n",
      "    2      1.233481   1.27629    0.540529                 \n",
      "    3      1.23161    1.271877   0.543612                 \n",
      "    4      1.236873   1.275465   0.537004                 \n",
      "    5      1.217533   1.269105   0.551101                 \n",
      "    6      1.2        1.257847   0.548458                 \n",
      "    7      1.175196   1.259171   0.55022                  \n",
      "    8      1.167488   1.250896   0.544934                 \n",
      "    9      1.15592    1.278416   0.550661                 \n",
      "    10     1.157006   1.253423   0.552863                 \n",
      "    11     1.15742    1.253472   0.551101                  \n",
      "    12     1.147838   1.252431   0.555947                 \n",
      "    13     1.123004   1.244008   0.548458                 \n",
      "    14     1.11987    1.241218   0.546256                  \n",
      "    15     1.115705   1.247234   0.555066                  \n",
      "    16     1.095273   1.238376   0.556388                 \n",
      "    17     1.098234   1.237258   0.563877                 \n",
      "    18     1.088894   1.235968   0.549339                 \n",
      "    19     1.072944   1.233002   0.555947                 \n",
      "    20     1.075194   1.229227   0.557269                 \n",
      "    21     1.06491    1.233338   0.555507                 \n",
      "    22     1.048114   1.236095   0.559471                 \n",
      "    23     1.054952   1.224796   0.559471                 \n",
      "    24     1.041058   1.243802   0.556388                 \n",
      "    25     1.033636   1.221593   0.561674                  \n",
      "    26     1.02046    1.22503    0.560352                 \n",
      "    27     1.024753   1.23216    0.557269                 \n",
      "    28     1.011683   1.238804   0.567401                  \n",
      "    29     1.015862   1.229995   0.564758                  \n",
      "    30     1.001439   1.230244   0.553744                  \n",
      "    31     0.995771   1.22808    0.559031                  \n",
      "    32     0.983211   1.223669   0.562996                  \n",
      "    33     0.980927   1.218939   0.564758                  \n",
      "    34     0.96937    1.213605   0.56652                   \n",
      "    35     0.961829   1.231537   0.55815                   \n",
      "    36     0.966826   1.21792    0.562996                  \n",
      "    37     0.941626   1.220741   0.565639                  \n",
      "    38     0.944843   1.234988   0.555947                  \n",
      "    39     0.93752    1.227554   0.568722                  \n",
      "    40     0.94052    1.211494   0.560793                  \n",
      "    41     0.93976    1.228469   0.570485                  \n",
      "    42     0.922293   1.215995   0.567401                  \n",
      "    43     0.905079   1.219028   0.563436                  \n",
      "    44     0.903626   1.221683   0.561674                  \n",
      "    45     0.915379   1.227403   0.559471                  \n",
      "    46     0.894423   1.22922    0.561233                  \n",
      "    47     0.902934   1.225842   0.551982                  \n",
      "    48     0.892064   1.229417   0.561674                  \n",
      "    49     0.879614   1.230691   0.565198                  \n",
      "    50     0.874019   1.225061   0.557709                  \n",
      "    51     0.859574   1.226191   0.565198                  \n",
      "    52     0.863044   1.220566   0.560352                  \n",
      "    53     0.863865   1.225015   0.564758                  \n",
      "    54     0.853428   1.220403   0.562115                  \n",
      "    55     0.853541   1.233831   0.557709                  \n",
      "    56     0.86307    1.235933   0.552423                  \n",
      "    57     0.834969   1.235384   0.546696                  \n",
      "    58     0.838473   1.246407   0.560352                  \n",
      "    59     0.839785   1.235156   0.557709                  \n",
      "    60     0.832203   1.242558   0.551982                  \n",
      "    61     0.824017   1.226665   0.562555                  \n",
      "    62     0.817516   1.234777   0.563877                  \n",
      "    63     0.816306   1.235407   0.559471                  \n",
      "    64     0.81965    1.233551   0.562115                  \n",
      "    65     0.809005   1.241294   0.556388                  \n",
      "    66     0.796397   1.242799   0.557709                  \n",
      "    67     0.793246   1.240857   0.568722                  \n",
      "    68     0.792994   1.240294   0.557709                  \n",
      "    69     0.774924   1.243103   0.55859                   \n",
      "    70     0.784114   1.23585    0.55815                   \n",
      "    71     0.785018   1.234168   0.555507                  \n",
      "    72     0.763829   1.234954   0.566079                  \n",
      "    73     0.781006   1.242821   0.557709                  \n",
      "    74     0.782276   1.24891    0.561674                  \n",
      "    75     0.768109   1.244125   0.559471                  \n",
      "    76     0.764491   1.247289   0.555947                  \n",
      "    77     0.758733   1.25189    0.551101                  \n",
      "    78     0.762361   1.247961   0.560352                  \n",
      "    79     0.748988   1.259028   0.559471                  \n",
      "    80     0.752892   1.248447   0.55815                   \n",
      "    81     0.759422   1.245525   0.562115                  \n",
      "    82     0.750723   1.235167   0.561233                  \n",
      "    83     0.73576    1.251861   0.556828                  \n",
      "    84     0.748815   1.25344    0.565198                  \n",
      "    85     0.747166   1.247494   0.566079                  \n",
      "    86     0.737605   1.257672   0.559031                  \n",
      "    87     0.726009   1.262313   0.555947                  \n",
      "    88     0.735466   1.262721   0.55815                   \n",
      "    89     0.739166   1.273559   0.561233                  \n",
      "    90     0.72237    1.273382   0.559912                  \n",
      "    91     0.717151   1.268993   0.55815                   \n",
      "    92     0.718211   1.271525   0.560793                  \n",
      "    93     0.708547   1.264953   0.55859                   \n",
      "    94     0.702976   1.26382    0.562555                  \n",
      "    95     0.696141   1.278233   0.560352                  \n",
      "    96     0.704666   1.269957   0.562555                  \n",
      "    97     0.712623   1.264014   0.564317                  \n",
      "    98     0.701869   1.267199   0.559031                  \n",
      "    99     0.690187   1.277415   0.556388                  \n",
      "   100     0.694731   1.276408   0.559912                  \n",
      "   101     0.708116   1.271386   0.562115                  \n",
      "   102     0.687841   1.272555   0.563877                  \n",
      "   103     0.684142   1.283544   0.562115                  \n",
      "   104     0.675472   1.290297   0.561674                  \n",
      "   105     0.679045   1.282113   0.56696                   \n",
      "   106     0.684818   1.286728   0.55859                   \n",
      "   107     0.679174   1.292066   0.559031                  \n",
      "   108     0.669378   1.281054   0.562115                  \n",
      "   109     0.669722   1.288637   0.562555                  \n",
      "   110     0.656      1.292442   0.563436                  \n",
      "   111     0.669302   1.302476   0.562555                  \n",
      "   112     0.657468   1.301757   0.562115                  \n",
      "   113     0.673276   1.289227   0.559471                  \n",
      "   114     0.660381   1.287064   0.563436                  \n",
      "   115     0.654099   1.292903   0.557269                  \n",
      "   116     0.668299   1.291695   0.559471                  \n",
      "   117     0.655752   1.312907   0.555947                  \n",
      "   118     0.656277   1.29589    0.561233                  \n",
      "   119     0.660223   1.301472   0.559031                  \n",
      "   120     0.658702   1.297172   0.556828                  \n",
      "   121     0.657073   1.28774    0.560793                  \n",
      "   122     0.66203    1.291555   0.555066                  \n",
      "   123     0.65216    1.287151   0.556828                  \n",
      "   124     0.652359   1.301293   0.561233                  \n",
      "   125     0.648901   1.306631   0.565198                  \n",
      "   126     0.635885   1.301607   0.562996                  \n",
      "   127     0.646287   1.292456   0.564317                  \n",
      "   128     0.652304   1.294025   0.560793                  \n",
      "   129     0.648829   1.306788   0.557709                  \n",
      "   130     0.629881   1.313728   0.554626                  \n",
      "   131     0.624116   1.306443   0.559471                  \n",
      "   132     0.619558   1.313796   0.563436                  \n",
      "   133     0.613037   1.305141   0.560793                  \n",
      "   134     0.633728   1.306494   0.556388                  \n",
      "   135     0.613413   1.294736   0.567401                  \n",
      "   136     0.626498   1.298369   0.557269                  \n",
      "   137     0.625337   1.307487   0.561674                  \n",
      "   138     0.606445   1.312342   0.56652                   \n",
      "   139     0.617421   1.30484    0.566079                  \n",
      "   140     0.61476    1.298562   0.564317                  \n",
      "   141     0.621951   1.305044   0.564758                  \n",
      "   142     0.620459   1.299569   0.562996                  \n",
      "   143     0.610296   1.311561   0.559471                  \n",
      "   144     0.608779   1.313595   0.565639                  \n",
      "   145     0.607396   1.315892   0.570044                  \n",
      "   146     0.602978   1.310348   0.564758                  \n",
      "   147     0.605884   1.305668   0.559031                  \n",
      "   148     0.601808   1.328027   0.561674                  \n",
      "   149     0.600352   1.328305   0.559912                  \n",
      "   150     0.605798   1.332449   0.563436                  \n",
      "   151     0.598731   1.321423   0.555507                  \n",
      "   152     0.593817   1.332482   0.562115                  \n",
      "   153     0.599667   1.319844   0.55815                   \n",
      "   154     0.589238   1.319507   0.560352                  \n",
      "   155     0.590561   1.312429   0.563436                  \n",
      "   156     0.594656   1.321561   0.569163                  \n",
      "   157     0.590181   1.318324   0.562996                  \n",
      "   158     0.589214   1.319377   0.56696                   \n",
      "   159     0.584094   1.306899   0.559912                  \n",
      "   160     0.583305   1.318105   0.560793                  \n",
      "   161     0.591204   1.336694   0.556828                  \n",
      "   162     0.572753   1.34172    0.561233                  \n",
      "   163     0.584081   1.338934   0.560793                  \n",
      "   164     0.580057   1.343101   0.562555                  \n",
      "   165     0.581571   1.353941   0.559912                  \n",
      "   166     0.569839   1.340638   0.563436                  \n",
      "   167     0.569617   1.342894   0.557269                  \n",
      "   168     0.57243    1.355735   0.561674                  \n",
      "   169     0.580024   1.338136   0.55859                   \n",
      "   170     0.578816   1.341522   0.562555                  \n",
      "   171     0.572516   1.338547   0.561233                  \n",
      "   172     0.570096   1.340649   0.559912                  \n",
      "   173     0.568945   1.340035   0.55859                   \n",
      "   174     0.56571    1.348905   0.562115                  \n",
      "   175     0.581353   1.356797   0.564317                  \n",
      "   176     0.562718   1.346432   0.561674                  \n",
      "   177     0.568406   1.355216   0.566079                  \n",
      "   178     0.567246   1.358937   0.559031                  \n",
      "   179     0.561946   1.352142   0.562115                  \n",
      "   180     0.565195   1.351317   0.562555                  \n",
      "   181     0.545607   1.359034   0.55859                   \n",
      "   182     0.550421   1.347423   0.562555                  \n",
      "   183     0.551233   1.362297   0.563877                  \n",
      "   184     0.54831    1.361329   0.562996                  \n",
      "   185     0.550532   1.356005   0.55859                   \n",
      "   186     0.549916   1.368139   0.560793                  \n",
      "   187     0.557924   1.350603   0.562115                  \n",
      "   188     0.560512   1.352816   0.566079                  \n",
      "   189     0.551746   1.359251   0.562996                  \n",
      "   190     0.541333   1.358993   0.566079                  \n",
      "   191     0.540756   1.372814   0.563436                  \n",
      "   192     0.545269   1.353797   0.561674                  \n",
      "   193     0.546848   1.361625   0.559471                  \n",
      "   194     0.542648   1.370788   0.561674                  \n",
      "   195     0.542125   1.359173   0.559912                  \n",
      "   196     0.530001   1.358159   0.559912                  \n",
      "   197     0.540171   1.361648   0.564758                  \n",
      "   198     0.551827   1.369738   0.560793                  \n",
      "   199     0.544986   1.373581   0.560793                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.37358]), 0.5607929516468805]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.precompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c70d97d62115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "learn.fit(1e-2, 100, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('224_preF_b58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('224_preF_b58')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96784f7dee6249c891fdde4ff2d4f690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting training on small images for a few epochs, then switching to bigger images, and continuing training is an amazingly effective way to avoid overfitting.\n",
    "\n",
    "# http://forums.fast.ai/t/planet-classification-challenge/7824/96\n",
    "# set_data doesnt change the model at all. It just gives it new data to train with.\n",
    "learn.set_data(get_data(299, 80)) \n",
    "learn.freeze()\n",
    "\n",
    "#Source:   \n",
    "#    def set_data(self, data, precompute=False):\n",
    "#        super().set_data(data)\n",
    "#        if precompute:\n",
    "#            self.unfreeze()\n",
    "#            self.save_fc1()\n",
    "#            self.freeze()\n",
    "#            self.precompute = True\n",
    "#        else:\n",
    "#            self.freeze()\n",
    "#File:      ~/fastai/courses/dl1/fastai/conv_learner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d-1',\n",
       "              OrderedDict([('input_shape', [-1, 3, 299, 299]),\n",
       "                           ('output_shape', [-1, 64, 150, 150]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9408)])),\n",
       "             ('BatchNorm2d-2',\n",
       "              OrderedDict([('input_shape', [-1, 64, 150, 150]),\n",
       "                           ('output_shape', [-1, 64, 150, 150]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 128)])),\n",
       "             ('ReLU-3',\n",
       "              OrderedDict([('input_shape', [-1, 64, 150, 150]),\n",
       "                           ('output_shape', [-1, 64, 150, 150]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('MaxPool2d-4',\n",
       "              OrderedDict([('input_shape', [-1, 64, 150, 150]),\n",
       "                           ('output_shape', [-1, 64, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-5',\n",
       "              OrderedDict([('input_shape', [-1, 64, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 16384)])),\n",
       "             ('BatchNorm2d-6',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-7',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-8',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-9',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-10',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-11',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-12',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('Conv2d-13',\n",
       "              OrderedDict([('input_shape', [-1, 64, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 16384)])),\n",
       "             ('BatchNorm2d-14',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-15',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-16',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-17',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-18',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-19',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-20',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-21',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-22',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-23',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-24',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-25',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-26',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-27',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-28',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-29',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-30',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-31',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-32',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-33',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-34',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 131072)])),\n",
       "             ('BatchNorm2d-35',\n",
       "              OrderedDict([('input_shape', [-1, 512, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-36',\n",
       "              OrderedDict([('input_shape', [-1, 512, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-37',\n",
       "              OrderedDict([('input_shape', [-1, 512, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-38',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-39',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-40',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-41',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('Conv2d-42',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 131072)])),\n",
       "             ('BatchNorm2d-43',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-44',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-45',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-46',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-47',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-48',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-49',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-50',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-51',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-52',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-53',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-54',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-55',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-56',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-57',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-58',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-59',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-60',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-61',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-62',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-63',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-64',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-65',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-66',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-67',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-68',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-69',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-70',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-71',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-72',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 524288)])),\n",
       "             ('BatchNorm2d-73',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-74',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-75',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-76',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-77',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-78',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-79',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('Conv2d-80',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 524288)])),\n",
       "             ('BatchNorm2d-81',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-82',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-83',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-84',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-85',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-86',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-87',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-88',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-89',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-90',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-91',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-92',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-93',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-94',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-95',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-96',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-97',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-98',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-99',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-100',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-101',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-102',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-103',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-104',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-105',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-106',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-107',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-108',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-109',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-110',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-111',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-112',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-113',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-114',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-115',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-116',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-117',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-118',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-119',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-120',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-121',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-122',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-123',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-124',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-125',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-126',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-127',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-128',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-129',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-130',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-131',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-132',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-133',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-134',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-135',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-136',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-137',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-138',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-139',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-140',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-141',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-142',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-143',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-144',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-145',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-146',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-147',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-148',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-149',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-150',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-151',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-152',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-153',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-154',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-155',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-156',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-157',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-158',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-159',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-160',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-161',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-162',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-163',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-164',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-165',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-166',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-167',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-168',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-169',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-170',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-171',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-172',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-173',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-174',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-175',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-176',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-177',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-178',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-179',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-180',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-181',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-182',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-183',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-184',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-185',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-186',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-187',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-188',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-189',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-190',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-191',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-192',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-193',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-194',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-195',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-196',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-197',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-198',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-199',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-200',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-201',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-202',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-203',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-204',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-205',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-206',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-207',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-208',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-209',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-210',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-211',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-212',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-213',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-214',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-215',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-216',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-217',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-218',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-219',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-220',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-221',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-222',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-223',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-224',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-225',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-226',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-227',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-228',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-229',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-230',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-231',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-232',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-233',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-234',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-235',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-236',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-237',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-238',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-239',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-240',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-241',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-242',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-243',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-244',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-245',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-246',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-247',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-248',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-249',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-250',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-251',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-252',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-253',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-254',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-255',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-256',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-257',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-258',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-259',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-260',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-261',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-262',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-263',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-264',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-265',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-266',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-267',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-268',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-269',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-270',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-271',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-272',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-273',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-274',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-275',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-276',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-277',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-278',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-279',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-280',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-281',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2097152)])),\n",
       "             ('BatchNorm2d-282',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-283',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-284',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-285',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-286',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-287',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-288',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('Conv2d-289',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2097152)])),\n",
       "             ('BatchNorm2d-290',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-291',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-292',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-293',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-294',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-295',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-296',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-297',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-298',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-299',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-300',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-301',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-302',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-303',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-304',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-305',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-306',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-307',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-308',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-309',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveMaxPool2d-310',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveAvgPool2d-311',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveConcatPool2d-312',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 4096, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Flatten-313',\n",
       "              OrderedDict([('input_shape', [-1, 4096, 1, 1]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-314',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8192)])),\n",
       "             ('Dropout-315',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-316',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2097664)])),\n",
       "             ('ReLU-317',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-318',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('Dropout-319',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-320',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 12]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6156)])),\n",
       "             ('LogSoftmax-321',\n",
       "              OrderedDict([('input_shape', [-1, 12]),\n",
       "                           ('output_shape', [-1, 12]),\n",
       "                           ('nb_params', 0)]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c463da88530e46be93028e6fa29c03ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.626417   1.426835   0.503524  \n",
      "    1      1.595296   1.415666   0.511013                   \n",
      "    2      1.552414   1.397453   0.514097                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.39745]), 0.514096918203232]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(5e-3, 3, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss is much lower than training loss. This is a sign of underfitting. Cycle_len=1 may be too short. Let's set cycle_mult=2 to find better parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4bd11556e14dc3be65e09eda4dfb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.515581   1.383625   0.522026  \n",
      "    1      1.507568   1.370226   0.519383                   \n",
      "    2      1.471789   1.36212    0.52467                    \n",
      "    3      1.465919   1.36207    0.522026                   \n",
      "    4      1.451652   1.337367   0.533921                   \n",
      "    5      1.427047   1.333412   0.53304                    \n",
      "    6      1.415496   1.328908   0.531278                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.32891]), 0.531277537477174]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you are under fitting, it means cycle_len=1 is too short (learning rate is getting reset before it had the chance to zoom in properly).\n",
    "learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2) # 1+2+4 = 7 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss and validation loss are getting closer and smaller. We are on right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5330396475770925, 1.3259945803139326)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds, y = learn.TTA() # (5, 2044, 120), (2044,)\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2270, array([9, 9, 9, 9, 9]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.val_ds.y), data.val_ds.y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aa48489fce4863bbefd3ad30476171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.437424   1.327512   0.537445  \n",
      "    1      1.412686   1.320846   0.529956                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.32085]), 0.5299559487645321]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 1, cycle_len=2) # 1+1 = 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5259911894273128, 1.3275131736628687)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds, y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=np.array([5e-5,5e-4,5e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af18979f8d24f6d83dcac04a6ed7151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.set_data(get_data(299, 48)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668e2c7c4ca64a0d8ee3ea988160294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85%| | 193/228 [01:58<00:20,  1.67it/s, loss=5.47]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrf=learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0d779acb94408da8bc432d45dc97ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.448863   1.311074   0.534361  \n",
      "    1      1.405124   1.303028   0.538767                   \n",
      "    2      1.396005   1.306464   0.535683                   \n",
      "    3      1.402845   1.303234   0.547137                   \n",
      "    4      1.372364   1.297798   0.544934                   \n",
      "    5      1.342952   1.290582   0.542731                   \n",
      "    6      1.379837   1.28929    0.546696                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.28929]), 0.5466960388395755]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_12cls_unf_bs48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_12cls_unf_bs48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ecb8b5806a417dbfe25ade95d37e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1023), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.372671   1.290845   0.547577  \n",
      "    1      1.380739   1.278982   0.542291                   \n",
      "    2      1.368688   1.279418   0.546696                   \n",
      "    3      1.367094   1.272629   0.548018                   \n",
      "    4      1.345491   1.272921   0.553304                   \n",
      "    5      1.333466   1.267104   0.553744                   \n",
      "    6      1.334859   1.264128   0.553304                   \n",
      "    7      1.371418   1.269992   0.547137                   \n",
      "    8      1.364804   1.263759   0.559031                   \n",
      "    9      1.343403   1.25903    0.561674                   \n",
      "    10     1.344066   1.254189   0.557269                   \n",
      "    11     1.332897   1.248577   0.552423                   \n",
      "    12     1.28492    1.243182   0.556388                   \n",
      "    13     1.310286   1.241637   0.554185                   \n",
      "    14     1.273155   1.241403   0.554626                   \n",
      "    15     1.35625    1.242023   0.557269                   \n",
      "    16     1.328576   1.245065   0.54978                    \n",
      "    17     1.290075   1.243276   0.559471                   \n",
      "    18     1.304591   1.246133   0.560793                   \n",
      "    19     1.278822   1.233118   0.559471                   \n",
      "    20     1.281957   1.234232   0.553744                   \n",
      "    21     1.242081   1.222856   0.566079                   \n",
      "    22     1.263597   1.224243   0.561674                   \n",
      "    23     1.244797   1.217598   0.567841                   \n",
      "    24     1.232354   1.216485   0.564317                   \n",
      "    25     1.259178   1.212913   0.566079                   \n",
      "    26     1.240846   1.20705    0.565198                   \n",
      "    27     1.236394   1.207521   0.567401                   \n",
      "    28     1.234203   1.203971   0.568722                   \n",
      "    29     1.235416   1.206624   0.570925                   \n",
      "    30     1.199661   1.206389   0.563877                   \n",
      "    31     1.241285   1.206337   0.563877                   \n",
      " 99%|| 188/190 [02:18<00:01,  1.38it/s, loss=1.27]"
     ]
    }
   ],
   "source": [
    "learn.fit(lr, 10, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is so similar to ImageNet dataset. Training convolution layers doesn't help much. We are not going to unfreeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission\n",
    "\n",
    "https://youtu.be/9C06ZPF8Uuc?t=1905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA(is_test=True) # use test dataset rather than validation dataset\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "#accuracy_np(probs, y), metrcs.log_loss(y, probs) # This does not make sense since test dataset has no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape # (n_images, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(probs)\n",
    "df.columns = data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, 'id', [o[5:-4] for o in data.test_ds.fnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM = f'{PATH}/subm/'\n",
    "os.makedirs(SUBM, exist_ok=True)\n",
    "df.to_csv(f'{SUBM}subm.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(f'{SUBM}subm.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = data.val_ds.fnames[0]\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(PATH + fn).resize((150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1.\n",
    "trn_tfms, val_tfms = tfms_from_model(arch, sz)\n",
    "ds = FilesIndexArrayDataset([fn], np.array([0]), val_tfms, PATH)\n",
    "dl = DataLoader(ds)\n",
    "preds = learn.predict_dl(dl)\n",
    "np.argmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2.\n",
    "trn_tfms, val_tfms = tfms_from_model(arch, sz)\n",
    "im = val_tfms(open_image(PATH + fn)) # open_image() returns numpy.ndarray\n",
    "preds = learn.predict_array(im[None])\n",
    "np.argmax(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastaiold",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0147ea39a83144aa99e7877a908ca502": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "02af08793c9a4c5ba0cbd7462a715f89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "03c7476e32ce48c2b38a7023c2dcc0a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "075434e4b0234c5fa61c8d63fec51679": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "08ba1e10c1a641f7876680a632f6821d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b0edcf9ef0754ca4bb7fe6e8790c464b",
       "max": 6,
       "style": "IPY_MODEL_c13bdace91114d639b408aa120c97f79",
       "value": 6
      }
     },
     "0b49f5119c6d4a25bb575a20e33fe219": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12cc98b1fc954667bd49c08972278f1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13cbbf3a554a40f09d40df39c2cacf47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "148c1abd5f1747a2a787c09cef3071bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_902c060a37b4491788462dce6be1798b",
       "style": "IPY_MODEL_88db019106ca4893816ea780e567a175",
       "value": "100% 3/3 [16:30&lt;00:00, 330.17s/it]"
      }
     },
     "16993380d0fa42b888a52dbf1bb42f20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c74dedf09f57490a9f1ac9af54e14bf3",
       "max": 6,
       "style": "IPY_MODEL_557d35c901f54575b5963e3b7ce932a3",
       "value": 6
      }
     },
     "19c5557812814cb28bc7e0cc80c65c5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c241c0e5b5844508c1c50e2a7c09b1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c509f99160e41d5af6e565b1541e571": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9352e3fd2af54e09ad67afb4ac446b27",
       "style": "IPY_MODEL_62e7ecfbbea7427385deb25ed2f4232f",
       "value": "100% 6/6 [00:00&lt;00:00, 380.26it/s]"
      }
     },
     "1e2f1c40607144eb9cb7b9e257d06f35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1fc90792a2b74c38aaf73659c3756831": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "201ae86e437e4b5390782e9df272326d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c8871c57d7bd4b61a99633668fb44621",
        "IPY_MODEL_38eb37d7293d4273a6ae06b5a451617d"
       ],
       "layout": "IPY_MODEL_1c241c0e5b5844508c1c50e2a7c09b1b"
      }
     },
     "20bfce64bf354e2398353b6a2a31d0e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21e75994f5e749748dd5693819e51d93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "22952b3bfa674aff898e452b30bd319e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "26e2b0e7abb24e3fad1b5c0c166aa44d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_363595af2f5f4ad3a3f5a9e46069d58e",
       "max": 5,
       "style": "IPY_MODEL_02af08793c9a4c5ba0cbd7462a715f89",
       "value": 5
      }
     },
     "288366a4c5ba435286c34d6d5ad69201": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33f7abed940a4b24b01110da8ec67ddc",
        "IPY_MODEL_93f4380114fe48b88db18d0c97736fd4"
       ],
       "layout": "IPY_MODEL_5b17418f64954c14843a914942f8ce3d"
      }
     },
     "292be092bc964dd7a48bc62c906f5814": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2d1dfcb285154a7bac2db184a87f3c4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ff92f760a2b46a99576ab77daccac1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33ce7c9b732e4715ae12b71669addfdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_20bfce64bf354e2398353b6a2a31d0e5",
       "max": 6,
       "style": "IPY_MODEL_830aa543858f4a54a14aad440bcbde34",
       "value": 6
      }
     },
     "33f7abed940a4b24b01110da8ec67ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7ba0d94e2ef544fe9126eacee6588468",
       "max": 6,
       "style": "IPY_MODEL_8c8cb52ff5e54c55a297af7571a105ea",
       "value": 6
      }
     },
     "363595af2f5f4ad3a3f5a9e46069d58e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37f00a662a3c4b5f9f29db6903cfc5ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "37faed3c96f343e895d8f29ab07137c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "38eb37d7293d4273a6ae06b5a451617d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_42c23fec7e814a9aaf39ffa59c0ffe20",
       "style": "IPY_MODEL_7163788d004d4c15b0b0137a654079a0",
       "value": "100% 7/7 [35:35&lt;00:00, 305.08s/it]"
      }
     },
     "39cf7d6b5e98460e9a621f1d74e32fad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb554ddf38bb4597b282655ee74ca321",
       "style": "IPY_MODEL_c45beabad9d34af5b39ce603b5308ef9",
       "value": "100% 5/5 [27:26&lt;00:00, 329.38s/it]"
      }
     },
     "39d7df82d70b40df9f1171dad9ce2b4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3a1160614abc4724b73cc9bc2e0d0d46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_19c5557812814cb28bc7e0cc80c65c5f",
       "style": "IPY_MODEL_87707de469bc4c4bb2ba2a2ed7322de1",
       "value": "100% 5/5 [00:28&lt;00:00,  5.70s/it]"
      }
     },
     "3b2e29458fdc49d0864a7fc9a1faafb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ed035ff92d947ad9059d4bc03f2576c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_16993380d0fa42b888a52dbf1bb42f20",
        "IPY_MODEL_a52bd3eee7aa4e9093ee525fc42ff232"
       ],
       "layout": "IPY_MODEL_66e271ee790148eb89ae51b9b22216aa"
      }
     },
     "40060908ceee46fb859195274eca3da8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4b3362ffeac948e490ce91f1e0786f2e",
        "IPY_MODEL_c09f764cc1c74e3aa75fea864a8d21f3"
       ],
       "layout": "IPY_MODEL_e6926f2efb5340528a00c5045376c380"
      }
     },
     "405077ae66ac44afbd35d90a6afd6cab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41a9d48c29ef4752bcd2ddc7076dac3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "42c23fec7e814a9aaf39ffa59c0ffe20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "43f1cb9394c5422798a91c1b6b0a5907": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45a9f25bf3964af68dfce8bbb2091917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4654ebf91a484cb8b153920fb89bb522": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b3362ffeac948e490ce91f1e0786f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_67e1b7b183484f03b2f63c22c44e8713",
       "max": 6,
       "style": "IPY_MODEL_12cc98b1fc954667bd49c08972278f1d",
       "value": 6
      }
     },
     "4c5e108b0b9145cb9987a673dda89bb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d7cbd1c9b0b45359d4d31ad4e50e441": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "54f04d9c381446649b742e1242de6953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_f6539e378c294a6c955da278da4ade4e",
       "max": 2,
       "style": "IPY_MODEL_b1cddc992f864976b0f4132e0c4819f4",
       "value": 2
      }
     },
     "557d35c901f54575b5963e3b7ce932a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5791e4167111408ca3f1be2e16334803": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b0ec0306b6440c1a068951bf0d4a50c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5b17418f64954c14843a914942f8ce3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b494db622384a7fabd7ed86d7f0575c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_26e2b0e7abb24e3fad1b5c0c166aa44d",
        "IPY_MODEL_c05cfeeac9a04c68a7387dc65bf48227"
       ],
       "layout": "IPY_MODEL_4654ebf91a484cb8b153920fb89bb522"
      }
     },
     "5c13867f02614d49bc038af563f5a5a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7234b227a46b404598576e19a230a94e",
       "max": 6,
       "style": "IPY_MODEL_7ae2ea4a6e144242ad504c2032fbb9b9",
       "value": 6
      }
     },
     "5d3aa8de854a491485458bcaf88b9de4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c6a3a9d502b94ffba6cb6708c9762bd0",
        "IPY_MODEL_8d9389e09e1d44d88ffffdb83ac6f407"
       ],
       "layout": "IPY_MODEL_03c7476e32ce48c2b38a7023c2dcc0a1"
      }
     },
     "5f4033806f5144f28e0999110bf34612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_e59a2233ae454d81ada1b8494763e7bc",
       "max": 2,
       "style": "IPY_MODEL_292be092bc964dd7a48bc62c906f5814",
       "value": 2
      }
     },
     "61defccfe9dc4330931ce420b9976dcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4d7cbd1c9b0b45359d4d31ad4e50e441",
       "style": "IPY_MODEL_45a9f25bf3964af68dfce8bbb2091917",
       "value": "100% 2/2 [10:08&lt;00:00, 304.26s/it]"
      }
     },
     "62e7ecfbbea7427385deb25ed2f4232f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "62e8fbf1622b4a36a186e7f319552990": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_41a9d48c29ef4752bcd2ddc7076dac3c",
       "max": 2,
       "style": "IPY_MODEL_5b0ec0306b6440c1a068951bf0d4a50c",
       "value": 2
      }
     },
     "65d9210f3d3345019a1fcf4ad82f1ae9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "66179fdeb4534eb281f2690ded33950e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "66e271ee790148eb89ae51b9b22216aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67e1b7b183484f03b2f63c22c44e8713": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7163788d004d4c15b0b0137a654079a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7234b227a46b404598576e19a230a94e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "77132b06e4ad40fdabdc0dd93922af21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "784cd320488247aa87d575558f81d06b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7998921f656343ffadf133220692f859": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_0b49f5119c6d4a25bb575a20e33fe219",
       "max": 5,
       "style": "IPY_MODEL_cfeaf84bdbe34c7898e1ea8fc61b539c",
       "value": 5
      }
     },
     "7ad08fc0082a41bc882cedf6004206dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33ce7c9b732e4715ae12b71669addfdf",
        "IPY_MODEL_bc6e6b33b84344f094704454a0cf6f6b"
       ],
       "layout": "IPY_MODEL_2ff92f760a2b46a99576ab77daccac1d"
      }
     },
     "7adf432cd98f46df927bf9da9d94b182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7ae2ea4a6e144242ad504c2032fbb9b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7b215de0d6b64c3897c22bd817bbcb8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_1e2f1c40607144eb9cb7b9e257d06f35",
       "max": 3,
       "style": "IPY_MODEL_f38f8ae81eca43f9b1828369ebffd849",
       "value": 3
      }
     },
     "7ba0d94e2ef544fe9126eacee6588468": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8228413e70844312b5aa3128e3bea3ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_890f789420e4416697cafbf732f38b95",
       "style": "IPY_MODEL_b52cba9c0daf49aab716b75c91137766",
       "value": "100% 2/2 [00:06&lt;00:00,  3.12s/it]"
      }
     },
     "830aa543858f4a54a14aad440bcbde34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "868f105f38aa47b2b0223680dc202859": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87707de469bc4c4bb2ba2a2ed7322de1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "879f0f26c06041698336e79b9b92b0c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c19f1e5cf61444b5926c89b67c374dc8",
        "IPY_MODEL_148c1abd5f1747a2a787c09cef3071bc"
       ],
       "layout": "IPY_MODEL_b4a9636246544cd7a3ebcff56595bea2"
      }
     },
     "88db019106ca4893816ea780e567a175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "890f789420e4416697cafbf732f38b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c8cb52ff5e54c55a297af7571a105ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8d9389e09e1d44d88ffffdb83ac6f407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1fc90792a2b74c38aaf73659c3756831",
       "style": "IPY_MODEL_66179fdeb4534eb281f2690ded33950e",
       "value": "100% 2/2 [00:06&lt;00:00,  3.17s/it]"
      }
     },
     "902c060a37b4491788462dce6be1798b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "91b150ff417c44d1bc68e66d1b3c5925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9352e3fd2af54e09ad67afb4ac446b27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93f4380114fe48b88db18d0c97736fd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b7e6303bb38542c9a8282079c33beca7",
       "style": "IPY_MODEL_21e75994f5e749748dd5693819e51d93",
       "value": "100% 6/6 [00:00&lt;00:00, 343.35it/s]"
      }
     },
     "956dbbddf33a476db031c3d1b8ad5b4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7998921f656343ffadf133220692f859",
        "IPY_MODEL_39cf7d6b5e98460e9a621f1d74e32fad"
       ],
       "layout": "IPY_MODEL_65d9210f3d3345019a1fcf4ad82f1ae9"
      }
     },
     "a19c0f9c5cb34e9e8cdfe5c356084b80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a52bd3eee7aa4e9093ee525fc42ff232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_405077ae66ac44afbd35d90a6afd6cab",
       "style": "IPY_MODEL_fa7fbd7aa5ab41a5aaabfca1db46058e",
       "value": "100% 6/6 [00:00&lt;00:00, 377.47it/s]"
      }
     },
     "a6ba13af33984f1b96ed120d3798b035": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6cf535c8f30471e947503cae6bdb611": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_e77a75b0e9b446318ef399e225003e65",
       "max": 7,
       "style": "IPY_MODEL_c8469a077e5449b69821e6b27b8e74e5",
       "value": 7
      }
     },
     "a7afeac8de60479e8c92248312fde8fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aad1581c410d48feb48e6b943c14bdb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b0edcf9ef0754ca4bb7fe6e8790c464b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b1258a718296423d9f4d9c0535aae63b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3b2e29458fdc49d0864a7fc9a1faafb6",
       "style": "IPY_MODEL_37f00a662a3c4b5f9f29db6903cfc5ba",
       "value": "100% 3/3 [15:16&lt;00:00, 305.39s/it]"
      }
     },
     "b1cddc992f864976b0f4132e0c4819f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b3ae58ce0bce4cd1b678a043b8ce4415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_54f04d9c381446649b742e1242de6953",
        "IPY_MODEL_e10c6be126a045d9809b90c0880e7362"
       ],
       "layout": "IPY_MODEL_77132b06e4ad40fdabdc0dd93922af21"
      }
     },
     "b4a9636246544cd7a3ebcff56595bea2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b4c9fe6ac6f649dc9ed8a077a5eaeb20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b52cba9c0daf49aab716b75c91137766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b7e6303bb38542c9a8282079c33beca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba26964d72134b3aa6095e3102f1531b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc6e6b33b84344f094704454a0cf6f6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e6337586212341de8a4de0ce6bd25649",
       "style": "IPY_MODEL_7adf432cd98f46df927bf9da9d94b182",
       "value": "100% 6/6 [00:00&lt;00:00, 270.73it/s]"
      }
     },
     "bd8469f87f2d4e789610110acd07571d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed2e720acb7a467c85bd7db465281523",
        "IPY_MODEL_3a1160614abc4724b73cc9bc2e0d0d46"
       ],
       "layout": "IPY_MODEL_37faed3c96f343e895d8f29ab07137c0"
      }
     },
     "c01a6ed0806a4aaaaf580bf56e78398f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c05cfeeac9a04c68a7387dc65bf48227": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7afeac8de60479e8c92248312fde8fc",
       "style": "IPY_MODEL_22952b3bfa674aff898e452b30bd319e",
       "value": "100% 5/5 [00:16&lt;00:00,  3.28s/it]"
      }
     },
     "c09f764cc1c74e3aa75fea864a8d21f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c80d11f161704ab3bda86e7d1dfaff03",
       "style": "IPY_MODEL_e1df5ccb47594688afaee7c51d9cfeed",
       "value": "100% 6/6 [00:00&lt;00:00, 377.26it/s]"
      }
     },
     "c13bdace91114d639b408aa120c97f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c19f1e5cf61444b5926c89b67c374dc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_43f1cb9394c5422798a91c1b6b0a5907",
       "max": 3,
       "style": "IPY_MODEL_aad1581c410d48feb48e6b943c14bdb8",
       "value": 3
      }
     },
     "c28408357da544cd8c8bbd5c7469e541": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5c13867f02614d49bc038af563f5a5a2",
        "IPY_MODEL_1c509f99160e41d5af6e565b1541e571"
       ],
       "layout": "IPY_MODEL_e41e9dc77ce44f3fad0aee4db429cd7d"
      }
     },
     "c30905e882964682bbe231e445b57833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08ba1e10c1a641f7876680a632f6821d",
        "IPY_MODEL_e6f6231f671e4871b9e496cce50bf2b9"
       ],
       "layout": "IPY_MODEL_db256ef0046f46d085353ffba880c8fa"
      }
     },
     "c438be876cfc49e8920f2704c6cddbdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_de9c288444e74ea38dbac2ef1b1078e4",
        "IPY_MODEL_fe3c562acb4e432280f6432df0710577"
       ],
       "layout": "IPY_MODEL_ba26964d72134b3aa6095e3102f1531b"
      }
     },
     "c45beabad9d34af5b39ce603b5308ef9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c522ca216e5a49e4a7624f94b7215c52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_62e8fbf1622b4a36a186e7f319552990",
        "IPY_MODEL_8228413e70844312b5aa3128e3bea3ad"
       ],
       "layout": "IPY_MODEL_cb6af7a441964132bb58e75ef7b4bd19"
      }
     },
     "c6a3a9d502b94ffba6cb6708c9762bd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_868f105f38aa47b2b0223680dc202859",
       "max": 2,
       "style": "IPY_MODEL_f4c78df23c774bb290250936916d962e",
       "value": 2
      }
     },
     "c74dedf09f57490a9f1ac9af54e14bf3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c80d11f161704ab3bda86e7d1dfaff03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8469a077e5449b69821e6b27b8e74e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c8871c57d7bd4b61a99633668fb44621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_2d1dfcb285154a7bac2db184a87f3c4e",
       "max": 7,
       "style": "IPY_MODEL_ce4922cb72aa4e5c8b5713e47e49029e",
       "value": 7
      }
     },
     "c8ef187b9a1249018de7378cee824894": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b215de0d6b64c3897c22bd817bbcb8d",
        "IPY_MODEL_b1258a718296423d9f4d9c0535aae63b"
       ],
       "layout": "IPY_MODEL_784cd320488247aa87d575558f81d06b"
      }
     },
     "cac9427c464f4db9b71c36da9245c066": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb554ddf38bb4597b282655ee74ca321": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cb6af7a441964132bb58e75ef7b4bd19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce4922cb72aa4e5c8b5713e47e49029e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cfeaf84bdbe34c7898e1ea8fc61b539c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d2b4db8ba2cb4c859ba3e0041c243193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_13cbbf3a554a40f09d40df39c2cacf47",
       "style": "IPY_MODEL_91b150ff417c44d1bc68e66d1b3c5925",
       "value": "100% 7/7 [38:27&lt;00:00, 329.65s/it]"
      }
     },
     "db256ef0046f46d085353ffba880c8fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "de9c288444e74ea38dbac2ef1b1078e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_0147ea39a83144aa99e7877a908ca502",
       "max": 5,
       "style": "IPY_MODEL_075434e4b0234c5fa61c8d63fec51679",
       "value": 5
      }
     },
     "def3b0d0275c454e95c5c2952fc9d017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5f4033806f5144f28e0999110bf34612",
        "IPY_MODEL_61defccfe9dc4330931ce420b9976dcd"
       ],
       "layout": "IPY_MODEL_e22e48eeda32478cbb6bf53babd9e700"
      }
     },
     "e10c6be126a045d9809b90c0880e7362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5791e4167111408ca3f1be2e16334803",
       "style": "IPY_MODEL_c01a6ed0806a4aaaaf580bf56e78398f",
       "value": "100% 2/2 [10:59&lt;00:00, 329.53s/it]"
      }
     },
     "e1df5ccb47594688afaee7c51d9cfeed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e22e48eeda32478cbb6bf53babd9e700": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e41e9dc77ce44f3fad0aee4db429cd7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e59a2233ae454d81ada1b8494763e7bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6337586212341de8a4de0ce6bd25649": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6926f2efb5340528a00c5045376c380": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6f6231f671e4871b9e496cce50bf2b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4c5e108b0b9145cb9987a673dda89bb6",
       "style": "IPY_MODEL_a19c0f9c5cb34e9e8cdfe5c356084b80",
       "value": "100% 6/6 [00:00&lt;00:00, 42.32it/s]"
      }
     },
     "e77a75b0e9b446318ef399e225003e65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ed2e720acb7a467c85bd7db465281523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_f9852feeb51543e4a0d21c5cd6326380",
       "max": 5,
       "style": "IPY_MODEL_cac9427c464f4db9b71c36da9245c066",
       "value": 5
      }
     },
     "f2cf6963cc204b18921e63e1933ad18b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a6cf535c8f30471e947503cae6bdb611",
        "IPY_MODEL_d2b4db8ba2cb4c859ba3e0041c243193"
       ],
       "layout": "IPY_MODEL_b4c9fe6ac6f649dc9ed8a077a5eaeb20"
      }
     },
     "f38f8ae81eca43f9b1828369ebffd849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f4c78df23c774bb290250936916d962e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f6539e378c294a6c955da278da4ade4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f9852feeb51543e4a0d21c5cd6326380": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa7fbd7aa5ab41a5aaabfca1db46058e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fe3c562acb4e432280f6432df0710577": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a6ba13af33984f1b96ed120d3798b035",
       "style": "IPY_MODEL_39d7df82d70b40df9f1171dad9ce2b4c",
       "value": "100% 5/5 [25:26&lt;00:00, 305.38s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
